{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41049c98",
   "metadata": {},
   "source": [
    "# A2 Coursework Neural Models\n",
    "## Llama models promting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ddcd2b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675eacc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/torchmetrics/utilities/imports.py:23: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c1b8680a26456fa23c59d6e0ed12d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import Trainer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments \n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import IA3Config, PrefixTuningConfig\n",
    "from evaluate import load\n",
    "\n",
    "login(token=\"\")\n",
    "\n",
    "bleu_metric = load(\"sacrebleu\")\n",
    "comet_metric = load(\"comet\")\n",
    "rouge_metric = load(\"rouge\")\n",
    "chrf_metric = load(\"chrf\")\n",
    "\n",
    "DEBUG_MODE = False\n",
    "DEBUG_FRACTION = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b077f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "opus_dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-fr\")\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    opus_dataset = DatasetDict({\n",
    "        split: opus_dataset[split]\n",
    "            .shuffle(seed=42)\n",
    "            .select(range(int(len(opus_dataset[split]) * DEBUG_FRACTION)))\n",
    "        for split in opus_dataset.keys()\n",
    "    })\n",
    "\n",
    "print(opus_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2a07e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tok_length = 24\n",
    "\n",
    "checkpoint_llama = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(\n",
    "    checkpoint_llama, \n",
    "    padding=True,\n",
    "    pad_to_multiple_of=8,\n",
    "    truncation=True,\n",
    "    max_length=max_tok_length,\n",
    "    padding_side='left',\n",
    ")\n",
    "\n",
    "tokenizer_llama.pad_token = tokenizer_llama.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67c25e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = \"en\"\n",
    "tag_lang = \"fr\"\n",
    "\n",
    "source_language = \"English\"\n",
    "target_language = \"French\"\n",
    "\n",
    "def preprocess_function_opus(batch, tokenizer):\n",
    "    source_texts = [t[src_lang] for t in batch[\"translation\"]]\n",
    "    target_texts = [t[tag_lang] for t in batch[\"translation\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        source_texts,\n",
    "        text_target=target_texts,\n",
    "    )\n",
    "    \n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aa9c2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b02cfe4b7e499bb6b712afaed470b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998769e72ccd4145ad3dc3af9a9ebd06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd84b2ebd084c73ba50f9eef62f9511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd1e60ca20d4b71bd1321cad4257d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Discarding source and target sentences with more than 24 tokens (num_proc=8):   0%|          | 0/100 [00:00<?,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbbe7395c2544a7bbb467804185f1198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Discarding source and target sentences with more than 24 tokens (num_proc=8):   0%|          | 0/50000 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf9886fdb5d412d9e696822c78a4d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Discarding source and target sentences with more than 24 tokens (num_proc=8):   0%|          | 0/100 [00:00<?,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opus_dataset_llama = opus_dataset.map(\n",
    "    lambda batch: preprocess_function_opus(batch, tokenizer_llama),\n",
    "    batched=True, \n",
    "    num_proc=8\n",
    ")\n",
    "\n",
    "opus_dataset_llama = opus_dataset_llama.filter(\n",
    "    lambda x: len(x[\"input_ids\"]) <= max_tok_length and len(x[\"labels\"]) <= max_tok_length,\n",
    "    desc=f\"Discarding source and target sentences with more than {max_tok_length} tokens\", num_proc=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d7b60cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2 109\n",
      " 3 764\n",
      " 4 1601\n",
      " 5 1911\n",
      " 6 2303\n",
      " 7 2521\n",
      " 8 2637\n",
      " 9 2523\n",
      "10 2330\n",
      "11 2042\n",
      "12 1878\n",
      "13 1567\n",
      "14 1310\n",
      "15 1143\n",
      "16 966\n",
      "17 813\n",
      "18 703\n",
      "19 596\n",
      "20 410\n",
      "21 341\n",
      "22 247\n",
      "23 185\n",
      "24 135\n"
     ]
    }
   ],
   "source": [
    "def show_length_distribution(tokenized_datasets):\n",
    "    dic = {}\n",
    "    for sample in tokenized_datasets['train']:\n",
    "        sample_length = len(sample['input_ids'])\n",
    "        if sample_length not in dic:\n",
    "            dic[sample_length] = 1\n",
    "        else:\n",
    "            dic[sample_length] += 1 \n",
    "\n",
    "    for i in range(1,max_tok_length+1):\n",
    "        if i in dic:\n",
    "            print(f\"{i:>2} {dic[i]:>3}\")\n",
    "            \n",
    "show_length_distribution(opus_dataset_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_max_tok_len(task_prefix, tokenizer):\n",
    "    s = \"\"\n",
    "    prefix_tok_len = len(tokenizer.encode(f\"{task_prefix}{src_lang}: {s} = {tag_lang}: \"))\n",
    "    max_tok_len = prefix_tok_len\n",
    "    # Adding 2 for new line in target sentence and eos_token_id token\n",
    "    max_tok_len += 2 * max_tok_length + 2\n",
    "    return max_tok_len\n",
    "\n",
    "def preprocess4training_function(sample, task_prefix, tokenizer):\n",
    "\n",
    "    max_tok_len = get_training_max_tok_len(task_prefix, tokenizer)\n",
    "    sample_size = len(sample[\"translation\"])\n",
    "    inputs  = [f\"{task_prefix}{source_language}: {s[src_lang]} = {target_language}: \" for s in sample[\"translation\"]]\n",
    "\n",
    "    targets = [f\"{s[tag_lang]}\\n\" for s in sample[\"translation\"]]\n",
    "    # targets = [f\"{s[tag_lang]} <END>\" for s in sample[\"translation\"]]\n",
    "\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    labels = tokenizer(targets)\n",
    "    \n",
    "    for i in range(sample_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "\n",
    "    for i in range(sample_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_tok_len - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_tok_len - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        labels[\"input_ids\"][i] = [-100] * (max_tok_len - len(sample_input_ids)) + label_input_ids\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_tok_len])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_tok_len])\n",
    "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_tok_len])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f303f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_max_tok_len(num_shots, task_prefix, tokenizer):\n",
    "    \n",
    "    s = \"\"\n",
    "    shots = \"\"\n",
    "    prefix_tok_len = len(tokenizer.encode(f\"{task_prefix}{shots}{src_lang}: {s} = {tag_lang}: \"))\n",
    "    shot_tok_len   = len(tokenizer.encode(f\"{src_lang}: {s} = {tag_lang}: {s}\\n\"))\n",
    "    max_tok_len = prefix_tok_len\n",
    "    max_tok_len += num_shots * (shot_tok_len + 2 * max_tok_length) \n",
    "    max_tok_len += max_tok_length\n",
    "    return task_prefix, max_tok_len \n",
    "\n",
    "def preprocess4test_function(test_sample, task_prefix, tokenizer, training_sample=None, num_shots = 1):\n",
    "        \n",
    "    if training_sample is None:\n",
    "        inputs = [f\"{task_prefix}{source_language}: {s[src_lang]} = {target_language}: \" for s in test_sample[\"translation\"]]\n",
    "        model_inputs = tokenizer(inputs,padding=True,)\n",
    "        return model_inputs\n",
    "    \n",
    "    task_prefix, max_tok_len = get_test_max_tok_len(num_shots, task_prefix, tokenizer)\n",
    "    \n",
    "    shots = \"\"\n",
    "\n",
    "    random_seed = time.time()\n",
    "    t_sample = training_sample.shuffle(seed=int(random_seed)).select(range(num_shots))\n",
    "    # for s in t_sample[\"translation\"]: shots += f\"{source_language}: {s[src_lang]} = {target_language}: {s[tag_lang]} <END>\"\n",
    "    for s in t_sample[\"translation\"]: shots += f\"{source_language}: {s[src_lang]} = {target_language}: {s[tag_lang]}\\n\"\n",
    "    \n",
    "    \n",
    "    inputs = [f\"{task_prefix}{shots}{source_language}: {s[src_lang]} = {target_language}: \" for s in test_sample[\"translation\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_tok_len, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31b6bc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6d0a3286244d649cbcd19f655a067f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_prefix = f\"Translate from {source_language} to {target_language}:\\n\"    \n",
    "\n",
    "devset_llama = opus_dataset_llama['validation'].map(\n",
    "    lambda x: preprocess4training_function(x, task_prefix, tokenizer_llama),\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6de7107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 4103, 9632, 515, 4223, 304, 5176, 29901, 13, 24636, 29901, 306, 29915, 29885, 1811, 304, 1827, 393, 1105, 2397, 338, 19281, 4939, 29889, 353, 5176, 29901, 29871, 1, 7747, 712, 432, 29915, 404, 15802, 316, 2970, 29892, 274, 29915, 342, 712, 1105, 2397, 707, 14979, 17856, 29889, 29871, 32000, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1, 7747, 712, 432, 29915, 404, 15802, 316, 2970, 29892, 274, 29915, 342, 712, 1105, 2397, 707, 14979, 17856, 29889, 29871, 32000, 2]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 4103, 9632, 515, 4223, 304, 5176, 29901, 13, 24636, 29901, 1126, 1568, 310, 278, 931, 29892, 393, 29915, 29879, 599, 263, 7145, 29893, 1607, 4225, 29889, 887, 29915, 645, 437, 29889, 353, 5176, 29901, 29871, 1, 8748, 274, 29915, 342, 18484, 2257, 4555, 4625, 3008, 28230, 966, 269, 1179, 19452, 29889, 29871, 32000, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1, 8748, 274, 29915, 342, 18484, 2257, 4555, 4625, 3008, 28230, 966, 269, 1179, 19452, 29889, 29871, 32000, 2]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 4103, 9632, 515, 4223, 304, 5176, 29901, 13, 24636, 29901, 450, 970, 884, 756, 263, 6297, 304, 1708, 297, 445, 22056, 4034, 29889, 353, 5176, 29901, 29871, 1, 951, 970, 24853, 8648, 443, 17889, 1465, 2257, 760, 264, 29294, 29889, 29871, 32000, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1, 951, 970, 24853, 8648, 443, 17889, 1465, 2257, 760, 264, 29294, 29889, 29871, 32000, 2]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 4103, 9632, 515, 4223, 304, 5176, 29901, 13, 24636, 29901, 288, 678, 6916, 297, 7483, 11780, 29889, 353, 5176, 29901, 29871, 1, 288, 966, 1874, 4110, 1104, 271, 10270, 3479, 3008, 29877, 1144, 427, 7483, 29871, 32000, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1, 288, 966, 1874, 4110, 1104, 271, 10270, 3479, 3008, 29877, 1144, 427, 7483, 29871, 32000, 2]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 4103, 9632, 515, 4223, 304, 5176, 29901, 13, 24636, 29901, 18110, 29892, 591, 29915, 645, 5353, 445, 2678, 29889, 306, 29915, 645, 1074, 366, 29889, 353, 5176, 29901, 29871, 1, 8965, 269, 29915, 342, 285, 5059, 29948, 425, 923, 4909, 29889, 29871, 32000, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1, 8965, 269, 29915, 342, 285, 5059, 29948, 425, 923, 4909, 29889, 29871, 32000, 2]\n"
     ]
    }
   ],
   "source": [
    "for sample in devset_llama.select(range(5)):\n",
    "    print(sample['input_ids'])\n",
    "    print(sample['attention_mask'])\n",
    "    print(sample['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d8f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_seq2seq_model(\n",
    "    checkpoint,\n",
    "    quantization_config\n",
    "):\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        checkpoint,\n",
    "        token=True,\n",
    "        quantization_config=quantization_config,\n",
    "        dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        # attn_implementation=\"flash_attention_2\"\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_kbit_training(\n",
    "        model,\n",
    "        use_gradient_checkpointing=False,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf2589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff3bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config_llama = GenerationConfig.from_pretrained(\n",
    "    checkpoint_llama,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1337915",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generation Config OPT:\", generation_config_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb29b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085d0ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model,\n",
    "    batch_tokenized_test,\n",
    "    tokenizer,\n",
    "    generation_config,\n",
    "    max_tok_len,\n",
    "    num_beams=1,\n",
    "):\n",
    "    model.eval()\n",
    "    input_sequences = []\n",
    "    preds_sequences = []\n",
    "    labels_sequences = []\n",
    "    \n",
    "    # end_token_id = tokenizer.convert_tokens_to_ids(\"<END>\")\n",
    "    \n",
    "    for batch in batch_tokenized_test:\n",
    "        input_ids = torch.tensor(batch[\"input_ids\"]).cuda()\n",
    "        attention_mask = torch.tensor(batch[\"attention_mask\"]).cuda()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                generation_config=generation_config,\n",
    "                max_new_tokens= max_tok_length,\n",
    "                # max_length= max_tok_len,\n",
    "                num_beams=num_beams,\n",
    "                do_sample=False,\n",
    "                # eos_token_id=end_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.2,\n",
    "                no_repeat_ngram_size=3,\n",
    "            )\n",
    "        \n",
    "        preds_sequences.extend(\n",
    "            tokenizer.batch_decode(\n",
    "                generated_ids, skip_special_tokens=True\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        input_sequences.extend(\n",
    "            tokenizer.batch_decode(\n",
    "                input_ids, skip_special_tokens=True\n",
    "            )\n",
    "        )      \n",
    "        \n",
    "        labels_sequences.extend(\n",
    "            tokenizer.batch_decode(\n",
    "                batch[\"labels\"], skip_special_tokens=True\n",
    "            )\n",
    "        )\n",
    "    return input_sequences, preds_sequences, labels_sequences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e397710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(sample, tokenizer):\n",
    "    inputs, preds, labels = sample\n",
    "\n",
    "    clean_preds = []\n",
    "\n",
    "    for input, pred in zip(inputs, preds):\n",
    "        text = pred.removeprefix(input).strip()\n",
    "\n",
    "        # if \"<END>\" in text:\n",
    "        #     clean_preds.append(text.split(\"<END>\")[0].strip())\n",
    "        # else:\n",
    "        #     clean_preds.append(text)\n",
    "        \n",
    "        if \"\\n\" in text:\n",
    "            clean_preds.append(text.split(\"\\n\")[0].strip())\n",
    "        else:\n",
    "            clean_preds.append(text)\n",
    "\n",
    "    results = {}\n",
    "    results[\"BLEU\"] = bleu_metric.compute(\n",
    "        predictions=clean_preds,\n",
    "        references=[[s] for s in labels]\n",
    "    )[\"score\"]\n",
    "    \n",
    "    results[\"rogueL\"] = rouge_metric.compute(\n",
    "        predictions=clean_preds,\n",
    "        references=labels\n",
    "    )[\"rougeL\"]\n",
    "    \n",
    "    results[\"COMET\"] = comet_metric.compute(\n",
    "        predictions=clean_preds,\n",
    "        references=labels,\n",
    "        sources = inputs\n",
    "    )[\"mean_score\"] * 100\n",
    "    \n",
    "    results[\"chrF\"] = chrf_metric.compute(\n",
    "        predictions=clean_preds,\n",
    "        references=labels\n",
    "    )[\"score\"]\n",
    "    \n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    results[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    results = {k: round(v, 4) for k, v in results.items()}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8193ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_seq2seq_model(\n",
    "    checkpoint_llama,\n",
    "    quantization_config\n",
    ")\n",
    "\n",
    "# model.resize_token_embeddings(len(tokenizer_llama))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716fb84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_list = []\n",
    "\n",
    "for model_name, base_cfg in {\n",
    "    \"llama_1shot_1beams\": (1, 1),\n",
    "    \"llama_1shot_4beams\": (1, 4),\n",
    "    \"llama_5shot_1beams\": (5, 1),\n",
    "    \"llama_5shot_4beams\": (5, 4),\n",
    "    \"llama_10shot_1beams\": (10, 1),\n",
    "    \"llama_10shot_4beams\": (10, 4),\n",
    "}.items():\n",
    "    num_shots, num_beams = base_cfg\n",
    "    cfg_list.append({\n",
    "        \"model_name\": model_name,\n",
    "        \"num_shots\": num_shots,\n",
    "        \"num_beams\": num_beams\n",
    "    })\n",
    "    \n",
    "results_list = []\n",
    "\n",
    "\n",
    "for cfg_item in cfg_list:\n",
    "    name = cfg_item[\"model_name\"]\n",
    "    num_shots = cfg_item[\"num_shots\"]\n",
    "    num_beams = cfg_item[\"num_beams\"]\n",
    "    print(f\"Evaluating model: {name} with num_beams={num_beams} and num_shots={num_shots}\")\n",
    "    \n",
    "    testset = opus_dataset_llama['test'].map(\n",
    "        lambda x: preprocess4test_function(x, task_prefix, tokenizer_llama, devset_llama, num_shots=num_shots),\n",
    "        batched=True\n",
    "    )\n",
    "\n",
    "    testset_batched = testset.batch(test_batch_size)\n",
    "    \n",
    "    _, max_len = get_test_max_tok_len(num_shots, task_prefix, tokenizer_llama)\n",
    "    \n",
    "    inputs, preds, labels = evaluate_model(model, testset_batched, tokenizer_llama, generation_config_llama, max_len, num_beams)\n",
    "    results = compute_metrics((inputs, preds, labels), tokenizer_llama)\n",
    "    results[\"model_name\"] = name\n",
    "    results[\"num_beams\"] = num_beams\n",
    "\n",
    "    results_list.append(results)\n",
    "    \n",
    "    print(results)\n",
    "        \n",
    "    del testset\n",
    "    del testset_batched\n",
    "    del inputs\n",
    "    del preds\n",
    "    del labels\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    gc.collect()\n",
    "\n",
    "df_results = pd.DataFrame(results_list)\n",
    "df_results.to_csv(\"results_llama_promting.csv\", index=False)\n",
    "print(\"Results saved to results_llama_promting.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
