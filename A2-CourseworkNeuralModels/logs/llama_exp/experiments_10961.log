Running experiment: llama
Python: /home/alumno.upv.es/gdipal1/envs/ta-project/bin/python
PyTorch version: 2.9.1+cu128
CUDA available: True
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/torchmetrics/utilities/imports.py:23: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import DistributionNotFound, get_distribution
Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]Fetching 5 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 15827.56it/s]
Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`
Encoder model frozen.
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']
DatasetDict({
    test: Dataset({
        features: ['translation'],
        num_rows: 2000
    })
    train: Dataset({
        features: ['translation'],
        num_rows: 1000000
    })
    validation: Dataset({
        features: ['translation'],
        num_rows: 2000
    })
})
Map (num_proc=8):   0%|          | 0/2000 [00:00<?, ? examples/s]Map (num_proc=8):  12%|â–ˆâ–Ž        | 250/2000 [00:00<00:04, 398.06 examples/s]Map (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:01<00:00, 1999.27 examples/s]
Map (num_proc=8):   0%|          | 0/1000000 [00:00<?, ? examples/s]Map (num_proc=8):   0%|          | 1000/1000000 [00:00<11:14, 1481.20 examples/s]Map (num_proc=8):   1%|          | 10000/1000000 [00:00<01:00, 16314.61 examples/s]Map (num_proc=8):   2%|â–         | 22000/1000000 [00:00<00:27, 35764.05 examples/s]Map (num_proc=8):   3%|â–Ž         | 33000/1000000 [00:01<00:18, 51554.59 examples/s]Map (num_proc=8):   4%|â–         | 42000/1000000 [00:01<00:16, 59358.26 examples/s]Map (num_proc=8):   5%|â–Œ         | 52000/1000000 [00:01<00:13, 69242.99 examples/s]Map (num_proc=8):   6%|â–Œ         | 62000/1000000 [00:01<00:12, 74649.07 examples/s]Map (num_proc=8):   7%|â–‹         | 73000/1000000 [00:01<00:11, 82879.62 examples/s]Map (num_proc=8):   8%|â–Š         | 83000/1000000 [00:01<00:17, 51116.85 examples/s]Map (num_proc=8):   9%|â–‰         | 91000/1000000 [00:01<00:18, 49176.96 examples/s]Map (num_proc=8):  10%|â–‰         | 98000/1000000 [00:02<00:17, 50184.06 examples/s]Map (num_proc=8):  10%|â–ˆ         | 105000/1000000 [00:02<00:17, 52316.42 examples/s]Map (num_proc=8):  12%|â–ˆâ–        | 115000/1000000 [00:02<00:14, 61467.66 examples/s]Map (num_proc=8):  13%|â–ˆâ–Ž        | 126000/1000000 [00:02<00:12, 71339.53 examples/s]Map (num_proc=8):  14%|â–ˆâ–        | 138000/1000000 [00:02<00:10, 81557.86 examples/s]Map (num_proc=8):  15%|â–ˆâ–        | 147000/1000000 [00:02<00:10, 81685.52 examples/s]Map (num_proc=8):  16%|â–ˆâ–Œ        | 157000/1000000 [00:02<00:09, 84649.76 examples/s]Map (num_proc=8):  17%|â–ˆâ–‹        | 167000/1000000 [00:02<00:09, 88354.25 examples/s]Map (num_proc=8):  18%|â–ˆâ–Š        | 177000/1000000 [00:02<00:09, 90418.60 examples/s]Map (num_proc=8):  19%|â–ˆâ–‰        | 189000/1000000 [00:03<00:08, 92474.94 examples/s]Map (num_proc=8):  20%|â–ˆâ–‰        | 199000/1000000 [00:03<00:09, 85622.11 examples/s]Map (num_proc=8):  21%|â–ˆâ–ˆ        | 208000/1000000 [00:03<00:09, 82924.03 examples/s]Map (num_proc=8):  22%|â–ˆâ–ˆâ–       | 217000/1000000 [00:03<00:10, 77836.77 examples/s]Map (num_proc=8):  23%|â–ˆâ–ˆâ–Ž       | 227000/1000000 [00:03<00:09, 82622.61 examples/s]Map (num_proc=8):  24%|â–ˆâ–ˆâ–Ž       | 236000/1000000 [00:03<00:09, 77208.75 examples/s]Map (num_proc=8):  24%|â–ˆâ–ˆâ–       | 244000/1000000 [00:03<00:09, 76842.66 examples/s]Map (num_proc=8):  25%|â–ˆâ–ˆâ–Œ       | 253000/1000000 [00:03<00:09, 77976.28 examples/s]Map (num_proc=8):  26%|â–ˆâ–ˆâ–‹       | 264000/1000000 [00:04<00:08, 84641.12 examples/s]Map (num_proc=8):  28%|â–ˆâ–ˆâ–Š       | 275000/1000000 [00:04<00:07, 91322.53 examples/s]Map (num_proc=8):  29%|â–ˆâ–ˆâ–Š       | 286000/1000000 [00:04<00:07, 95700.16 examples/s]Map (num_proc=8):  30%|â–ˆâ–ˆâ–‰       | 296000/1000000 [00:04<00:07, 89159.39 examples/s]Map (num_proc=8):  31%|â–ˆâ–ˆâ–ˆ       | 307000/1000000 [00:04<00:07, 91227.28 examples/s]Map (num_proc=8):  32%|â–ˆâ–ˆâ–ˆâ–      | 319000/1000000 [00:04<00:07, 93421.14 examples/s]Map (num_proc=8):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 330000/1000000 [00:04<00:07, 88773.48 examples/s]Map (num_proc=8):  34%|â–ˆâ–ˆâ–ˆâ–      | 339000/1000000 [00:04<00:07, 85524.33 examples/s]Map (num_proc=8):  35%|â–ˆâ–ˆâ–ˆâ–      | 348000/1000000 [00:05<00:07, 81965.69 examples/s]Map (num_proc=8):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 359000/1000000 [00:05<00:07, 82200.08 examples/s]Map (num_proc=8):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 368000/1000000 [00:05<00:08, 75636.27 examples/s]Map (num_proc=8):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 377000/1000000 [00:05<00:07, 78879.21 examples/s]Map (num_proc=8):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 386000/1000000 [00:05<00:08, 74230.44 examples/s]Map (num_proc=8):  40%|â–ˆâ–ˆâ–ˆâ–‰      | 395000/1000000 [00:05<00:07, 76025.45 examples/s]Map (num_proc=8):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 403000/1000000 [00:05<00:07, 76786.37 examples/s]Map (num_proc=8):  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 413000/1000000 [00:05<00:07, 81239.08 examples/s]Map (num_proc=8):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 423000/1000000 [00:05<00:06, 83690.47 examples/s]Map (num_proc=8):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 435000/1000000 [00:06<00:06, 87528.22 examples/s]Map (num_proc=8):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 447000/1000000 [00:06<00:05, 93384.34 examples/s]Map (num_proc=8):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 457000/1000000 [00:06<00:05, 93490.61 examples/s]Map (num_proc=8):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 467000/1000000 [00:06<00:06, 87409.44 examples/s]Map (num_proc=8):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 477000/1000000 [00:06<00:05, 87236.08 examples/s]Map (num_proc=8):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 486000/1000000 [00:06<00:06, 80415.75 examples/s]Map (num_proc=8):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 497000/1000000 [00:06<00:05, 84647.92 examples/s]Map (num_proc=8):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 506000/1000000 [00:06<00:06, 82030.24 examples/s]Map (num_proc=8):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 517000/1000000 [00:07<00:05, 89371.78 examples/s]Map (num_proc=8):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 528000/1000000 [00:07<00:05, 85866.68 examples/s]Map (num_proc=8):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 538000/1000000 [00:07<00:05, 89073.23 examples/s]Map (num_proc=8):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 548000/1000000 [00:07<00:05, 81384.31 examples/s]Map (num_proc=8):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 560000/1000000 [00:07<00:04, 89182.33 examples/s]Map (num_proc=8):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 572000/1000000 [00:07<00:04, 89190.66 examples/s]Map (num_proc=8):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 584000/1000000 [00:07<00:04, 94583.88 examples/s]Map (num_proc=8):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 595000/1000000 [00:07<00:04, 89477.25 examples/s]Map (num_proc=8):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 605000/1000000 [00:08<00:04, 91338.21 examples/s]Map (num_proc=8):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 615000/1000000 [00:08<00:04, 91249.03 examples/s]Map (num_proc=8):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 625000/1000000 [00:08<00:04, 85473.51 examples/s]Map (num_proc=8):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 634000/1000000 [00:08<00:04, 86193.71 examples/s]Map (num_proc=8):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 643000/1000000 [00:08<00:04, 80082.35 examples/s]Map (num_proc=8):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 655000/1000000 [00:08<00:03, 88329.40 examples/s]Map (num_proc=8):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 665000/1000000 [00:08<00:03, 84464.53 examples/s]Map (num_proc=8):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 676000/1000000 [00:08<00:03, 85870.12 examples/s]Map (num_proc=8):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 685000/1000000 [00:08<00:03, 84037.74 examples/s]Map (num_proc=8):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 696000/1000000 [00:09<00:03, 84794.45 examples/s]Map (num_proc=8):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 706000/1000000 [00:09<00:03, 86872.58 examples/s]Map (num_proc=8):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 715000/1000000 [00:09<00:03, 86903.78 examples/s]Map (num_proc=8):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 726000/1000000 [00:09<00:03, 90690.19 examples/s]Map (num_proc=8):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 736000/1000000 [00:09<00:02, 92406.42 examples/s]Map (num_proc=8):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 746000/1000000 [00:09<00:02, 89691.72 examples/s]Map (num_proc=8):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 756000/1000000 [00:09<00:02, 86942.15 examples/s]Map (num_proc=8):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 765000/1000000 [00:09<00:02, 85669.12 examples/s]Map (num_proc=8):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 774000/1000000 [00:09<00:02, 79912.11 examples/s]Map (num_proc=8):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 783000/1000000 [00:10<00:02, 82048.24 examples/s]Map (num_proc=8):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 792000/1000000 [00:10<00:02, 82489.03 examples/s]Map (num_proc=8):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 802000/1000000 [00:10<00:02, 86055.34 examples/s]Map (num_proc=8):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 813000/1000000 [00:10<00:02, 88040.36 examples/s]Map (num_proc=8):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 822000/1000000 [00:10<00:02, 83561.71 examples/s]Map (num_proc=8):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 832000/1000000 [00:10<00:01, 87829.33 examples/s]Map (num_proc=8):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 844000/1000000 [00:10<00:01, 94550.76 examples/s]Map (num_proc=8):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 854000/1000000 [00:10<00:01, 92063.31 examples/s]Map (num_proc=8):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 864000/1000000 [00:10<00:01, 89154.29 examples/s]Map (num_proc=8):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 873000/1000000 [00:11<00:01, 84222.72 examples/s]Map (num_proc=8):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 883000/1000000 [00:11<00:01, 86746.89 examples/s]Map (num_proc=8):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 892000/1000000 [00:11<00:01, 87327.18 examples/s]Map (num_proc=8):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 901000/1000000 [00:11<00:01, 83584.85 examples/s]Map (num_proc=8):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 910000/1000000 [00:11<00:01, 80100.47 examples/s]Map (num_proc=8):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 921000/1000000 [00:11<00:00, 86657.76 examples/s]Map (num_proc=8):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 931000/1000000 [00:11<00:00, 84259.32 examples/s]Map (num_proc=8):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 944000/1000000 [00:11<00:00, 88323.78 examples/s]Map (num_proc=8):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 953000/1000000 [00:12<00:00, 85293.51 examples/s]Map (num_proc=8):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 962000/1000000 [00:12<00:00, 76154.26 examples/s]Map (num_proc=8):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 970000/1000000 [00:12<00:00, 76054.05 examples/s]Map (num_proc=8):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 978000/1000000 [00:12<00:00, 67671.57 examples/s]Map (num_proc=8):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 985000/1000000 [00:12<00:00, 54434.01 examples/s]Map (num_proc=8):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 991000/1000000 [00:12<00:00, 48623.52 examples/s]Map (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 997000/1000000 [00:13<00:00, 34455.26 examples/s]Map (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000000/1000000 [00:13<00:00, 72442.31 examples/s]
Map (num_proc=8):   0%|          | 0/2000 [00:00<?, ? examples/s]Map (num_proc=8):  12%|â–ˆâ–Ž        | 250/2000 [00:00<00:04, 376.36 examples/s]Map (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:01<00:00, 1966.28 examples/s]
Discarding source and target sentences with more than 24 tokens (num_proc=8):   0%|          | 0/2000 [00:00<?, ? examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  12%|â–ˆâ–Ž        | 250/2000 [00:00<00:04, 377.67 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:01<00:00, 1927.35 examples/s]
Discarding source and target sentences with more than 24 tokens (num_proc=8):   0%|          | 0/1000000 [00:00<?, ? examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):   0%|          | 2000/1000000 [00:00<05:52, 2833.09 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):   3%|â–Ž         | 34000/1000000 [00:00<00:17, 54227.43 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):   8%|â–Š         | 82000/1000000 [00:00<00:07, 128396.04 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  13%|â–ˆâ–Ž        | 130000/1000000 [00:01<00:04, 189706.28 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  18%|â–ˆâ–Š        | 175000/1000000 [00:01<00:03, 242995.19 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  22%|â–ˆâ–ˆâ–       | 217000/1000000 [00:01<00:02, 281486.09 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  26%|â–ˆâ–ˆâ–Œ       | 256000/1000000 [00:01<00:02, 291901.71 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  30%|â–ˆâ–ˆâ–‰       | 298000/1000000 [00:01<00:02, 310619.43 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  34%|â–ˆâ–ˆâ–ˆâ–      | 343000/1000000 [00:01<00:01, 344947.68 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 384000/1000000 [00:01<00:01, 345792.08 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 422000/1000000 [00:01<00:01, 347685.00 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 461000/1000000 [00:01<00:01, 359045.59 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 499000/1000000 [00:02<00:01, 343923.70 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 544000/1000000 [00:02<00:01, 372604.02 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 583000/1000000 [00:02<00:01, 363028.25 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 621000/1000000 [00:02<00:01, 350052.91 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 666000/1000000 [00:02<00:00, 368749.44 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 704000/1000000 [00:02<00:00, 369338.08 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 743000/1000000 [00:02<00:00, 350925.05 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 787000/1000000 [00:02<00:00, 364377.81 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 825000/1000000 [00:02<00:00, 367321.61 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 864000/1000000 [00:03<00:00, 360354.99 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 905000/1000000 [00:03<00:00, 361923.49 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 942000/1000000 [00:03<00:00, 226908.87 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 972000/1000000 [00:03<00:00, 240045.03 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000000/1000000 [00:04<00:00, 249264.74 examples/s]
Discarding source and target sentences with more than 24 tokens (num_proc=8):   0%|          | 0/2000 [00:00<?, ? examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  12%|â–ˆâ–Ž        | 250/2000 [00:00<00:04, 392.92 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 2004.87 examples/s]
 2 2029
 3 16501
 4 31012
 5 38432
 6 47687
 7 51710
 8 52256
 9 49135
10 45485
11 41084
12 36289
13 31722
14 27407
15 22927
16 19664
17 16083
18 13190
19 10946
20 8317
21 6376
22 4758
23 3610
24 2776
Map:   0%|          | 0/982 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 982/982 [00:00<00:00, 8361.79 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 982/982 [00:00<00:00, 8064.07 examples/s]
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 4103, 9632, 515, 4223, 304, 5176, 29901, 13, 24636, 29901, 2688, 1539, 1728, 592, 29889, 353, 5176, 29901, 29871, 1, 16932, 409, 3435, 4325, 9996, 743, 7209, 19601, 29889, 13, 2]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1, 16932, 409, 3435, 4325, 9996, 743, 7209, 19601, 29889, 13, 2]
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 4103, 9632, 515, 4223, 304, 5176, 29901, 13, 24636, 29901, 14617, 274, 1161, 4723, 869, 353, 5176, 29901, 29871, 1, 435, 29915, 1794, 5146, 29948, 427, 15617, 680, 1671, 425, 269, 2603, 457, 29889, 13, 2]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1, 435, 29915, 1794, 5146, 29948, 427, 15617, 680, 1671, 425, 269, 2603, 457, 29889, 13, 2]
[2, 2, 2, 2, 2, 2, 2, 2, 1, 4103, 9632, 515, 4223, 304, 5176, 29901, 13, 24636, 29901, 450, 916, 2228, 29892, 607, 338, 310, 18886, 681, 26002, 29892, 338, 278, 1492, 304, 278, 325, 10896, 29889, 353, 5176, 29901, 29871, 1, 365, 29915, 18288, 1139, 29892, 270, 29915, 1540, 2011, 1318, 904, 12324, 29872, 29892, 707, 454, 17950, 316, 325, 10896, 29889, 13, 2]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1, 365, 29915, 18288, 1139, 29892, 270, 29915, 1540, 2011, 1318, 904, 12324, 29872, 29892, 707, 454, 17950, 316, 325, 10896, 29889, 13, 2]
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 4103, 9632, 515, 4223, 304, 5176, 29901, 13, 24636, 29901, 1724, 29892, 278, 1190, 9102, 4135, 29973, 353, 5176, 29901, 29871, 1, 751, 7768, 29892, 425, 21428, 14931, 1509, 1577, 13, 2]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1, 751, 7768, 29892, 425, 21428, 14931, 1509, 1577, 13, 2]
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 4103, 9632, 515, 4223, 304, 5176, 29901, 13, 24636, 29901, 1128, 1048, 27697, 11090, 389, 29874, 18864, 278, 4723, 355, 411, 502, 29973, 353, 5176, 29901, 29871, 1, 8748, 1354, 7890, 425, 790, 26477, 11090, 493, 29874, 27962, 454, 4723, 29899, 355, 2535, 8556, 1577, 13, 2]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1, 8748, 1354, 7890, 425, 790, 26477, 11090, 493, 29874, 27962, 454, 4723, 29899, 355, 2535, 8556, 1577, 13, 2]
Generation Config OPT: GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.18s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.37s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.79s/it]
Evaluating model: llama_1shot_1beams with num_beams=1 and num_shots=1
Map:   0%|          | 0/976 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 976/976 [00:00<00:00, 8421.67 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 976/976 [00:00<00:00, 8210.67 examples/s]
Batching examples:   0%|          | 0/976 [00:00<?, ? examples/s]Batching examples:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 524/976 [00:00<00:00, 5166.39 examples/s]Batching examples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 976/976 [00:00<00:00, 5112.88 examples/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Using default tokenizer.
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python experiments_llama.py ...
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
You are using a CUDA device ('NVIDIA L40S') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
{'BLEU': 8.4476, 'rogueL': 0.2645, 'COMET': 58.8205, 'chrF': 33.4474, 'gen_len': 1.0, 'model_name': 'llama_1shot_1beams', 'num_beams': 1}
Evaluating model: llama_1shot_4beams with num_beams=4 and num_shots=1
Using default tokenizer.
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python experiments_llama.py ...
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
{'BLEU': 12.9064, 'rogueL': 0.3327, 'COMET': 62.1615, 'chrF': 39.2587, 'gen_len': 1.0, 'model_name': 'llama_1shot_4beams', 'num_beams': 4}
Evaluating model: llama_5shot_1beams with num_beams=1 and num_shots=5
Map:   0%|          | 0/976 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 976/976 [00:00<00:00, 4758.92 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 976/976 [00:00<00:00, 4674.72 examples/s]
Batching examples:   0%|          | 0/976 [00:00<?, ? examples/s]Batching examples:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 408/976 [00:00<00:00, 4014.54 examples/s]Batching examples:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 820/976 [00:00<00:00, 4029.83 examples/s]Batching examples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 976/976 [00:00<00:00, 3944.13 examples/s]
Using default tokenizer.
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python experiments_llama.py ...
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
{'BLEU': 5.8097, 'rogueL': 0.1981, 'COMET': 54.1884, 'chrF': 26.9265, 'gen_len': 1.0, 'model_name': 'llama_5shot_1beams', 'num_beams': 1}
Evaluating model: llama_5shot_4beams with num_beams=4 and num_shots=5
Using default tokenizer.
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python experiments_llama.py ...
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
{'BLEU': 10.9747, 'rogueL': 0.2857, 'COMET': 58.6041, 'chrF': 34.741, 'gen_len': 1.0, 'model_name': 'llama_5shot_4beams', 'num_beams': 4}
Evaluating model: llama_10shot_1beams with num_beams=1 and num_shots=10
Map:   0%|          | 0/976 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 976/976 [00:00<00:00, 2112.76 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 976/976 [00:00<00:00, 2091.16 examples/s]
Batching examples:   0%|          | 0/976 [00:00<?, ? examples/s]Batching examples:  27%|â–ˆâ–ˆâ–‹       | 268/976 [00:00<00:00, 2664.20 examples/s]Batching examples:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 540/976 [00:00<00:00, 2666.60 examples/s]Batching examples:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 824/976 [00:00<00:00, 2734.03 examples/s]Batching examples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 976/976 [00:00<00:00, 2687.37 examples/s]
Using default tokenizer.
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python experiments_llama.py ...
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
{'BLEU': 6.6348, 'rogueL': 0.2259, 'COMET': 59.4581, 'chrF': 28.4411, 'gen_len': 1.0, 'model_name': 'llama_10shot_1beams', 'num_beams': 1}
Evaluating model: llama_10shot_4beams with num_beams=4 and num_shots=10
Using default tokenizer.
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python experiments_llama.py ...
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
{'BLEU': 12.8898, 'rogueL': 0.3245, 'COMET': 63.6231, 'chrF': 38.0228, 'gen_len': 1.0, 'model_name': 'llama_10shot_4beams', 'num_beams': 4}
Results saved to results_llama_promting.csv
