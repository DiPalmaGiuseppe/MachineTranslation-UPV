Running experiment: opt
Python: /home/alumno.upv.es/gdipal1/envs/ta-project/bin/python
PyTorch version: 2.9.1+cu128
CUDA available: True
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/torchmetrics/utilities/imports.py:23: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import DistributionNotFound, get_distribution
Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 93206.76it/s]
Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`
Encoder model frozen.
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']
DatasetDict({
    test: Dataset({
        features: ['translation'],
        num_rows: 2000
    })
    train: Dataset({
        features: ['translation'],
        num_rows: 1000000
    })
    validation: Dataset({
        features: ['translation'],
        num_rows: 2000
    })
})
Map (num_proc=8):   0%|          | 0/2000 [00:00<?, ? examples/s]Map (num_proc=8):  12%|█▎        | 250/2000 [00:00<00:04, 387.78 examples/s]Map (num_proc=8): 100%|██████████| 2000/2000 [00:01<00:00, 1991.02 examples/s]
Map (num_proc=8):   0%|          | 0/1000000 [00:00<?, ? examples/s]Map (num_proc=8):   0%|          | 1000/1000000 [00:00<11:26, 1454.21 examples/s]Map (num_proc=8):   1%|          | 10000/1000000 [00:00<01:04, 15359.50 examples/s]Map (num_proc=8):   2%|▏         | 18000/1000000 [00:00<00:36, 27225.78 examples/s]Map (num_proc=8):   3%|▎         | 27000/1000000 [00:01<00:24, 40057.32 examples/s]Map (num_proc=8):   4%|▍         | 38000/1000000 [00:01<00:17, 55995.28 examples/s]Map (num_proc=8):   5%|▍         | 47000/1000000 [00:01<00:15, 61918.70 examples/s]Map (num_proc=8):   6%|▌         | 56000/1000000 [00:01<00:14, 67320.80 examples/s]Map (num_proc=8):   6%|▋         | 65000/1000000 [00:01<00:13, 70520.78 examples/s]Map (num_proc=8):   7%|▋         | 74000/1000000 [00:01<00:12, 73361.37 examples/s]Map (num_proc=8):   8%|▊         | 83000/1000000 [00:01<00:20, 45061.97 examples/s]Map (num_proc=8):   9%|▉         | 90000/1000000 [00:02<00:19, 45947.40 examples/s]Map (num_proc=8):  10%|▉         | 96000/1000000 [00:02<00:19, 46860.66 examples/s]Map (num_proc=8):  10%|█         | 102000/1000000 [00:02<00:18, 47364.02 examples/s]Map (num_proc=8):  11%|█         | 109000/1000000 [00:02<00:17, 50878.25 examples/s]Map (num_proc=8):  12%|█▏        | 119000/1000000 [00:02<00:14, 61173.40 examples/s]Map (num_proc=8):  13%|█▎        | 128000/1000000 [00:02<00:12, 67611.68 examples/s]Map (num_proc=8):  14%|█▎        | 136000/1000000 [00:02<00:12, 70148.48 examples/s]Map (num_proc=8):  14%|█▍        | 144000/1000000 [00:02<00:11, 72631.93 examples/s]Map (num_proc=8):  15%|█▌        | 152000/1000000 [00:02<00:11, 73753.13 examples/s]Map (num_proc=8):  16%|█▌        | 160000/1000000 [00:03<00:11, 75288.09 examples/s]Map (num_proc=8):  17%|█▋        | 168000/1000000 [00:03<00:10, 76620.57 examples/s]Map (num_proc=8):  18%|█▊        | 178000/1000000 [00:03<00:10, 76364.69 examples/s]Map (num_proc=8):  19%|█▊        | 186000/1000000 [00:03<00:10, 77156.87 examples/s]Map (num_proc=8):  20%|█▉        | 195000/1000000 [00:03<00:10, 80014.96 examples/s]Map (num_proc=8):  20%|██        | 204000/1000000 [00:03<00:09, 81855.12 examples/s]Map (num_proc=8):  21%|██▏       | 213000/1000000 [00:03<00:09, 83080.84 examples/s]Map (num_proc=8):  22%|██▏       | 222000/1000000 [00:03<00:10, 73836.65 examples/s]Map (num_proc=8):  23%|██▎       | 230000/1000000 [00:04<00:11, 64831.15 examples/s]Map (num_proc=8):  24%|██▎       | 237000/1000000 [00:04<00:12, 61017.15 examples/s]Map (num_proc=8):  24%|██▍       | 245000/1000000 [00:04<00:11, 64984.90 examples/s]Map (num_proc=8):  25%|██▌       | 253000/1000000 [00:04<00:10, 68271.89 examples/s]Map (num_proc=8):  26%|██▌       | 261000/1000000 [00:04<00:10, 70205.87 examples/s]Map (num_proc=8):  27%|██▋       | 271000/1000000 [00:04<00:09, 78042.70 examples/s]Map (num_proc=8):  28%|██▊       | 280000/1000000 [00:04<00:09, 74752.25 examples/s]Map (num_proc=8):  29%|██▉       | 289000/1000000 [00:04<00:09, 71378.81 examples/s]Map (num_proc=8):  30%|██▉       | 299000/1000000 [00:04<00:08, 78486.61 examples/s]Map (num_proc=8):  31%|███       | 308000/1000000 [00:05<00:08, 81218.89 examples/s]Map (num_proc=8):  32%|███▏      | 317000/1000000 [00:05<00:08, 81811.28 examples/s]Map (num_proc=8):  33%|███▎      | 326000/1000000 [00:05<00:08, 83908.49 examples/s]Map (num_proc=8):  34%|███▎      | 335000/1000000 [00:05<00:07, 84716.84 examples/s]Map (num_proc=8):  34%|███▍      | 344000/1000000 [00:05<00:08, 80323.52 examples/s]Map (num_proc=8):  35%|███▌      | 353000/1000000 [00:05<00:07, 81869.89 examples/s]Map (num_proc=8):  36%|███▌      | 362000/1000000 [00:05<00:10, 61565.76 examples/s]Map (num_proc=8):  37%|███▋      | 369000/1000000 [00:05<00:09, 63327.95 examples/s]Map (num_proc=8):  38%|███▊      | 377000/1000000 [00:06<00:09, 67200.53 examples/s]Map (num_proc=8):  39%|███▊      | 386000/1000000 [00:06<00:08, 70946.99 examples/s]Map (num_proc=8):  39%|███▉      | 394000/1000000 [00:06<00:09, 64164.15 examples/s]Map (num_proc=8):  40%|████      | 402000/1000000 [00:06<00:08, 67810.38 examples/s]Map (num_proc=8):  41%|████      | 411000/1000000 [00:06<00:08, 72248.39 examples/s]Map (num_proc=8):  42%|████▏     | 422000/1000000 [00:06<00:07, 81735.98 examples/s]Map (num_proc=8):  43%|████▎     | 431000/1000000 [00:06<00:07, 79265.02 examples/s]Map (num_proc=8):  44%|████▍     | 440000/1000000 [00:06<00:06, 80285.40 examples/s]Map (num_proc=8):  45%|████▍     | 449000/1000000 [00:06<00:07, 77175.39 examples/s]Map (num_proc=8):  46%|████▌     | 457000/1000000 [00:07<00:07, 77556.67 examples/s]Map (num_proc=8):  47%|████▋     | 466000/1000000 [00:07<00:06, 80265.33 examples/s]Map (num_proc=8):  48%|████▊     | 475000/1000000 [00:07<00:06, 81307.47 examples/s]Map (num_proc=8):  48%|████▊     | 485000/1000000 [00:07<00:06, 83763.85 examples/s]Map (num_proc=8):  49%|████▉     | 494000/1000000 [00:07<00:06, 72453.97 examples/s]Map (num_proc=8):  50%|█████     | 502000/1000000 [00:07<00:08, 59829.53 examples/s]Map (num_proc=8):  51%|█████     | 511000/1000000 [00:07<00:07, 66425.17 examples/s]Map (num_proc=8):  52%|█████▏    | 521000/1000000 [00:07<00:06, 71785.69 examples/s]Map (num_proc=8):  53%|█████▎    | 529000/1000000 [00:08<00:06, 72265.22 examples/s]Map (num_proc=8):  54%|█████▎    | 537000/1000000 [00:08<00:07, 65562.04 examples/s]Map (num_proc=8):  55%|█████▍    | 546000/1000000 [00:08<00:06, 71113.73 examples/s]Map (num_proc=8):  56%|█████▌    | 555000/1000000 [00:08<00:05, 75735.49 examples/s]Map (num_proc=8):  56%|█████▋    | 565000/1000000 [00:08<00:05, 81287.78 examples/s]Map (num_proc=8):  57%|█████▋    | 574000/1000000 [00:08<00:05, 73196.57 examples/s]Map (num_proc=8):  58%|█████▊    | 582000/1000000 [00:08<00:05, 74906.17 examples/s]Map (num_proc=8):  59%|█████▉    | 591000/1000000 [00:08<00:05, 78605.86 examples/s]Map (num_proc=8):  60%|██████    | 600000/1000000 [00:09<00:05, 78166.15 examples/s]Map (num_proc=8):  61%|██████    | 609000/1000000 [00:09<00:04, 81123.38 examples/s]Map (num_proc=8):  62%|██████▏   | 619000/1000000 [00:09<00:04, 84997.75 examples/s]Map (num_proc=8):  63%|██████▎   | 628000/1000000 [00:09<00:04, 85429.00 examples/s]Map (num_proc=8):  64%|██████▎   | 637000/1000000 [00:09<00:04, 83231.89 examples/s]Map (num_proc=8):  65%|██████▍   | 646000/1000000 [00:09<00:05, 69705.74 examples/s]Map (num_proc=8):  65%|██████▌   | 654000/1000000 [00:09<00:05, 62556.63 examples/s]Map (num_proc=8):  66%|██████▌   | 661000/1000000 [00:09<00:05, 64021.55 examples/s]Map (num_proc=8):  67%|██████▋   | 671000/1000000 [00:10<00:04, 71964.19 examples/s]Map (num_proc=8):  68%|██████▊   | 680000/1000000 [00:10<00:04, 71947.42 examples/s]Map (num_proc=8):  69%|██████▉   | 688000/1000000 [00:10<00:04, 67573.11 examples/s]Map (num_proc=8):  70%|██████▉   | 696000/1000000 [00:10<00:04, 67630.41 examples/s]Map (num_proc=8):  70%|███████   | 705000/1000000 [00:10<00:04, 73279.53 examples/s]Map (num_proc=8):  71%|███████▏  | 713000/1000000 [00:10<00:03, 74944.27 examples/s]Map (num_proc=8):  72%|███████▏  | 722000/1000000 [00:10<00:03, 76625.64 examples/s]Map (num_proc=8):  73%|███████▎  | 730000/1000000 [00:10<00:03, 77373.53 examples/s]Map (num_proc=8):  74%|███████▍  | 738000/1000000 [00:10<00:03, 76428.45 examples/s]Map (num_proc=8):  75%|███████▍  | 747000/1000000 [00:11<00:03, 79568.01 examples/s]Map (num_proc=8):  76%|███████▌  | 756000/1000000 [00:11<00:03, 78063.31 examples/s]Map (num_proc=8):  76%|███████▋  | 764000/1000000 [00:11<00:03, 76789.22 examples/s]Map (num_proc=8):  77%|███████▋  | 773000/1000000 [00:11<00:02, 80134.67 examples/s]Map (num_proc=8):  78%|███████▊  | 782000/1000000 [00:11<00:03, 69088.45 examples/s]Map (num_proc=8):  79%|███████▉  | 790000/1000000 [00:11<00:03, 64122.16 examples/s]Map (num_proc=8):  80%|███████▉  | 799000/1000000 [00:11<00:03, 63165.05 examples/s]Map (num_proc=8):  81%|████████  | 808000/1000000 [00:11<00:02, 68701.56 examples/s]Map (num_proc=8):  82%|████████▏ | 817000/1000000 [00:12<00:02, 73810.68 examples/s]Map (num_proc=8):  82%|████████▎ | 825000/1000000 [00:12<00:02, 74546.02 examples/s]Map (num_proc=8):  83%|████████▎ | 833000/1000000 [00:12<00:02, 73847.05 examples/s]Map (num_proc=8):  84%|████████▍ | 843000/1000000 [00:12<00:02, 72292.41 examples/s]Map (num_proc=8):  85%|████████▌ | 851000/1000000 [00:12<00:02, 73838.70 examples/s]Map (num_proc=8):  86%|████████▌ | 860000/1000000 [00:12<00:01, 77636.79 examples/s]Map (num_proc=8):  87%|████████▋ | 869000/1000000 [00:12<00:01, 78177.37 examples/s]Map (num_proc=8):  88%|████████▊ | 879000/1000000 [00:12<00:01, 84163.89 examples/s]Map (num_proc=8):  89%|████████▉ | 888000/1000000 [00:12<00:01, 82827.39 examples/s]Map (num_proc=8):  90%|████████▉ | 898000/1000000 [00:13<00:01, 86658.69 examples/s]Map (num_proc=8):  91%|█████████ | 907000/1000000 [00:13<00:01, 79109.28 examples/s]Map (num_proc=8):  92%|█████████▏| 916000/1000000 [00:13<00:01, 74244.90 examples/s]Map (num_proc=8):  92%|█████████▏| 924000/1000000 [00:13<00:01, 63187.36 examples/s]Map (num_proc=8):  93%|█████████▎| 933000/1000000 [00:13<00:00, 67322.13 examples/s]Map (num_proc=8):  94%|█████████▍| 942000/1000000 [00:13<00:00, 68052.60 examples/s]Map (num_proc=8):  95%|█████████▌| 951000/1000000 [00:13<00:00, 71556.03 examples/s]Map (num_proc=8):  96%|█████████▌| 959000/1000000 [00:13<00:00, 72447.23 examples/s]Map (num_proc=8):  97%|█████████▋| 967000/1000000 [00:14<00:00, 71744.43 examples/s]Map (num_proc=8):  98%|█████████▊| 975000/1000000 [00:14<00:00, 73097.72 examples/s]Map (num_proc=8):  98%|█████████▊| 984000/1000000 [00:14<00:00, 77655.67 examples/s]Map (num_proc=8):  99%|█████████▉| 992000/1000000 [00:14<00:00, 50180.53 examples/s]Map (num_proc=8): 100%|█████████▉| 999000/1000000 [00:14<00:00, 35531.32 examples/s]Map (num_proc=8): 100%|██████████| 1000000/1000000 [00:15<00:00, 65043.67 examples/s]
Map (num_proc=8):   0%|          | 0/2000 [00:00<?, ? examples/s]Map (num_proc=8):  12%|█▎        | 250/2000 [00:00<00:04, 365.33 examples/s]Map (num_proc=8): 100%|██████████| 2000/2000 [00:01<00:00, 1933.97 examples/s]
Discarding source and target sentences with more than 24 tokens (num_proc=8):   0%|          | 0/2000 [00:00<?, ? examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  12%|█▎        | 250/2000 [00:00<00:04, 395.16 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8): 100%|██████████| 2000/2000 [00:01<00:00, 1989.93 examples/s]
Discarding source and target sentences with more than 24 tokens (num_proc=8):   0%|          | 0/1000000 [00:00<?, ? examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):   0%|          | 1000/1000000 [00:00<11:24, 1459.33 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):   3%|▎         | 32000/1000000 [00:00<00:18, 52304.64 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):   8%|▊         | 80000/1000000 [00:00<00:07, 127331.35 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  13%|█▎        | 128000/1000000 [00:01<00:04, 189289.97 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  18%|█▊        | 176000/1000000 [00:01<00:03, 239272.69 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  22%|██▏       | 224000/1000000 [00:01<00:02, 275492.95 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  27%|██▋       | 272000/1000000 [00:01<00:02, 302693.96 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  32%|███▏      | 320000/1000000 [00:01<00:02, 325526.75 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  37%|███▋      | 368000/1000000 [00:01<00:01, 340106.75 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  41%|████      | 410000/1000000 [00:01<00:01, 357536.67 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  45%|████▌     | 451000/1000000 [00:01<00:01, 362231.85 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  49%|████▉     | 489000/1000000 [00:02<00:01, 353559.30 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  53%|█████▎    | 531000/1000000 [00:02<00:01, 352888.39 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  58%|█████▊    | 576000/1000000 [00:02<00:01, 372128.74 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  62%|██████▏   | 615000/1000000 [00:02<00:01, 367993.64 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  65%|██████▌   | 653000/1000000 [00:02<00:00, 362287.09 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  70%|██████▉   | 695000/1000000 [00:02<00:00, 369257.13 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  73%|███████▎  | 733000/1000000 [00:02<00:00, 369050.59 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  77%|███████▋  | 772000/1000000 [00:02<00:00, 374177.69 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  81%|████████  | 811000/1000000 [00:02<00:00, 356833.46 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  86%|████████▌ | 856000/1000000 [00:03<00:00, 366818.89 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  89%|████████▉ | 893000/1000000 [00:03<00:00, 233088.58 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  92%|█████████▎| 925000/1000000 [00:03<00:00, 248081.58 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  97%|█████████▋| 967000/1000000 [00:03<00:00, 277261.07 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8): 100%|██████████| 1000000/1000000 [00:04<00:00, 249889.11 examples/s]
Discarding source and target sentences with more than 24 tokens (num_proc=8):   0%|          | 0/2000 [00:00<?, ? examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8):  12%|█▎        | 250/2000 [00:00<00:04, 398.55 examples/s]Discarding source and target sentences with more than 24 tokens (num_proc=8): 100%|██████████| 2000/2000 [00:00<00:00, 2028.45 examples/s]
 2 2368
 3 21893
 4 35549
 5 50367
 6 58205
 7 61251
 8 56474
 9 52012
10 44968
11 38662
12 31477
13 25285
14 19509
15 14898
16 11599
17 8082
18 5791
19 4297
20 3090
21 2330
22 1607
23 1196
24 886
Map:   0%|          | 0/551796 [00:00<?, ? examples/s]Map:   0%|          | 1000/551796 [00:00<01:04, 8501.96 examples/s]Map:   0%|          | 2000/551796 [00:00<01:05, 8406.55 examples/s]Map:   1%|          | 3000/551796 [00:00<01:04, 8467.09 examples/s]Map:   1%|          | 4000/551796 [00:00<01:04, 8554.59 examples/s]Map:   1%|          | 5000/551796 [00:00<01:03, 8564.41 examples/s]Map:   1%|          | 6000/551796 [00:00<01:04, 8516.66 examples/s]Map:   1%|▏         | 7000/551796 [00:01<01:54, 4762.55 examples/s]Map:   1%|▏         | 8000/551796 [00:01<01:38, 5525.87 examples/s]Map:   2%|▏         | 9000/551796 [00:01<01:27, 6191.94 examples/s]Map:   2%|▏         | 10000/551796 [00:01<01:20, 6723.58 examples/s]Map:   2%|▏         | 11000/551796 [00:01<01:15, 7164.81 examples/s]Map:   2%|▏         | 12000/551796 [00:01<01:11, 7497.82 examples/s]Map:   2%|▏         | 13000/551796 [00:01<01:09, 7746.26 examples/s]Map:   3%|▎         | 14000/551796 [00:01<01:07, 7935.00 examples/s]Map:   3%|▎         | 15000/551796 [00:02<01:06, 8062.61 examples/s]Map:   3%|▎         | 16000/551796 [00:02<01:05, 8137.38 examples/s]Map:   3%|▎         | 17000/551796 [00:02<01:05, 8198.11 examples/s]Map:   3%|▎         | 18000/551796 [00:02<01:05, 8110.58 examples/s]Map:   3%|▎         | 19000/551796 [00:02<01:39, 5359.68 examples/s]Map:   4%|▎         | 20000/551796 [00:02<01:31, 5826.91 examples/s]Map:   4%|▍         | 21000/551796 [00:03<01:25, 6216.35 examples/s]Map:   4%|▍         | 22000/551796 [00:03<01:21, 6530.25 examples/s]Map:   4%|▍         | 23000/551796 [00:03<01:18, 6757.32 examples/s]Map:   4%|▍         | 24000/551796 [00:03<01:15, 6963.00 examples/s]Map:   5%|▍         | 25000/551796 [00:03<01:14, 7101.91 examples/s]Map:   5%|▍         | 26000/551796 [00:03<01:13, 7192.46 examples/s]Map:   5%|▍         | 27000/551796 [00:03<01:12, 7260.50 examples/s]Map:   5%|▌         | 28000/551796 [00:03<01:11, 7306.12 examples/s]Map:   5%|▌         | 29000/551796 [00:04<01:10, 7363.38 examples/s]Map:   5%|▌         | 30000/551796 [00:04<01:10, 7357.92 examples/s]Map:   6%|▌         | 31000/551796 [00:04<01:39, 5238.84 examples/s]Map:   6%|▌         | 32000/551796 [00:04<01:30, 5727.39 examples/s]Map:   6%|▌         | 33000/551796 [00:04<01:24, 6143.50 examples/s]Map:   6%|▌         | 34000/551796 [00:04<01:20, 6465.64 examples/s]Map:   6%|▋         | 35000/551796 [00:05<01:17, 6708.80 examples/s]Map:   7%|▋         | 36000/551796 [00:05<01:15, 6806.90 examples/s]Map:   7%|▋         | 37000/551796 [00:05<01:13, 6993.66 examples/s]Map:   7%|▋         | 38000/551796 [00:05<01:12, 7128.49 examples/s]Map:   7%|▋         | 39000/551796 [00:05<01:11, 7221.06 examples/s]Map:   7%|▋         | 40000/551796 [00:05<01:10, 7292.35 examples/s]Map:   7%|▋         | 41000/551796 [00:05<01:09, 7342.65 examples/s]Map:   8%|▊         | 42000/551796 [00:06<01:09, 7383.74 examples/s]Map:   8%|▊         | 43000/551796 [00:06<01:37, 5242.58 examples/s]Map:   8%|▊         | 44000/551796 [00:06<01:28, 5730.87 examples/s]Map:   8%|▊         | 45000/551796 [00:06<01:22, 6130.90 examples/s]Map:   8%|▊         | 46000/551796 [00:06<01:18, 6449.16 examples/s]Map:   9%|▊         | 47000/551796 [00:06<01:15, 6701.93 examples/s]Map:   9%|▊         | 48000/551796 [00:07<01:13, 6876.02 examples/s]Map:   9%|▉         | 49000/551796 [00:07<01:11, 7029.53 examples/s]Map:   9%|▉         | 50000/551796 [00:07<01:10, 7108.87 examples/s]Map:   9%|▉         | 51000/551796 [00:07<01:09, 7199.54 examples/s]Map:   9%|▉         | 52000/551796 [00:07<01:08, 7257.08 examples/s]Map:  10%|▉         | 53000/551796 [00:07<01:08, 7310.94 examples/s]Map:  10%|▉         | 54000/551796 [00:07<01:07, 7355.02 examples/s]Map:  10%|▉         | 55000/551796 [00:08<01:35, 5193.95 examples/s]Map:  10%|█         | 56000/551796 [00:08<01:27, 5686.56 examples/s]Map:  10%|█         | 57000/551796 [00:08<01:21, 6095.75 examples/s]Map:  11%|█         | 58000/551796 [00:08<01:16, 6428.71 examples/s]Map:  11%|█         | 59000/551796 [00:08<01:13, 6677.87 examples/s]Map:  11%|█         | 60000/551796 [00:08<01:13, 6654.57 examples/s]Map:  11%|█         | 61000/551796 [00:09<01:11, 6847.16 examples/s]Map:  11%|█         | 62000/551796 [00:09<01:10, 6977.87 examples/s]Map:  11%|█▏        | 63000/551796 [00:09<01:09, 7063.27 examples/s]Map:  12%|█▏        | 64000/551796 [00:09<01:08, 7146.25 examples/s]Map:  12%|█▏        | 65000/551796 [00:09<01:07, 7187.59 examples/s]Map:  12%|█▏        | 66000/551796 [00:09<01:07, 7230.57 examples/s]Map:  12%|█▏        | 67000/551796 [00:10<01:35, 5051.89 examples/s]Map:  12%|█▏        | 68000/551796 [00:10<01:26, 5572.28 examples/s]Map:  13%|█▎        | 69000/551796 [00:10<01:20, 5995.12 examples/s]Map:  13%|█▎        | 70000/551796 [00:10<01:16, 6322.96 examples/s]Map:  13%|█▎        | 71000/551796 [00:10<01:12, 6587.11 examples/s]Map:  13%|█▎        | 72000/551796 [00:10<01:10, 6782.24 examples/s]Map:  13%|█▎        | 73000/551796 [00:10<01:08, 6960.93 examples/s]Map:  13%|█▎        | 74000/551796 [00:10<01:07, 7078.47 examples/s]Map:  14%|█▎        | 75000/551796 [00:11<01:06, 7156.60 examples/s]Map:  14%|█▍        | 76000/551796 [00:11<01:06, 7196.34 examples/s]Map:  14%|█▍        | 77000/551796 [00:11<01:05, 7240.72 examples/s]Map:  14%|█▍        | 78000/551796 [00:11<01:05, 7258.22 examples/s]Map:  14%|█▍        | 79000/551796 [00:11<01:31, 5179.83 examples/s]Map:  14%|█▍        | 80000/551796 [00:12<01:23, 5647.43 examples/s]Map:  15%|█▍        | 81000/551796 [00:12<01:17, 6038.33 examples/s]Map:  15%|█▍        | 82000/551796 [00:12<01:13, 6401.20 examples/s]Map:  15%|█▌        | 83000/551796 [00:12<01:10, 6689.82 examples/s]Map:  15%|█▌        | 84000/551796 [00:12<01:08, 6867.23 examples/s]Map:  15%|█▌        | 85000/551796 [00:12<01:06, 7017.54 examples/s]Map:  16%|█▌        | 86000/551796 [00:12<01:05, 7113.46 examples/s]Map:  16%|█▌        | 87000/551796 [00:12<01:04, 7169.59 examples/s]Map:  16%|█▌        | 88000/551796 [00:13<01:04, 7218.46 examples/s]Map:  16%|█▌        | 89000/551796 [00:13<01:03, 7258.84 examples/s]Map:  16%|█▋        | 90000/551796 [00:13<01:03, 7276.50 examples/s]Map:  16%|█▋        | 91000/551796 [00:13<01:28, 5207.58 examples/s]Map:  17%|█▋        | 92000/551796 [00:13<01:20, 5689.87 examples/s]Map:  17%|█▋        | 93000/551796 [00:13<01:15, 6093.44 examples/s]Map:  17%|█▋        | 94000/551796 [00:14<01:11, 6423.29 examples/s]Map:  17%|█▋        | 95000/551796 [00:14<01:07, 6722.10 examples/s]Map:  17%|█▋        | 96000/551796 [00:14<01:05, 6910.69 examples/s]Map:  18%|█▊        | 97000/551796 [00:14<01:04, 7083.15 examples/s]Map:  18%|█▊        | 98000/551796 [00:14<01:03, 7156.96 examples/s]Map:  18%|█▊        | 99000/551796 [00:14<01:02, 7223.81 examples/s]Map:  18%|█▊        | 100000/551796 [00:14<01:02, 7261.20 examples/s]Map:  18%|█▊        | 101000/551796 [00:15<01:01, 7273.53 examples/s]Map:  18%|█▊        | 102000/551796 [00:15<01:01, 7276.41 examples/s]Map:  19%|█▊        | 103000/551796 [00:15<01:27, 5112.96 examples/s]Map:  19%|█▉        | 104000/551796 [00:15<01:19, 5600.13 examples/s]Map:  19%|█▉        | 105000/551796 [00:15<01:13, 6041.83 examples/s]Map:  19%|█▉        | 106000/551796 [00:15<01:09, 6382.15 examples/s]Map:  19%|█▉        | 107000/551796 [00:16<01:06, 6656.86 examples/s]Map:  20%|█▉        | 108000/551796 [00:16<01:04, 6854.29 examples/s]Map:  20%|█▉        | 109000/551796 [00:16<01:03, 6986.64 examples/s]Map:  20%|█▉        | 110000/551796 [00:16<01:02, 7074.60 examples/s]Map:  20%|██        | 111000/551796 [00:16<01:01, 7141.29 examples/s]Map:  20%|██        | 112000/551796 [00:16<01:01, 7186.00 examples/s]Map:  20%|██        | 113000/551796 [00:16<01:00, 7232.33 examples/s]Map:  21%|██        | 114000/551796 [00:17<01:00, 7233.05 examples/s]Map:  21%|██        | 115000/551796 [00:17<01:25, 5083.86 examples/s]Map:  21%|██        | 116000/551796 [00:17<01:17, 5594.72 examples/s]Map:  21%|██        | 117000/551796 [00:17<01:12, 6035.83 examples/s]Map:  21%|██▏       | 118000/551796 [00:17<01:08, 6376.69 examples/s]Map:  22%|██▏       | 119000/551796 [00:17<01:05, 6653.26 examples/s]Map:  22%|██▏       | 120000/551796 [00:18<01:03, 6838.22 examples/s]Map:  22%|██▏       | 121000/551796 [00:18<01:01, 6983.62 examples/s]Map:  22%|██▏       | 122000/551796 [00:18<01:00, 7080.86 examples/s]Map:  22%|██▏       | 123000/551796 [00:18<01:00, 7142.85 examples/s]Map:  22%|██▏       | 124000/551796 [00:18<01:00, 7097.93 examples/s]Map:  23%|██▎       | 125000/551796 [00:18<00:59, 7163.95 examples/s]Map:  23%|██▎       | 126000/551796 [00:18<00:58, 7244.78 examples/s]Map:  23%|██▎       | 127000/551796 [00:19<01:24, 5020.28 examples/s]Map:  23%|██▎       | 128000/551796 [00:19<01:16, 5544.51 examples/s]Map:  23%|██▎       | 129000/551796 [00:19<01:10, 5960.63 examples/s]Map:  24%|██▎       | 130000/551796 [00:19<01:06, 6300.41 examples/s]Map:  24%|██▎       | 131000/551796 [00:19<01:03, 6585.36 examples/s]Map:  24%|██▍       | 132000/551796 [00:19<01:01, 6793.83 examples/s]Map:  24%|██▍       | 133000/551796 [00:20<01:00, 6958.25 examples/s]Map:  24%|██▍       | 134000/551796 [00:20<00:59, 7079.20 examples/s]Map:  24%|██▍       | 135000/551796 [00:20<00:57, 7190.49 examples/s]Map:  25%|██▍       | 136000/551796 [00:20<00:57, 7228.07 examples/s]Map:  25%|██▍       | 137000/551796 [00:20<00:57, 7262.30 examples/s]Map:  25%|██▌       | 138000/551796 [00:20<00:56, 7289.97 examples/s]Map:  25%|██▌       | 139000/551796 [00:21<01:19, 5207.72 examples/s]Map:  25%|██▌       | 140000/551796 [00:21<01:12, 5701.15 examples/s]Map:  26%|██▌       | 141000/551796 [00:21<01:07, 6128.76 examples/s]Map:  26%|██▌       | 142000/551796 [00:21<01:03, 6443.79 examples/s]Map:  26%|██▌       | 143000/551796 [00:21<01:01, 6694.12 examples/s]Map:  26%|██▌       | 144000/551796 [00:21<00:59, 6853.92 examples/s]Map:  26%|██▋       | 145000/551796 [00:21<00:57, 7016.81 examples/s]Map:  26%|██▋       | 146000/551796 [00:21<00:57, 7109.29 examples/s]Map:  27%|██▋       | 147000/551796 [00:22<00:56, 7199.41 examples/s]Map:  27%|██▋       | 148000/551796 [00:22<00:55, 7245.26 examples/s]Map:  27%|██▋       | 149000/551796 [00:22<00:55, 7260.82 examples/s]Map:  27%|██▋       | 150000/551796 [00:22<00:57, 6996.11 examples/s]Map:  27%|██▋       | 151000/551796 [00:22<01:21, 4891.46 examples/s]Map:  28%|██▊       | 152000/551796 [00:23<01:14, 5402.55 examples/s]Map:  28%|██▊       | 153000/551796 [00:23<01:08, 5821.73 examples/s]Map:  28%|██▊       | 154000/551796 [00:23<01:04, 6143.55 examples/s]Map:  28%|██▊       | 155000/551796 [00:23<01:01, 6458.88 examples/s]Map:  28%|██▊       | 156000/551796 [00:23<00:59, 6688.29 examples/s]Map:  28%|██▊       | 157000/551796 [00:23<00:58, 6774.21 examples/s]Map:  29%|██▊       | 158000/551796 [00:23<00:55, 7044.22 examples/s]Map:  29%|██▉       | 159000/551796 [00:23<00:53, 7355.99 examples/s]Map:  29%|██▉       | 160000/551796 [00:24<00:51, 7602.55 examples/s]Map:  29%|██▉       | 161000/551796 [00:24<00:50, 7792.90 examples/s]Map:  29%|██▉       | 162000/551796 [00:24<00:49, 7914.92 examples/s]Map:  30%|██▉       | 163000/551796 [00:24<01:15, 5126.66 examples/s]Map:  30%|██▉       | 164000/551796 [00:24<01:07, 5777.41 examples/s]Map:  30%|██▉       | 165000/551796 [00:24<01:00, 6361.19 examples/s]Map:  30%|███       | 166000/551796 [00:25<00:56, 6772.25 examples/s]Map:  30%|███       | 167000/551796 [00:25<00:53, 7166.61 examples/s]Map:  30%|███       | 168000/551796 [00:25<00:51, 7466.46 examples/s]Map:  31%|███       | 169000/551796 [00:25<00:49, 7710.42 examples/s]Map:  31%|███       | 170000/551796 [00:25<00:48, 7878.04 examples/s]Map:  31%|███       | 171000/551796 [00:25<00:48, 7799.74 examples/s]Map:  31%|███       | 172000/551796 [00:25<00:48, 7896.39 examples/s]Map:  31%|███▏      | 173000/551796 [00:25<00:47, 7983.95 examples/s]Map:  32%|███▏      | 174000/551796 [00:26<00:47, 7947.68 examples/s]Map:  32%|███▏      | 175000/551796 [00:26<01:17, 4847.54 examples/s]Map:  32%|███▏      | 176000/551796 [00:26<01:08, 5509.97 examples/s]Map:  32%|███▏      | 177000/551796 [00:26<01:01, 6104.71 examples/s]Map:  32%|███▏      | 178000/551796 [00:26<00:56, 6606.25 examples/s]Map:  32%|███▏      | 179000/551796 [00:26<00:53, 7004.23 examples/s]Map:  33%|███▎      | 180000/551796 [00:27<00:50, 7315.27 examples/s]Map:  33%|███▎      | 181000/551796 [00:27<00:49, 7560.59 examples/s]Map:  33%|███▎      | 182000/551796 [00:27<00:47, 7733.80 examples/s]Map:  33%|███▎      | 183000/551796 [00:27<00:46, 7894.58 examples/s]Map:  33%|███▎      | 184000/551796 [00:27<00:46, 7957.11 examples/s]Map:  34%|███▎      | 185000/551796 [00:27<00:45, 8014.30 examples/s]Map:  34%|███▎      | 186000/551796 [00:27<00:45, 8035.73 examples/s]Map:  34%|███▍      | 187000/551796 [00:28<01:09, 5273.21 examples/s]Map:  34%|███▍      | 188000/551796 [00:28<01:01, 5885.98 examples/s]Map:  34%|███▍      | 189000/551796 [00:28<00:56, 6430.78 examples/s]Map:  34%|███▍      | 190000/551796 [00:28<00:52, 6863.97 examples/s]Map:  35%|███▍      | 191000/551796 [00:28<00:50, 7210.91 examples/s]Map:  35%|███▍      | 192000/551796 [00:28<00:48, 7455.69 examples/s]Map:  35%|███▍      | 193000/551796 [00:28<00:46, 7654.64 examples/s]Map:  35%|███▌      | 194000/551796 [00:28<00:45, 7799.57 examples/s]Map:  35%|███▌      | 195000/551796 [00:29<00:44, 7941.58 examples/s]Map:  36%|███▌      | 196000/551796 [00:29<00:44, 8005.68 examples/s]Map:  36%|███▌      | 197000/551796 [00:29<00:43, 8082.47 examples/s]Map:  36%|███▌      | 198000/551796 [00:29<00:43, 8104.07 examples/s]Map:  36%|███▌      | 199000/551796 [00:29<01:07, 5212.30 examples/s]Map:  36%|███▌      | 200000/551796 [00:29<01:00, 5847.17 examples/s]Map:  36%|███▋      | 201000/551796 [00:30<00:54, 6396.70 examples/s]Map:  37%|███▋      | 202000/551796 [00:30<00:51, 6825.82 examples/s]Map:  37%|███▋      | 203000/551796 [00:30<00:48, 7172.76 examples/s]Map:  37%|███▋      | 204000/551796 [00:30<00:46, 7432.70 examples/s]Map:  37%|███▋      | 205000/551796 [00:30<00:45, 7666.62 examples/s]Map:  37%|███▋      | 206000/551796 [00:30<00:44, 7820.55 examples/s]Map:  38%|███▊      | 207000/551796 [00:30<00:43, 7951.46 examples/s]Map:  38%|███▊      | 208000/551796 [00:30<00:42, 8013.85 examples/s]Map:  38%|███▊      | 209000/551796 [00:31<00:42, 8069.83 examples/s]Map:  38%|███▊      | 210000/551796 [00:31<00:42, 8101.42 examples/s]Map:  38%|███▊      | 211000/551796 [00:31<01:04, 5300.76 examples/s]Map:  38%|███▊      | 212000/551796 [00:31<00:57, 5909.92 examples/s]Map:  39%|███▊      | 213000/551796 [00:31<00:52, 6459.05 examples/s]Map:  39%|███▉      | 214000/551796 [00:31<00:49, 6891.89 examples/s]Map:  39%|███▉      | 215000/551796 [00:31<00:46, 7254.04 examples/s]Map:  39%|███▉      | 216000/551796 [00:32<00:44, 7482.33 examples/s]Map:  39%|███▉      | 217000/551796 [00:32<00:43, 7706.58 examples/s]Map:  40%|███▉      | 218000/551796 [00:32<00:42, 7856.78 examples/s]Map:  40%|███▉      | 219000/551796 [00:32<00:41, 7949.39 examples/s]Map:  40%|███▉      | 220000/551796 [00:32<00:41, 8016.04 examples/s]Map:  40%|████      | 221000/551796 [00:32<00:40, 8086.74 examples/s]Map:  40%|████      | 222000/551796 [00:32<00:40, 8156.12 examples/s]Map:  40%|████      | 223000/551796 [00:33<01:02, 5220.08 examples/s]Map:  41%|████      | 224000/551796 [00:33<00:55, 5856.69 examples/s]Map:  41%|████      | 225000/551796 [00:33<00:51, 6397.78 examples/s]Map:  41%|████      | 226000/551796 [00:33<00:48, 6748.59 examples/s]Map:  41%|████      | 227000/551796 [00:33<00:47, 6827.78 examples/s]Map:  41%|████▏     | 228000/551796 [00:33<00:46, 6971.05 examples/s]Map:  42%|████▏     | 229000/551796 [00:33<00:45, 7078.80 examples/s]Map:  42%|████▏     | 230000/551796 [00:34<00:45, 7143.34 examples/s]Map:  42%|████▏     | 231000/551796 [00:34<00:44, 7204.96 examples/s]Map:  42%|████▏     | 232000/551796 [00:34<00:44, 7226.54 examples/s]Map:  42%|████▏     | 233000/551796 [00:34<00:43, 7269.71 examples/s]Map:  42%|████▏     | 234000/551796 [00:34<00:43, 7270.62 examples/s]Map:  43%|████▎     | 235000/551796 [00:35<01:02, 5053.64 examples/s]Map:  43%|████▎     | 236000/551796 [00:35<00:56, 5560.29 examples/s]Map:  43%|████▎     | 237000/551796 [00:35<00:52, 5983.62 examples/s]Map:  43%|████▎     | 238000/551796 [00:35<00:49, 6311.61 examples/s]Map:  43%|████▎     | 239000/551796 [00:35<00:47, 6565.50 examples/s]Map:  43%|████▎     | 240000/551796 [00:35<00:46, 6747.82 examples/s]Map:  44%|████▎     | 241000/551796 [00:35<00:44, 6908.46 examples/s]Map:  44%|████▍     | 242000/551796 [00:35<00:44, 7012.13 examples/s]Map:  44%|████▍     | 243000/551796 [00:36<00:43, 7090.46 examples/s]Map:  44%|████▍     | 244000/551796 [00:36<00:43, 7130.60 examples/s]Map:  44%|████▍     | 245000/551796 [00:36<00:42, 7159.22 examples/s]Map:  45%|████▍     | 246000/551796 [00:36<00:42, 7186.32 examples/s]Map:  45%|████▍     | 247000/551796 [00:36<00:59, 5147.74 examples/s]Map:  45%|████▍     | 248000/551796 [00:36<00:53, 5629.74 examples/s]Map:  45%|████▌     | 249000/551796 [00:37<00:50, 6047.09 examples/s]Map:  45%|████▌     | 250000/551796 [00:37<00:47, 6353.56 examples/s]Map:  45%|████▌     | 251000/551796 [00:37<00:45, 6610.42 examples/s]Map:  46%|████▌     | 252000/551796 [00:37<00:44, 6799.48 examples/s]Map:  46%|████▌     | 253000/551796 [00:37<00:43, 6940.35 examples/s]Map:  46%|████▌     | 254000/551796 [00:37<00:42, 7033.50 examples/s]Map:  46%|████▌     | 255000/551796 [00:37<00:41, 7135.00 examples/s]Map:  46%|████▋     | 256000/551796 [00:38<00:41, 7177.51 examples/s]Map:  47%|████▋     | 257000/551796 [00:38<00:40, 7221.85 examples/s]Map:  47%|████▋     | 258000/551796 [00:38<00:40, 7228.51 examples/s]Map:  47%|████▋     | 259000/551796 [00:38<00:56, 5151.28 examples/s]Map:  47%|████▋     | 260000/551796 [00:38<00:51, 5644.45 examples/s]Map:  47%|████▋     | 261000/551796 [00:38<00:48, 6053.04 examples/s]Map:  47%|████▋     | 262000/551796 [00:39<00:45, 6354.46 examples/s]Map:  48%|████▊     | 263000/551796 [00:39<00:43, 6607.68 examples/s]Map:  48%|████▊     | 264000/551796 [00:39<00:42, 6778.86 examples/s]Map:  48%|████▊     | 265000/551796 [00:39<00:41, 6918.75 examples/s]Map:  48%|████▊     | 266000/551796 [00:39<00:40, 7011.08 examples/s]Map:  48%|████▊     | 267000/551796 [00:39<00:40, 7092.54 examples/s]Map:  49%|████▊     | 268000/551796 [00:39<00:39, 7141.73 examples/s]Map:  49%|████▊     | 269000/551796 [00:40<00:39, 7192.89 examples/s]Map:  49%|████▉     | 270000/551796 [00:40<00:39, 7217.47 examples/s]Map:  49%|████▉     | 271000/551796 [00:40<00:54, 5156.47 examples/s]Map:  49%|████▉     | 272000/551796 [00:40<00:49, 5646.99 examples/s]Map:  49%|████▉     | 273000/551796 [00:40<00:46, 6057.99 examples/s]Map:  50%|████▉     | 274000/551796 [00:40<00:43, 6367.74 examples/s]Map:  50%|████▉     | 275000/551796 [00:41<00:41, 6612.87 examples/s]Map:  50%|█████     | 276000/551796 [00:41<00:40, 6791.47 examples/s]Map:  50%|█████     | 277000/551796 [00:41<00:39, 6916.46 examples/s]Map:  50%|█████     | 278000/551796 [00:41<00:39, 7008.29 examples/s]Map:  51%|█████     | 279000/551796 [00:41<00:38, 7079.73 examples/s]Map:  51%|█████     | 280000/551796 [00:41<00:38, 7126.08 examples/s]Map:  51%|█████     | 281000/551796 [00:41<00:37, 7153.14 examples/s]Map:  51%|█████     | 282000/551796 [00:42<00:37, 7150.16 examples/s]Map:  51%|█████▏    | 283000/551796 [00:42<00:52, 5138.89 examples/s]Map:  51%|█████▏    | 284000/551796 [00:42<00:47, 5627.67 examples/s]Map:  52%|█████▏    | 285000/551796 [00:42<00:44, 6036.67 examples/s]Map:  52%|█████▏    | 286000/551796 [00:42<00:41, 6345.50 examples/s]Map:  52%|█████▏    | 287000/551796 [00:42<00:40, 6597.74 examples/s]Map:  52%|█████▏    | 288000/551796 [00:43<00:38, 6769.50 examples/s]Map:  52%|█████▏    | 289000/551796 [00:43<00:38, 6915.62 examples/s]Map:  53%|█████▎    | 290000/551796 [00:43<00:37, 7017.95 examples/s]Map:  53%|█████▎    | 291000/551796 [00:43<00:36, 7086.88 examples/s]Map:  53%|█████▎    | 292000/551796 [00:43<00:36, 7121.27 examples/s]Map:  53%|█████▎    | 293000/551796 [00:43<00:36, 7170.61 examples/s]Map:  53%|█████▎    | 294000/551796 [00:43<00:35, 7183.86 examples/s]Map:  53%|█████▎    | 295000/551796 [00:44<00:49, 5161.77 examples/s]Map:  54%|█████▎    | 296000/551796 [00:44<00:45, 5648.87 examples/s]Map:  54%|█████▍    | 297000/551796 [00:44<00:42, 6053.11 examples/s]Map:  54%|█████▍    | 298000/551796 [00:44<00:39, 6360.87 examples/s]Map:  54%|█████▍    | 299000/551796 [00:44<00:38, 6615.24 examples/s]Map:  54%|█████▍    | 300000/551796 [00:44<00:36, 6813.28 examples/s]Map:  55%|█████▍    | 301000/551796 [00:45<00:36, 6936.25 examples/s]Map:  55%|█████▍    | 302000/551796 [00:45<00:35, 7040.08 examples/s]Map:  55%|█████▍    | 303000/551796 [00:45<00:34, 7122.98 examples/s]Map:  55%|█████▌    | 304000/551796 [00:45<00:34, 7170.90 examples/s]Map:  55%|█████▌    | 305000/551796 [00:45<00:34, 7196.91 examples/s]Map:  55%|█████▌    | 306000/551796 [00:45<00:34, 7201.97 examples/s]Map:  56%|█████▌    | 307000/551796 [00:46<00:47, 5152.66 examples/s]Map:  56%|█████▌    | 308000/551796 [00:46<00:43, 5647.74 examples/s]Map:  56%|█████▌    | 309000/551796 [00:46<00:40, 6060.17 examples/s]Map:  56%|█████▌    | 310000/551796 [00:46<00:37, 6369.78 examples/s]Map:  56%|█████▋    | 311000/551796 [00:46<00:36, 6612.45 examples/s]Map:  57%|█████▋    | 312000/551796 [00:46<00:35, 6793.61 examples/s]Map:  57%|█████▋    | 313000/551796 [00:46<00:34, 6941.81 examples/s]Map:  57%|█████▋    | 314000/551796 [00:47<00:33, 7036.60 examples/s]Map:  57%|█████▋    | 315000/551796 [00:47<00:33, 7098.43 examples/s]Map:  57%|█████▋    | 316000/551796 [00:47<00:33, 7137.16 examples/s]Map:  57%|█████▋    | 317000/551796 [00:47<00:32, 7163.86 examples/s]Map:  58%|█████▊    | 318000/551796 [00:47<00:32, 7186.68 examples/s]Map:  58%|█████▊    | 319000/551796 [00:47<00:45, 5152.37 examples/s]Map:  58%|█████▊    | 320000/551796 [00:48<00:41, 5636.94 examples/s]Map:  58%|█████▊    | 321000/551796 [00:48<00:38, 6044.16 examples/s]Map:  58%|█████▊    | 322000/551796 [00:48<00:36, 6349.38 examples/s]Map:  59%|█████▊    | 323000/551796 [00:48<00:34, 6606.19 examples/s]Map:  59%|█████▊    | 324000/551796 [00:48<00:33, 6791.74 examples/s]Map:  59%|█████▉    | 325000/551796 [00:48<00:32, 6928.65 examples/s]Map:  59%|█████▉    | 326000/551796 [00:48<00:32, 7023.31 examples/s]Map:  59%|█████▉    | 327000/551796 [00:48<00:31, 7085.22 examples/s]Map:  59%|█████▉    | 328000/551796 [00:49<00:31, 7125.79 examples/s]Map:  60%|█████▉    | 329000/551796 [00:49<00:31, 7170.35 examples/s]Map:  60%|█████▉    | 330000/551796 [00:49<00:30, 7184.40 examples/s]Map:  60%|█████▉    | 331000/551796 [00:49<00:42, 5149.42 examples/s]Map:  60%|██████    | 332000/551796 [00:49<00:38, 5639.65 examples/s]Map:  60%|██████    | 333000/551796 [00:49<00:36, 6059.06 examples/s]Map:  61%|██████    | 334000/551796 [00:50<00:34, 6372.65 examples/s]Map:  61%|██████    | 335000/551796 [00:50<00:32, 6618.70 examples/s]Map:  61%|██████    | 336000/551796 [00:50<00:31, 6797.81 examples/s]Map:  61%|██████    | 337000/551796 [00:50<00:30, 6936.75 examples/s]Map:  61%|██████▏   | 338000/551796 [00:50<00:30, 7025.92 examples/s]Map:  61%|██████▏   | 339000/551796 [00:50<00:29, 7114.30 examples/s]Map:  62%|██████▏   | 340000/551796 [00:50<00:29, 7148.28 examples/s]Map:  62%|██████▏   | 341000/551796 [00:51<00:29, 7186.83 examples/s]Map:  62%|██████▏   | 342000/551796 [00:51<00:29, 7205.44 examples/s]Map:  62%|██████▏   | 343000/551796 [00:51<00:40, 5157.43 examples/s]Map:  62%|██████▏   | 344000/551796 [00:51<00:36, 5644.93 examples/s]Map:  63%|██████▎   | 345000/551796 [00:51<00:34, 6055.17 examples/s]Map:  63%|██████▎   | 346000/551796 [00:51<00:32, 6380.00 examples/s]Map:  63%|██████▎   | 347000/551796 [00:52<00:30, 6623.74 examples/s]Map:  63%|██████▎   | 348000/551796 [00:52<00:29, 6803.81 examples/s]Map:  63%|██████▎   | 349000/551796 [00:52<00:29, 6934.65 examples/s]Map:  63%|██████▎   | 350000/551796 [00:52<00:28, 7046.98 examples/s]Map:  64%|██████▎   | 351000/551796 [00:52<00:28, 7124.09 examples/s]Map:  64%|██████▍   | 352000/551796 [00:52<00:27, 7172.48 examples/s]Map:  64%|██████▍   | 353000/551796 [00:52<00:27, 7229.55 examples/s]Map:  64%|██████▍   | 354000/551796 [00:53<00:27, 7234.06 examples/s]Map:  64%|██████▍   | 355000/551796 [00:53<00:37, 5179.02 examples/s]Map:  65%|██████▍   | 356000/551796 [00:53<00:34, 5674.05 examples/s]Map:  65%|██████▍   | 357000/551796 [00:53<00:32, 6069.94 examples/s]Map:  65%|██████▍   | 358000/551796 [00:53<00:30, 6377.08 examples/s]Map:  65%|██████▌   | 359000/551796 [00:53<00:29, 6631.38 examples/s]Map:  65%|██████▌   | 360000/551796 [00:54<00:28, 6815.77 examples/s]Map:  65%|██████▌   | 361000/551796 [00:54<00:27, 6938.98 examples/s]Map:  66%|██████▌   | 362000/551796 [00:54<00:26, 7031.06 examples/s]Map:  66%|██████▌   | 363000/551796 [00:54<00:26, 7113.17 examples/s]Map:  66%|██████▌   | 364000/551796 [00:54<00:26, 7148.20 examples/s]Map:  66%|██████▌   | 365000/551796 [00:54<00:25, 7199.05 examples/s]Map:  66%|██████▋   | 366000/551796 [00:54<00:25, 7213.63 examples/s]Map:  67%|██████▋   | 367000/551796 [00:55<00:35, 5172.58 examples/s]Map:  67%|██████▋   | 368000/551796 [00:55<00:32, 5662.14 examples/s]Map:  67%|██████▋   | 369000/551796 [00:55<00:30, 6066.48 examples/s]Map:  67%|██████▋   | 370000/551796 [00:55<00:28, 6376.63 examples/s]Map:  67%|██████▋   | 371000/551796 [00:55<00:27, 6635.52 examples/s]Map:  67%|██████▋   | 372000/551796 [00:55<00:26, 6827.17 examples/s]Map:  68%|██████▊   | 373000/551796 [00:56<00:25, 6946.52 examples/s]Map:  68%|██████▊   | 374000/551796 [00:56<00:25, 7037.30 examples/s]Map:  68%|██████▊   | 375000/551796 [00:56<00:24, 7113.63 examples/s]Map:  68%|██████▊   | 376000/551796 [00:56<00:24, 7156.51 examples/s]Map:  68%|██████▊   | 377000/551796 [00:56<00:24, 7191.72 examples/s]Map:  69%|██████▊   | 378000/551796 [00:56<00:24, 7198.94 examples/s]Map:  69%|██████▊   | 379000/551796 [00:57<00:33, 5170.85 examples/s]Map:  69%|██████▉   | 380000/551796 [00:57<00:30, 5658.16 examples/s]Map:  69%|██████▉   | 381000/551796 [00:57<00:28, 6065.63 examples/s]Map:  69%|██████▉   | 382000/551796 [00:57<00:26, 6370.33 examples/s]Map:  69%|██████▉   | 383000/551796 [00:57<00:25, 6627.03 examples/s]Map:  70%|██████▉   | 384000/551796 [00:57<00:24, 6807.44 examples/s]Map:  70%|██████▉   | 385000/551796 [00:57<00:24, 6937.61 examples/s]Map:  70%|██████▉   | 386000/551796 [00:58<00:23, 6943.46 examples/s]Map:  70%|███████   | 387000/551796 [00:58<00:23, 7010.27 examples/s]Map:  70%|███████   | 388000/551796 [00:58<00:23, 7055.11 examples/s]Map:  70%|███████   | 389000/551796 [00:58<00:22, 7127.06 examples/s]Map:  71%|███████   | 390000/551796 [00:58<00:22, 7167.12 examples/s]Map:  71%|███████   | 391000/551796 [00:58<00:31, 5101.69 examples/s]Map:  71%|███████   | 392000/551796 [00:59<00:28, 5600.92 examples/s]Map:  71%|███████   | 393000/551796 [00:59<00:26, 6030.06 examples/s]Map:  71%|███████▏  | 394000/551796 [00:59<00:24, 6367.81 examples/s]Map:  72%|███████▏  | 395000/551796 [00:59<00:23, 6624.80 examples/s]Map:  72%|███████▏  | 396000/551796 [00:59<00:22, 6794.75 examples/s]Map:  72%|███████▏  | 397000/551796 [00:59<00:22, 6924.26 examples/s]Map:  72%|███████▏  | 398000/551796 [00:59<00:21, 7014.97 examples/s]Map:  72%|███████▏  | 399000/551796 [01:00<00:21, 7090.72 examples/s]Map:  72%|███████▏  | 400000/551796 [01:00<00:21, 7118.45 examples/s]Map:  73%|███████▎  | 401000/551796 [01:00<00:21, 7176.59 examples/s]Map:  73%|███████▎  | 402000/551796 [01:00<00:20, 7210.65 examples/s]Map:  73%|███████▎  | 403000/551796 [01:00<00:28, 5184.73 examples/s]Map:  73%|███████▎  | 404000/551796 [01:00<00:26, 5673.20 examples/s]Map:  73%|███████▎  | 405000/551796 [01:01<00:24, 6093.42 examples/s]Map:  74%|███████▎  | 406000/551796 [01:01<00:22, 6418.08 examples/s]Map:  74%|███████▍  | 407000/551796 [01:01<00:21, 6661.57 examples/s]Map:  74%|███████▍  | 408000/551796 [01:01<00:21, 6842.56 examples/s]Map:  74%|███████▍  | 409000/551796 [01:01<00:20, 6966.32 examples/s]Map:  74%|███████▍  | 410000/551796 [01:01<00:20, 7066.60 examples/s]Map:  74%|███████▍  | 411000/551796 [01:01<00:19, 7147.44 examples/s]Map:  75%|███████▍  | 412000/551796 [01:01<00:19, 7198.46 examples/s]Map:  75%|███████▍  | 413000/551796 [01:02<00:19, 7207.64 examples/s]Map:  75%|███████▌  | 414000/551796 [01:02<00:19, 7235.87 examples/s]Map:  75%|███████▌  | 415000/551796 [01:02<00:26, 5178.26 examples/s]Map:  75%|███████▌  | 416000/551796 [01:02<00:23, 5665.11 examples/s]Map:  76%|███████▌  | 417000/551796 [01:02<00:22, 6057.73 examples/s]Map:  76%|███████▌  | 418000/551796 [01:02<00:20, 6373.74 examples/s]Map:  76%|███████▌  | 419000/551796 [01:03<00:20, 6618.45 examples/s]Map:  76%|███████▌  | 420000/551796 [01:03<00:19, 6809.33 examples/s]Map:  76%|███████▋  | 421000/551796 [01:03<00:18, 6951.90 examples/s]Map:  76%|███████▋  | 422000/551796 [01:03<00:18, 7051.14 examples/s]Map:  77%|███████▋  | 423000/551796 [01:03<00:18, 7118.72 examples/s]Map:  77%|███████▋  | 424000/551796 [01:03<00:17, 7153.57 examples/s]Map:  77%|███████▋  | 425000/551796 [01:03<00:17, 7181.92 examples/s]Map:  77%|███████▋  | 426000/551796 [01:04<00:17, 7213.68 examples/s]Map:  77%|███████▋  | 427000/551796 [01:04<00:24, 5186.71 examples/s]Map:  78%|███████▊  | 428000/551796 [01:04<00:21, 5682.30 examples/s]Map:  78%|███████▊  | 429000/551796 [01:04<00:20, 6081.15 examples/s]Map:  78%|███████▊  | 430000/551796 [01:04<00:18, 6413.78 examples/s]Map:  78%|███████▊  | 431000/551796 [01:04<00:18, 6667.36 examples/s]Map:  78%|███████▊  | 432000/551796 [01:05<00:17, 6853.06 examples/s]Map:  78%|███████▊  | 433000/551796 [01:05<00:16, 6988.71 examples/s]Map:  79%|███████▊  | 434000/551796 [01:05<00:16, 7075.03 examples/s]Map:  79%|███████▉  | 435000/551796 [01:05<00:16, 7137.65 examples/s]Map:  79%|███████▉  | 436000/551796 [01:05<00:16, 7170.10 examples/s]Map:  79%|███████▉  | 437000/551796 [01:05<00:15, 7223.58 examples/s]Map:  79%|███████▉  | 438000/551796 [01:05<00:15, 7244.76 examples/s]Map:  80%|███████▉  | 439000/551796 [01:06<00:21, 5197.77 examples/s]Map:  80%|███████▉  | 440000/551796 [01:06<00:19, 5669.04 examples/s]Map:  80%|███████▉  | 441000/551796 [01:06<00:18, 6081.24 examples/s]Map:  80%|████████  | 442000/551796 [01:06<00:17, 6399.21 examples/s]Map:  80%|████████  | 443000/551796 [01:06<00:16, 6640.40 examples/s]Map:  80%|████████  | 444000/551796 [01:06<00:15, 6820.90 examples/s]Map:  81%|████████  | 445000/551796 [01:07<00:15, 6954.13 examples/s]Map:  81%|████████  | 446000/551796 [01:07<00:14, 7061.37 examples/s]Map:  81%|████████  | 447000/551796 [01:07<00:14, 7116.68 examples/s]Map:  81%|████████  | 448000/551796 [01:07<00:14, 7156.78 examples/s]Map:  81%|████████▏ | 449000/551796 [01:07<00:14, 7205.56 examples/s]Map:  82%|████████▏ | 450000/551796 [01:07<00:14, 7233.78 examples/s]Map:  82%|████████▏ | 451000/551796 [01:08<00:19, 5199.70 examples/s]Map:  82%|████████▏ | 452000/551796 [01:08<00:17, 5678.22 examples/s]Map:  82%|████████▏ | 453000/551796 [01:08<00:16, 6075.69 examples/s]Map:  82%|████████▏ | 454000/551796 [01:08<00:15, 6394.24 examples/s]Map:  82%|████████▏ | 455000/551796 [01:08<00:14, 6641.66 examples/s]Map:  83%|████████▎ | 456000/551796 [01:08<00:14, 6826.14 examples/s]Map:  83%|████████▎ | 457000/551796 [01:08<00:13, 6957.71 examples/s]Map:  83%|████████▎ | 458000/551796 [01:09<00:13, 7060.92 examples/s]Map:  83%|████████▎ | 459000/551796 [01:09<00:12, 7147.26 examples/s]Map:  83%|████████▎ | 460000/551796 [01:09<00:12, 7187.65 examples/s]Map:  84%|████████▎ | 461000/551796 [01:09<00:12, 7228.72 examples/s]Map:  84%|████████▎ | 462000/551796 [01:09<00:12, 7269.22 examples/s]Map:  84%|████████▍ | 463000/551796 [01:09<00:17, 5209.06 examples/s]Map:  84%|████████▍ | 464000/551796 [01:10<00:15, 5678.20 examples/s]Map:  84%|████████▍ | 465000/551796 [01:10<00:14, 6083.38 examples/s]Map:  84%|████████▍ | 466000/551796 [01:10<00:13, 6407.39 examples/s]Map:  85%|████████▍ | 467000/551796 [01:10<00:12, 6641.63 examples/s]Map:  85%|████████▍ | 468000/551796 [01:10<00:12, 6836.23 examples/s]Map:  85%|████████▍ | 469000/551796 [01:10<00:11, 6986.78 examples/s]Map:  85%|████████▌ | 470000/551796 [01:10<00:11, 7078.30 examples/s]Map:  85%|████████▌ | 471000/551796 [01:10<00:11, 7163.23 examples/s]Map:  86%|████████▌ | 472000/551796 [01:11<00:11, 7197.54 examples/s]Map:  86%|████████▌ | 473000/551796 [01:11<00:10, 7239.34 examples/s]Map:  86%|████████▌ | 474000/551796 [01:11<00:10, 7255.26 examples/s]Map:  86%|████████▌ | 475000/551796 [01:11<00:14, 5217.31 examples/s]Map:  86%|████████▋ | 476000/551796 [01:11<00:13, 5704.72 examples/s]Map:  86%|████████▋ | 477000/551796 [01:11<00:12, 6105.71 examples/s]Map:  87%|████████▋ | 478000/551796 [01:12<00:11, 6405.80 examples/s]Map:  87%|████████▋ | 479000/551796 [01:12<00:10, 6639.61 examples/s]Map:  87%|████████▋ | 480000/551796 [01:12<00:10, 6823.30 examples/s]Map:  87%|████████▋ | 481000/551796 [01:12<00:10, 6967.94 examples/s]Map:  87%|████████▋ | 482000/551796 [01:12<00:09, 7054.23 examples/s]Map:  88%|████████▊ | 483000/551796 [01:12<00:09, 7133.84 examples/s]Map:  88%|████████▊ | 484000/551796 [01:12<00:09, 7162.33 examples/s]Map:  88%|████████▊ | 485000/551796 [01:13<00:09, 7222.69 examples/s]Map:  88%|████████▊ | 486000/551796 [01:13<00:09, 7234.41 examples/s]Map:  88%|████████▊ | 487000/551796 [01:13<00:12, 5204.03 examples/s]Map:  88%|████████▊ | 488000/551796 [01:13<00:11, 5694.52 examples/s]Map:  89%|████████▊ | 489000/551796 [01:13<00:10, 6100.21 examples/s]Map:  89%|████████▉ | 490000/551796 [01:13<00:09, 6419.09 examples/s]Map:  89%|████████▉ | 491000/551796 [01:14<00:09, 6666.33 examples/s]Map:  89%|████████▉ | 492000/551796 [01:14<00:08, 6817.81 examples/s]Map:  89%|████████▉ | 493000/551796 [01:14<00:08, 6961.93 examples/s]Map:  90%|████████▉ | 494000/551796 [01:14<00:08, 7074.40 examples/s]Map:  90%|████████▉ | 495000/551796 [01:14<00:07, 7152.56 examples/s]Map:  90%|████████▉ | 496000/551796 [01:14<00:07, 7204.10 examples/s]Map:  90%|█████████ | 497000/551796 [01:14<00:07, 7243.38 examples/s]Map:  90%|█████████ | 498000/551796 [01:15<00:07, 7263.56 examples/s]Map:  90%|█████████ | 499000/551796 [01:15<00:10, 5185.56 examples/s]Map:  91%|█████████ | 500000/551796 [01:15<00:09, 5660.99 examples/s]Map:  91%|█████████ | 501000/551796 [01:15<00:08, 6058.70 examples/s]Map:  91%|█████████ | 502000/551796 [01:15<00:07, 6379.32 examples/s]Map:  91%|█████████ | 503000/551796 [01:15<00:07, 6650.91 examples/s]Map:  91%|█████████▏| 504000/551796 [01:16<00:07, 6815.98 examples/s]Map:  92%|█████████▏| 505000/551796 [01:16<00:06, 6962.70 examples/s]Map:  92%|█████████▏| 506000/551796 [01:16<00:06, 7081.95 examples/s]Map:  92%|█████████▏| 507000/551796 [01:16<00:06, 7142.50 examples/s]Map:  92%|█████████▏| 508000/551796 [01:16<00:06, 7164.21 examples/s]Map:  92%|█████████▏| 509000/551796 [01:16<00:05, 7201.75 examples/s]Map:  92%|█████████▏| 510000/551796 [01:16<00:05, 7227.54 examples/s]Map:  93%|█████████▎| 511000/551796 [01:17<00:07, 5206.25 examples/s]Map:  93%|█████████▎| 512000/551796 [01:17<00:06, 5691.96 examples/s]Map:  93%|█████████▎| 513000/551796 [01:17<00:06, 6088.77 examples/s]Map:  93%|█████████▎| 514000/551796 [01:17<00:05, 6394.55 examples/s]Map:  93%|█████████▎| 515000/551796 [01:17<00:05, 6648.07 examples/s]Map:  94%|█████████▎| 516000/551796 [01:17<00:05, 6812.93 examples/s]Map:  94%|█████████▎| 517000/551796 [01:17<00:05, 6955.89 examples/s]Map:  94%|█████████▍| 518000/551796 [01:18<00:04, 7023.18 examples/s]Map:  94%|█████████▍| 519000/551796 [01:18<00:04, 7106.64 examples/s]Map:  94%|█████████▍| 520000/551796 [01:18<00:04, 7156.15 examples/s]Map:  94%|█████████▍| 521000/551796 [01:18<00:04, 7199.50 examples/s]Map:  95%|█████████▍| 522000/551796 [01:18<00:04, 7213.37 examples/s]Map:  95%|█████████▍| 523000/551796 [01:19<00:05, 5198.93 examples/s]Map:  95%|█████████▍| 524000/551796 [01:19<00:04, 5694.77 examples/s]Map:  95%|█████████▌| 525000/551796 [01:19<00:04, 6098.50 examples/s]Map:  95%|█████████▌| 526000/551796 [01:19<00:04, 6414.87 examples/s]Map:  96%|█████████▌| 527000/551796 [01:19<00:03, 6648.37 examples/s]Map:  96%|█████████▌| 528000/551796 [01:19<00:03, 6828.88 examples/s]Map:  96%|█████████▌| 529000/551796 [01:19<00:03, 6967.25 examples/s]Map:  96%|█████████▌| 530000/551796 [01:19<00:03, 7051.73 examples/s]Map:  96%|█████████▌| 531000/551796 [01:20<00:02, 7104.99 examples/s]Map:  96%|█████████▋| 532000/551796 [01:20<00:02, 7150.59 examples/s]Map:  97%|█████████▋| 533000/551796 [01:20<00:02, 7193.24 examples/s]Map:  97%|█████████▋| 534000/551796 [01:20<00:02, 7198.28 examples/s]Map:  97%|█████████▋| 535000/551796 [01:20<00:03, 5177.38 examples/s]Map:  97%|█████████▋| 536000/551796 [01:20<00:02, 5660.39 examples/s]Map:  97%|█████████▋| 537000/551796 [01:21<00:02, 6070.41 examples/s]Map:  97%|█████████▋| 538000/551796 [01:21<00:02, 6387.52 examples/s]Map:  98%|█████████▊| 539000/551796 [01:21<00:01, 6617.09 examples/s]Map:  98%|█████████▊| 540000/551796 [01:21<00:01, 6779.66 examples/s]Map:  98%|█████████▊| 541000/551796 [01:21<00:01, 6927.04 examples/s]Map:  98%|█████████▊| 542000/551796 [01:21<00:01, 6996.98 examples/s]Map:  98%|█████████▊| 543000/551796 [01:21<00:01, 7038.53 examples/s]Map:  99%|█████████▊| 544000/551796 [01:22<00:01, 7091.94 examples/s]Map:  99%|█████████▉| 545000/551796 [01:22<00:00, 7119.65 examples/s]Map:  99%|█████████▉| 546000/551796 [01:22<00:00, 7169.48 examples/s]Map:  99%|█████████▉| 547000/551796 [01:22<00:00, 5161.86 examples/s]Map:  99%|█████████▉| 548000/551796 [01:22<00:00, 5639.40 examples/s]Map:  99%|█████████▉| 549000/551796 [01:22<00:00, 6044.50 examples/s]Map: 100%|█████████▉| 550000/551796 [01:23<00:00, 6358.04 examples/s]Map: 100%|█████████▉| 551000/551796 [01:23<00:00, 6612.63 examples/s]Map: 100%|██████████| 551796/551796 [01:23<00:00, 6735.75 examples/s]Map: 100%|██████████| 551796/551796 [01:23<00:00, 6609.64 examples/s]
Map:   0%|          | 0/916 [00:00<?, ? examples/s]Map: 100%|██████████| 916/916 [00:00<00:00, 7297.73 examples/s]Map: 100%|██████████| 916/916 [00:00<00:00, 6956.16 examples/s]
Map:   0%|          | 0/904 [00:00<?, ? examples/s]Map: 100%|██████████| 904/904 [00:00<00:00, 14650.55 examples/s]
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 19163, 19593, 31, 2370, 7, 1515, 35, 50118, 35007, 35, 111, 370, 58, 23, 10, 2353, 912, 20479, 123, 328, 5457, 1515, 35, 1437]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
[2, 12, 468, 1827, 1437, 10221, 324, 329, 1177, 2341, 263, 748, 1827, 48406, 16151, 6534, 784, 108, 6166, 5563, 90, 263, 2353, 328]
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 19163, 19593, 31, 2370, 7, 1515, 35, 50118, 35007, 35, 111, 3394, 18, 164, 7, 1067, 7, 5, 341, 512, 32656, 116, 5457, 1515, 35, 1437]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
[2, 12444, 118, 13205, 2242, 1371, 8477, 9194, 16235, 263, 17377, 37529, 17487, 1776, 34776, 4]
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 19163, 19593, 31, 2370, 7, 1515, 35, 50118, 35007, 35, 226, 4450, 9240, 1536, 5457, 1515, 35, 1437]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
[2, 37434, 1535, 9240, 8624]
[2, 2, 2, 2, 2, 2, 19163, 19593, 31, 2370, 7, 1515, 35, 50118, 35007, 35, 854, 14684, 725, 6, 28842, 108, 104, 19551, 12408, 38, 37279, 13387, 230, 33835, 8438, 3063, 40930, 4, 5457, 1515, 35, 1437]
[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
[2, 495, 5110, 6, 4112, 162, 1077, 5655, 734]
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 19163, 19593, 31, 2370, 7, 1515, 35, 50118, 35007, 35, 2155, 8, 38, 58, 11, 10, 512, 2058, 4, 5457, 1515, 35, 1437]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
[2, 22611, 4400, 7458, 118, 1437, 10221, 2485, 385, 1253, 542, 3213, 263, 17377, 19103, 734]
Generation Config OPT: GenerationConfig {
  "bos_token_id": 2,
  "eos_token_id": 2,
  "pad_token_id": 1
}

Batching examples:   0%|          | 0/904 [00:00<?, ? examples/s]Batching examples: 100%|██████████| 904/904 [00:00<00:00, 18353.51 examples/s]
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Model facebook/opt-1.3b Trainable Parameters:
trainable params: 3,145,728 || all params: 1,318,891,520 || trainable%: 0.2385
/home/alumno.upv.es/gdipal1/MachineTranslation-UPV/A2-CourseworkNeuralModels/experiments_opt.py:339: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  return Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.
The model is already on multiple devices. Skipping the move to device specified in `args`.
{'loss': 1.8946, 'grad_norm': 0.8771408796310425, 'learning_rate': 9.883971152727695e-05, 'epoch': 0.028995592669914173}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4975, 'grad_norm': 0.714546799659729, 'learning_rate': 9.738571594742353e-05, 'epoch': 0.05799118533982835}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4737, 'grad_norm': 0.6810775399208069, 'learning_rate': 9.593172036757009e-05, 'epoch': 0.08698677800974251}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4681, 'grad_norm': 0.7027797102928162, 'learning_rate': 9.447772478771664e-05, 'epoch': 0.1159823706796567}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4592, 'grad_norm': 0.6728222370147705, 'learning_rate': 9.302372920786321e-05, 'epoch': 0.14497796334957086}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4564, 'grad_norm': 0.6695721745491028, 'learning_rate': 9.156973362800979e-05, 'epoch': 0.17397355601948503}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4539, 'grad_norm': 0.6813843846321106, 'learning_rate': 9.011573804815634e-05, 'epoch': 0.20296914868939922}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4415, 'grad_norm': 0.7534464597702026, 'learning_rate': 8.86617424683029e-05, 'epoch': 0.2319647413593134}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.446, 'grad_norm': 0.7197200655937195, 'learning_rate': 8.720774688844946e-05, 'epoch': 0.2609603340292276}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4327, 'grad_norm': 0.6994686722755432, 'learning_rate': 8.575375130859603e-05, 'epoch': 0.2899559266991417}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4377, 'grad_norm': 0.647472620010376, 'learning_rate': 8.429975572874259e-05, 'epoch': 0.3189515193690559}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4303, 'grad_norm': 0.6871551871299744, 'learning_rate': 8.284576014888916e-05, 'epoch': 0.34794711203897005}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4273, 'grad_norm': 0.7339317798614502, 'learning_rate': 8.139176456903571e-05, 'epoch': 0.37694270470888425}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4231, 'grad_norm': 0.6714032292366028, 'learning_rate': 7.993776898918227e-05, 'epoch': 0.40593829737879844}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4242, 'grad_norm': 0.7269977927207947, 'learning_rate': 7.848377340932884e-05, 'epoch': 0.4349338900487126}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4204, 'grad_norm': 0.641823947429657, 'learning_rate': 7.70297778294754e-05, 'epoch': 0.4639294827186268}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4175, 'grad_norm': 0.7322521805763245, 'learning_rate': 7.557578224962196e-05, 'epoch': 0.49292507538854097}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4084, 'grad_norm': 0.6519210934638977, 'learning_rate': 7.412178666976853e-05, 'epoch': 0.5219206680584552}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.41, 'grad_norm': 0.689352810382843, 'learning_rate': 7.266779108991509e-05, 'epoch': 0.5509162607283693}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4104, 'grad_norm': 0.6458583474159241, 'learning_rate': 7.121379551006166e-05, 'epoch': 0.5799118533982834}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4055, 'grad_norm': 0.7192828059196472, 'learning_rate': 6.975979993020821e-05, 'epoch': 0.6089074460681977}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.405, 'grad_norm': 0.6509802341461182, 'learning_rate': 6.830580435035477e-05, 'epoch': 0.6379030387381118}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4016, 'grad_norm': 0.7268456220626831, 'learning_rate': 6.685180877050135e-05, 'epoch': 0.666898631408026}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4058, 'grad_norm': 0.6802828907966614, 'learning_rate': 6.53978131906479e-05, 'epoch': 0.6958942240779401}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4078, 'grad_norm': 0.8895373344421387, 'learning_rate': 6.394381761079446e-05, 'epoch': 0.7248898167478544}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4007, 'grad_norm': 0.6857932806015015, 'learning_rate': 6.248982203094103e-05, 'epoch': 0.7538854094177685}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3941, 'grad_norm': 0.731391191482544, 'learning_rate': 6.103582645108759e-05, 'epoch': 0.7828810020876826}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.407, 'grad_norm': 0.766973614692688, 'learning_rate': 5.958183087123416e-05, 'epoch': 0.8118765947575969}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.395, 'grad_norm': 0.6831172704696655, 'learning_rate': 5.8127835291380715e-05, 'epoch': 0.840872187427511}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3946, 'grad_norm': 0.7417900562286377, 'learning_rate': 5.667383971152728e-05, 'epoch': 0.8698677800974252}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3903, 'grad_norm': 0.6611849665641785, 'learning_rate': 5.5219844131673836e-05, 'epoch': 0.8988633727673394}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3944, 'grad_norm': 0.6670522689819336, 'learning_rate': 5.376584855182041e-05, 'epoch': 0.9278589654372535}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3919, 'grad_norm': 0.732247531414032, 'learning_rate': 5.231185297196697e-05, 'epoch': 0.9568545581071677}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3965, 'grad_norm': 0.7052138447761536, 'learning_rate': 5.085785739211353e-05, 'epoch': 0.9858501507770819}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'eval_loss': 1.5153685808181763, 'eval_runtime': 2.0259, 'eval_samples_per_second': 452.142, 'eval_steps_per_second': 14.315, 'epoch': 1.0}
{'loss': 1.384, 'grad_norm': 0.7548350691795349, 'learning_rate': 4.940386181226009e-05, 'epoch': 1.014845743446996}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3822, 'grad_norm': 0.76758873462677, 'learning_rate': 4.794986623240666e-05, 'epoch': 1.0438413361169103}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3793, 'grad_norm': 0.6705727577209473, 'learning_rate': 4.649587065255322e-05, 'epoch': 1.0728369287868245}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.389, 'grad_norm': 0.7644495368003845, 'learning_rate': 4.504187507269978e-05, 'epoch': 1.1018325214567386}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.39, 'grad_norm': 0.7293086647987366, 'learning_rate': 4.358787949284634e-05, 'epoch': 1.1308281141266527}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3788, 'grad_norm': 0.7779750227928162, 'learning_rate': 4.213388391299291e-05, 'epoch': 1.1598237067965669}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.382, 'grad_norm': 0.7380210161209106, 'learning_rate': 4.067988833313947e-05, 'epoch': 1.188819299466481}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3763, 'grad_norm': 0.7276481986045837, 'learning_rate': 3.922589275328603e-05, 'epoch': 1.2178148921363952}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3769, 'grad_norm': 0.6999963521957397, 'learning_rate': 3.77718971734326e-05, 'epoch': 1.2468104848063095}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3756, 'grad_norm': 0.7640662789344788, 'learning_rate': 3.631790159357916e-05, 'epoch': 1.2758060774762237}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3764, 'grad_norm': 0.7188234925270081, 'learning_rate': 3.486390601372572e-05, 'epoch': 1.3048016701461378}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3653, 'grad_norm': 0.7645286917686462, 'learning_rate': 3.3409910433872286e-05, 'epoch': 1.333797262816052}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3754, 'grad_norm': 0.7547684907913208, 'learning_rate': 3.195591485401885e-05, 'epoch': 1.362792855485966}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3813, 'grad_norm': 0.7856164574623108, 'learning_rate': 3.0501919274165407e-05, 'epoch': 1.3917884481558804}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.375, 'grad_norm': 0.6897813677787781, 'learning_rate': 2.9047923694311968e-05, 'epoch': 1.4207840408257946}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.374, 'grad_norm': 0.7205057740211487, 'learning_rate': 2.7593928114458532e-05, 'epoch': 1.4497796334957087}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3761, 'grad_norm': 0.7226712703704834, 'learning_rate': 2.6139932534605093e-05, 'epoch': 1.4787752261656228}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3778, 'grad_norm': 0.7268203496932983, 'learning_rate': 2.468593695475166e-05, 'epoch': 1.507770818835537}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3799, 'grad_norm': 0.8192200660705566, 'learning_rate': 2.323194137489822e-05, 'epoch': 1.5367664115054511}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3765, 'grad_norm': 0.8471418023109436, 'learning_rate': 2.1777945795044786e-05, 'epoch': 1.5657620041753653}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3761, 'grad_norm': 0.759772002696991, 'learning_rate': 2.0323950215191347e-05, 'epoch': 1.5947575968452794}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3689, 'grad_norm': 0.7434905767440796, 'learning_rate': 1.886995463533791e-05, 'epoch': 1.6237531895151935}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3693, 'grad_norm': 0.7258458733558655, 'learning_rate': 1.741595905548447e-05, 'epoch': 1.652748782185108}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3705, 'grad_norm': 0.6937754154205322, 'learning_rate': 1.5961963475631032e-05, 'epoch': 1.681744374855022}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3733, 'grad_norm': 0.7113621234893799, 'learning_rate': 1.4507967895777597e-05, 'epoch': 1.7107399675249362}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3665, 'grad_norm': 0.7378513216972351, 'learning_rate': 1.3053972315924159e-05, 'epoch': 1.7397355601948505}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3678, 'grad_norm': 0.7888792157173157, 'learning_rate': 1.1599976736070722e-05, 'epoch': 1.7687311528647647}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3741, 'grad_norm': 0.7539795637130737, 'learning_rate': 1.0145981156217286e-05, 'epoch': 1.7977267455346788}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3692, 'grad_norm': 0.7753452062606812, 'learning_rate': 8.691985576363848e-06, 'epoch': 1.826722338204593}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3701, 'grad_norm': 0.8244947195053101, 'learning_rate': 7.237989996510411e-06, 'epoch': 1.855717930874507}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3685, 'grad_norm': 0.8068719506263733, 'learning_rate': 5.783994416656974e-06, 'epoch': 1.8847135235444212}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3677, 'grad_norm': 0.7423831820487976, 'learning_rate': 4.329998836803537e-06, 'epoch': 1.9137091162143354}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3684, 'grad_norm': 0.6973898410797119, 'learning_rate': 2.876003256950099e-06, 'epoch': 1.9427047088842495}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3694, 'grad_norm': 0.7138547301292419, 'learning_rate': 1.4220076770966618e-06, 'epoch': 1.9717003015541636}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'eval_loss': 1.498677372932434, 'eval_runtime': 2.0252, 'eval_samples_per_second': 452.301, 'eval_steps_per_second': 14.32, 'epoch': 2.0}
{'train_runtime': 5483.3351, 'train_samples_per_second': 201.263, 'train_steps_per_second': 6.29, 'train_loss': 1.4049403669109093, 'epoch': 2.0}
Evaluating model: opt_lora_5e5 with num_beams=1 and lr=5e-05
Using default tokenizer.
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python experiments_opt.py ...
💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
{'BLEU': 13.8289, 'rogueL': 0.3651, 'COMET': 63.8587, 'chrF': 35.4272, 'gen_len': 1.0, 'model_name': 'opt_lora_5e5', 'num_beams': 1}
Evaluating model: opt_lora_5e5 with num_beams=4 and lr=5e-05
Using default tokenizer.
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python experiments_opt.py ...
💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
{'BLEU': 17.3342, 'rogueL': 0.407, 'COMET': 66.9774, 'chrF': 38.9686, 'gen_len': 1.0, 'model_name': 'opt_lora_5e5', 'num_beams': 4}
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Model facebook/opt-1.3b Trainable Parameters:
trainable params: 3,145,728 || all params: 1,318,891,520 || trainable%: 0.2385
/home/alumno.upv.es/gdipal1/MachineTranslation-UPV/A2-CourseworkNeuralModels/experiments_opt.py:339: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  return Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.
The model is already on multiple devices. Skipping the move to device specified in `args`.
{'loss': 2.0372, 'grad_norm': 1.1765190362930298, 'learning_rate': 4.9419855763638475e-05, 'epoch': 0.028995592669914173}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5167, 'grad_norm': 0.8437642455101013, 'learning_rate': 4.8692857973711764e-05, 'epoch': 0.05799118533982835}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4893, 'grad_norm': 0.9253120422363281, 'learning_rate': 4.7965860183785046e-05, 'epoch': 0.08698677800974251}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4816, 'grad_norm': 0.9406810998916626, 'learning_rate': 4.723886239385832e-05, 'epoch': 0.1159823706796567}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4723, 'grad_norm': 0.8773737549781799, 'learning_rate': 4.6511864603931604e-05, 'epoch': 0.14497796334957086}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4702, 'grad_norm': 0.888645589351654, 'learning_rate': 4.578486681400489e-05, 'epoch': 0.17397355601948503}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4678, 'grad_norm': 0.9468318223953247, 'learning_rate': 4.505786902407817e-05, 'epoch': 0.20296914868939922}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4565, 'grad_norm': 0.9585760235786438, 'learning_rate': 4.433087123415145e-05, 'epoch': 0.2319647413593134}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4615, 'grad_norm': 0.9639074802398682, 'learning_rate': 4.360387344422473e-05, 'epoch': 0.2609603340292276}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4488, 'grad_norm': 0.9105509519577026, 'learning_rate': 4.2876875654298014e-05, 'epoch': 0.2899559266991417}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4541, 'grad_norm': 0.9117385149002075, 'learning_rate': 4.2149877864371296e-05, 'epoch': 0.3189515193690559}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4472, 'grad_norm': 0.9286637902259827, 'learning_rate': 4.142288007444458e-05, 'epoch': 0.34794711203897005}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4446, 'grad_norm': 1.0328513383865356, 'learning_rate': 4.0695882284517854e-05, 'epoch': 0.37694270470888425}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4407, 'grad_norm': 0.9019690155982971, 'learning_rate': 3.9968884494591136e-05, 'epoch': 0.40593829737879844}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4427, 'grad_norm': 1.006585955619812, 'learning_rate': 3.924188670466442e-05, 'epoch': 0.4349338900487126}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4395, 'grad_norm': 0.8925952315330505, 'learning_rate': 3.85148889147377e-05, 'epoch': 0.4639294827186268}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4368, 'grad_norm': 1.0145217180252075, 'learning_rate': 3.778789112481098e-05, 'epoch': 0.49292507538854097}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4286, 'grad_norm': 0.8920018672943115, 'learning_rate': 3.7060893334884264e-05, 'epoch': 0.5219206680584552}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.43, 'grad_norm': 0.91417396068573, 'learning_rate': 3.6333895544957546e-05, 'epoch': 0.5509162607283693}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4311, 'grad_norm': 0.8800697326660156, 'learning_rate': 3.560689775503083e-05, 'epoch': 0.5799118533982834}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4268, 'grad_norm': 0.9859825372695923, 'learning_rate': 3.4879899965104104e-05, 'epoch': 0.6089074460681977}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4257, 'grad_norm': 0.9265738129615784, 'learning_rate': 3.4152902175177386e-05, 'epoch': 0.6379030387381118}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4231, 'grad_norm': 1.0022757053375244, 'learning_rate': 3.3425904385250675e-05, 'epoch': 0.666898631408026}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4278, 'grad_norm': 0.9279288053512573, 'learning_rate': 3.269890659532395e-05, 'epoch': 0.6958942240779401}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4304, 'grad_norm': 1.3898203372955322, 'learning_rate': 3.197190880539723e-05, 'epoch': 0.7248898167478544}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4229, 'grad_norm': 0.9316128492355347, 'learning_rate': 3.1244911015470514e-05, 'epoch': 0.7538854094177685}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4167, 'grad_norm': 0.9412010312080383, 'learning_rate': 3.0517913225543797e-05, 'epoch': 0.7828810020876826}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4304, 'grad_norm': 0.9849230051040649, 'learning_rate': 2.979091543561708e-05, 'epoch': 0.8118765947575969}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.419, 'grad_norm': 0.9197087287902832, 'learning_rate': 2.9063917645690357e-05, 'epoch': 0.840872187427511}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4188, 'grad_norm': 0.9937001466751099, 'learning_rate': 2.833691985576364e-05, 'epoch': 0.8698677800974252}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4145, 'grad_norm': 0.8962485194206238, 'learning_rate': 2.7609922065836918e-05, 'epoch': 0.8988633727673394}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4189, 'grad_norm': 0.8985121250152588, 'learning_rate': 2.6882924275910204e-05, 'epoch': 0.9278589654372535}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4169, 'grad_norm': 0.9574214220046997, 'learning_rate': 2.6155926485983486e-05, 'epoch': 0.9568545581071677}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4212, 'grad_norm': 1.008346438407898, 'learning_rate': 2.5428928696056764e-05, 'epoch': 0.9858501507770819}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'eval_loss': 1.5375205278396606, 'eval_runtime': 2.026, 'eval_samples_per_second': 452.116, 'eval_steps_per_second': 14.314, 'epoch': 1.0}
{'loss': 1.4105, 'grad_norm': 1.0267332792282104, 'learning_rate': 2.4701930906130047e-05, 'epoch': 1.014845743446996}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4099, 'grad_norm': 1.025104284286499, 'learning_rate': 2.397493311620333e-05, 'epoch': 1.0438413361169103}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4068, 'grad_norm': 0.9117987751960754, 'learning_rate': 2.324793532627661e-05, 'epoch': 1.0728369287868245}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4175, 'grad_norm': 1.0363852977752686, 'learning_rate': 2.252093753634989e-05, 'epoch': 1.1018325214567386}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4178, 'grad_norm': 0.9802224636077881, 'learning_rate': 2.179393974642317e-05, 'epoch': 1.1308281141266527}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4069, 'grad_norm': 0.9761452078819275, 'learning_rate': 2.1066941956496454e-05, 'epoch': 1.1598237067965669}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4106, 'grad_norm': 1.0075982809066772, 'learning_rate': 2.0339944166569736e-05, 'epoch': 1.188819299466481}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4055, 'grad_norm': 0.9298469424247742, 'learning_rate': 1.9612946376643014e-05, 'epoch': 1.2178148921363952}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4051, 'grad_norm': 0.9480811953544617, 'learning_rate': 1.88859485867163e-05, 'epoch': 1.2468104848063095}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4045, 'grad_norm': 1.04381263256073, 'learning_rate': 1.815895079678958e-05, 'epoch': 1.2758060774762237}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4055, 'grad_norm': 0.979955792427063, 'learning_rate': 1.743195300686286e-05, 'epoch': 1.3048016701461378}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3946, 'grad_norm': 1.0934981107711792, 'learning_rate': 1.6704955216936143e-05, 'epoch': 1.333797262816052}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4043, 'grad_norm': 1.024436593055725, 'learning_rate': 1.5977957427009425e-05, 'epoch': 1.362792855485966}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4106, 'grad_norm': 1.0832239389419556, 'learning_rate': 1.5250959637082704e-05, 'epoch': 1.3917884481558804}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4043, 'grad_norm': 0.9426436424255371, 'learning_rate': 1.4523961847155984e-05, 'epoch': 1.4207840408257946}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.403, 'grad_norm': 1.0009671449661255, 'learning_rate': 1.3796964057229266e-05, 'epoch': 1.4497796334957087}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4056, 'grad_norm': 0.9520122408866882, 'learning_rate': 1.3069966267302547e-05, 'epoch': 1.4787752261656228}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4069, 'grad_norm': 0.9861910939216614, 'learning_rate': 1.234296847737583e-05, 'epoch': 1.507770818835537}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4093, 'grad_norm': 1.0907083749771118, 'learning_rate': 1.161597068744911e-05, 'epoch': 1.5367664115054511}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4065, 'grad_norm': 1.1344178915023804, 'learning_rate': 1.0888972897522393e-05, 'epoch': 1.5657620041753653}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4061, 'grad_norm': 0.9970554113388062, 'learning_rate': 1.0161975107595673e-05, 'epoch': 1.5947575968452794}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3993, 'grad_norm': 0.9913712739944458, 'learning_rate': 9.434977317668955e-06, 'epoch': 1.6237531895151935}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3993, 'grad_norm': 0.9477884769439697, 'learning_rate': 8.707979527742236e-06, 'epoch': 1.652748782185108}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4002, 'grad_norm': 0.9272667169570923, 'learning_rate': 7.980981737815516e-06, 'epoch': 1.681744374855022}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.403, 'grad_norm': 0.965908408164978, 'learning_rate': 7.253983947888798e-06, 'epoch': 1.7107399675249362}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3962, 'grad_norm': 0.9994738101959229, 'learning_rate': 6.5269861579620795e-06, 'epoch': 1.7397355601948505}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3972, 'grad_norm': 1.1017208099365234, 'learning_rate': 5.799988368035361e-06, 'epoch': 1.7687311528647647}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4038, 'grad_norm': 0.9956324696540833, 'learning_rate': 5.072990578108643e-06, 'epoch': 1.7977267455346788}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3999, 'grad_norm': 1.0359129905700684, 'learning_rate': 4.345992788181924e-06, 'epoch': 1.826722338204593}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4008, 'grad_norm': 1.085195541381836, 'learning_rate': 3.6189949982552054e-06, 'epoch': 1.855717930874507}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3983, 'grad_norm': 1.0505164861679077, 'learning_rate': 2.891997208328487e-06, 'epoch': 1.8847135235444212}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3979, 'grad_norm': 1.1232409477233887, 'learning_rate': 2.1649994184017683e-06, 'epoch': 1.9137091162143354}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3991, 'grad_norm': 0.9404458403587341, 'learning_rate': 1.4380016284750495e-06, 'epoch': 1.9427047088842495}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.3999, 'grad_norm': 0.9374620318412781, 'learning_rate': 7.110038385483309e-07, 'epoch': 1.9717003015541636}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'eval_loss': 1.524888277053833, 'eval_runtime': 2.0274, 'eval_samples_per_second': 451.806, 'eval_steps_per_second': 14.304, 'epoch': 2.0}
{'train_runtime': 5499.7503, 'train_samples_per_second': 200.662, 'train_steps_per_second': 6.271, 'train_loss': 1.4312147840881702, 'epoch': 2.0}
Evaluating model: opt_lora_1e4 with num_beams=1 and lr=0.0001
Using default tokenizer.
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python experiments_opt.py ...
💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
{'BLEU': 11.3637, 'rogueL': 0.3382, 'COMET': 61.3613, 'chrF': 33.062, 'gen_len': 1.0, 'model_name': 'opt_lora_1e4', 'num_beams': 1}
Evaluating model: opt_lora_1e4 with num_beams=4 and lr=0.0001
Using default tokenizer.
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python experiments_opt.py ...
💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
{'BLEU': 15.6747, 'rogueL': 0.3821, 'COMET': 65.6594, 'chrF': 36.8077, 'gen_len': 1.0, 'model_name': 'opt_lora_1e4', 'num_beams': 4}
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Model facebook/opt-1.3b Trainable Parameters:
trainable params: 442,368 || all params: 1,316,188,160 || trainable%: 0.0336
/home/alumno.upv.es/gdipal1/MachineTranslation-UPV/A2-CourseworkNeuralModels/experiments_opt.py:339: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  return Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.
The model is already on multiple devices. Skipping the move to device specified in `args`.
{'loss': 3.2981, 'grad_norm': 0.19823355972766876, 'learning_rate': 9.883971152727695e-05, 'epoch': 0.028995592669914173}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 2.0602, 'grad_norm': 0.1350824385881424, 'learning_rate': 9.738571594742353e-05, 'epoch': 0.05799118533982835}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.8515, 'grad_norm': 0.10939151793718338, 'learning_rate': 9.593172036757009e-05, 'epoch': 0.08698677800974251}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.7809, 'grad_norm': 0.10656281560659409, 'learning_rate': 9.447772478771664e-05, 'epoch': 0.1159823706796567}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.7218, 'grad_norm': 0.10630859434604645, 'learning_rate': 9.302372920786321e-05, 'epoch': 0.14497796334957086}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.681, 'grad_norm': 0.1269685924053192, 'learning_rate': 9.156973362800979e-05, 'epoch': 0.17397355601948503}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.6431, 'grad_norm': 0.13241198658943176, 'learning_rate': 9.011573804815634e-05, 'epoch': 0.20296914868939922}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.6072, 'grad_norm': 0.12089564651250839, 'learning_rate': 8.86617424683029e-05, 'epoch': 0.2319647413593134}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5967, 'grad_norm': 0.09981849789619446, 'learning_rate': 8.720774688844946e-05, 'epoch': 0.2609603340292276}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5728, 'grad_norm': 0.09583921730518341, 'learning_rate': 8.575375130859603e-05, 'epoch': 0.2899559266991417}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5716, 'grad_norm': 0.09133627265691757, 'learning_rate': 8.429975572874259e-05, 'epoch': 0.3189515193690559}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5601, 'grad_norm': 0.11483851075172424, 'learning_rate': 8.284576014888916e-05, 'epoch': 0.34794711203897005}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5505, 'grad_norm': 0.1089823842048645, 'learning_rate': 8.139176456903571e-05, 'epoch': 0.37694270470888425}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5425, 'grad_norm': 0.10126551240682602, 'learning_rate': 7.993776898918227e-05, 'epoch': 0.40593829737879844}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5442, 'grad_norm': 0.11447664350271225, 'learning_rate': 7.848377340932884e-05, 'epoch': 0.4349338900487126}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5388, 'grad_norm': 0.0895242989063263, 'learning_rate': 7.70297778294754e-05, 'epoch': 0.4639294827186268}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5324, 'grad_norm': 0.11324650794267654, 'learning_rate': 7.557578224962196e-05, 'epoch': 0.49292507538854097}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5234, 'grad_norm': 0.08773370087146759, 'learning_rate': 7.412178666976853e-05, 'epoch': 0.5219206680584552}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.522, 'grad_norm': 0.10264915227890015, 'learning_rate': 7.266779108991509e-05, 'epoch': 0.5509162607283693}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5212, 'grad_norm': 0.0890517607331276, 'learning_rate': 7.121379551006166e-05, 'epoch': 0.5799118533982834}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5175, 'grad_norm': 0.09681091457605362, 'learning_rate': 6.975979993020821e-05, 'epoch': 0.6089074460681977}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5149, 'grad_norm': 0.09144436568021774, 'learning_rate': 6.830580435035477e-05, 'epoch': 0.6379030387381118}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5108, 'grad_norm': 0.09554323554039001, 'learning_rate': 6.685180877050135e-05, 'epoch': 0.666898631408026}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.516, 'grad_norm': 0.09369087219238281, 'learning_rate': 6.53978131906479e-05, 'epoch': 0.6958942240779401}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5169, 'grad_norm': 0.11285582929849625, 'learning_rate': 6.394381761079446e-05, 'epoch': 0.7248898167478544}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5092, 'grad_norm': 0.09306000918149948, 'learning_rate': 6.248982203094103e-05, 'epoch': 0.7538854094177685}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5017, 'grad_norm': 0.09048277139663696, 'learning_rate': 6.103582645108759e-05, 'epoch': 0.7828810020876826}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5161, 'grad_norm': 0.09753386676311493, 'learning_rate': 5.958183087123416e-05, 'epoch': 0.8118765947575969}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5032, 'grad_norm': 0.10448847711086273, 'learning_rate': 5.8127835291380715e-05, 'epoch': 0.840872187427511}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5041, 'grad_norm': 0.09828044474124908, 'learning_rate': 5.667383971152728e-05, 'epoch': 0.8698677800974252}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4992, 'grad_norm': 0.08284468948841095, 'learning_rate': 5.5219844131673836e-05, 'epoch': 0.8988633727673394}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5032, 'grad_norm': 0.0830577164888382, 'learning_rate': 5.376584855182041e-05, 'epoch': 0.9278589654372535}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.502, 'grad_norm': 0.09282640367746353, 'learning_rate': 5.231185297196697e-05, 'epoch': 0.9568545581071677}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5049, 'grad_norm': 0.09783480316400528, 'learning_rate': 5.085785739211353e-05, 'epoch': 0.9858501507770819}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'eval_loss': 1.6129497289657593, 'eval_runtime': 2.2861, 'eval_samples_per_second': 400.683, 'eval_steps_per_second': 12.685, 'epoch': 1.0}
{'loss': 1.4977, 'grad_norm': 0.10268191993236542, 'learning_rate': 4.940386181226009e-05, 'epoch': 1.014845743446996}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4956, 'grad_norm': 0.08898946642875671, 'learning_rate': 4.794986623240666e-05, 'epoch': 1.0438413361169103}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4922, 'grad_norm': 0.09220778197050095, 'learning_rate': 4.649587065255322e-05, 'epoch': 1.0728369287868245}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5025, 'grad_norm': 0.09937657415866852, 'learning_rate': 4.504187507269978e-05, 'epoch': 1.1018325214567386}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5033, 'grad_norm': 0.09634817391633987, 'learning_rate': 4.358787949284634e-05, 'epoch': 1.1308281141266527}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4925, 'grad_norm': 0.09803567826747894, 'learning_rate': 4.213388391299291e-05, 'epoch': 1.1598237067965669}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.496, 'grad_norm': 0.09834500402212143, 'learning_rate': 4.067988833313947e-05, 'epoch': 1.188819299466481}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4909, 'grad_norm': 0.08329064399003983, 'learning_rate': 3.922589275328603e-05, 'epoch': 1.2178148921363952}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4921, 'grad_norm': 0.0879826545715332, 'learning_rate': 3.77718971734326e-05, 'epoch': 1.2468104848063095}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4896, 'grad_norm': 0.09437035769224167, 'learning_rate': 3.631790159357916e-05, 'epoch': 1.2758060774762237}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4909, 'grad_norm': 0.09049264341592789, 'learning_rate': 3.486390601372572e-05, 'epoch': 1.3048016701461378}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.48, 'grad_norm': 0.09426531940698624, 'learning_rate': 3.3409910433872286e-05, 'epoch': 1.333797262816052}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4906, 'grad_norm': 0.08861126750707626, 'learning_rate': 3.195591485401885e-05, 'epoch': 1.362792855485966}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4954, 'grad_norm': 0.0909341424703598, 'learning_rate': 3.0501919274165407e-05, 'epoch': 1.3917884481558804}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4897, 'grad_norm': 0.0955660492181778, 'learning_rate': 2.9047923694311968e-05, 'epoch': 1.4207840408257946}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4878, 'grad_norm': 0.08694979548454285, 'learning_rate': 2.7593928114458532e-05, 'epoch': 1.4497796334957087}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4905, 'grad_norm': 0.09116409718990326, 'learning_rate': 2.6139932534605093e-05, 'epoch': 1.4787752261656228}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4927, 'grad_norm': 0.08441159874200821, 'learning_rate': 2.468593695475166e-05, 'epoch': 1.507770818835537}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4942, 'grad_norm': 0.09228187799453735, 'learning_rate': 2.323194137489822e-05, 'epoch': 1.5367664115054511}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4909, 'grad_norm': 0.10178440809249878, 'learning_rate': 2.1777945795044786e-05, 'epoch': 1.5657620041753653}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4916, 'grad_norm': 0.08994750678539276, 'learning_rate': 2.0323950215191347e-05, 'epoch': 1.5947575968452794}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4861, 'grad_norm': 0.08479323983192444, 'learning_rate': 1.886995463533791e-05, 'epoch': 1.6237531895151935}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4849, 'grad_norm': 0.08666571974754333, 'learning_rate': 1.741595905548447e-05, 'epoch': 1.652748782185108}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4861, 'grad_norm': 0.07677895575761795, 'learning_rate': 1.5961963475631032e-05, 'epoch': 1.681744374855022}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.489, 'grad_norm': 0.0922129899263382, 'learning_rate': 1.4507967895777597e-05, 'epoch': 1.7107399675249362}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4828, 'grad_norm': 0.084874726831913, 'learning_rate': 1.3053972315924159e-05, 'epoch': 1.7397355601948505}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.482, 'grad_norm': 0.09352906048297882, 'learning_rate': 1.1599976736070722e-05, 'epoch': 1.7687311528647647}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4888, 'grad_norm': 0.09430665522813797, 'learning_rate': 1.0145981156217286e-05, 'epoch': 1.7977267455346788}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.485, 'grad_norm': 0.09232129156589508, 'learning_rate': 8.691985576363848e-06, 'epoch': 1.826722338204593}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4867, 'grad_norm': 0.11378771811723709, 'learning_rate': 7.237989996510411e-06, 'epoch': 1.855717930874507}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4849, 'grad_norm': 0.10766086727380753, 'learning_rate': 5.783994416656974e-06, 'epoch': 1.8847135235444212}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4842, 'grad_norm': 0.08733274787664413, 'learning_rate': 4.329998836803537e-06, 'epoch': 1.9137091162143354}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4854, 'grad_norm': 0.09347660839557648, 'learning_rate': 2.876003256950099e-06, 'epoch': 1.9427047088842495}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.4867, 'grad_norm': 0.08710689097642899, 'learning_rate': 1.4220076770966618e-06, 'epoch': 1.9717003015541636}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'eval_loss': 1.6016548871994019, 'eval_runtime': 2.2868, 'eval_samples_per_second': 400.568, 'eval_steps_per_second': 12.682, 'epoch': 2.0}
{'train_runtime': 6605.9699, 'train_samples_per_second': 167.06, 'train_steps_per_second': 5.221, 'train_loss': 1.557710202048312, 'epoch': 2.0}
Evaluating model: opt_ia3_5e5 with num_beams=1 and lr=5e-05
Using default tokenizer.
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python experiments_opt.py ...
💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
{'BLEU': 8.4841, 'rogueL': 0.2853, 'COMET': 57.6421, 'chrF': 28.7524, 'gen_len': 1.0, 'model_name': 'opt_ia3_5e5', 'num_beams': 1}
Evaluating model: opt_ia3_5e5 with num_beams=4 and lr=5e-05
Using default tokenizer.
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python experiments_opt.py ...
💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
{'BLEU': 12.264, 'rogueL': 0.3287, 'COMET': 62.4279, 'chrF': 32.1962, 'gen_len': 1.0, 'model_name': 'opt_ia3_5e5', 'num_beams': 4}
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Model facebook/opt-1.3b Trainable Parameters:
trainable params: 442,368 || all params: 1,316,188,160 || trainable%: 0.0336
/home/alumno.upv.es/gdipal1/MachineTranslation-UPV/A2-CourseworkNeuralModels/experiments_opt.py:339: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  return Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.
The model is already on multiple devices. Skipping the move to device specified in `args`.
{'loss': 3.7421, 'grad_norm': 0.31008368730545044, 'learning_rate': 4.9419855763638475e-05, 'epoch': 0.028995592669914173}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 2.6329, 'grad_norm': 0.20796652138233185, 'learning_rate': 4.8692857973711764e-05, 'epoch': 0.05799118533982835}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 2.1077, 'grad_norm': 0.1333353966474533, 'learning_rate': 4.7965860183785046e-05, 'epoch': 0.08698677800974251}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.9409, 'grad_norm': 0.13438278436660767, 'learning_rate': 4.723886239385832e-05, 'epoch': 0.1159823706796567}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.8667, 'grad_norm': 0.11284632235765457, 'learning_rate': 4.6511864603931604e-05, 'epoch': 0.14497796334957086}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.8239, 'grad_norm': 0.11922767758369446, 'learning_rate': 4.578486681400489e-05, 'epoch': 0.17397355601948503}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.7893, 'grad_norm': 0.13101407885551453, 'learning_rate': 4.505786902407817e-05, 'epoch': 0.20296914868939922}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.7506, 'grad_norm': 0.11693107336759567, 'learning_rate': 4.433087123415145e-05, 'epoch': 0.2319647413593134}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.7329, 'grad_norm': 0.11407914012670517, 'learning_rate': 4.360387344422473e-05, 'epoch': 0.2609603340292276}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.6984, 'grad_norm': 0.10734251141548157, 'learning_rate': 4.2876875654298014e-05, 'epoch': 0.2899559266991417}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.6867, 'grad_norm': 0.10418742895126343, 'learning_rate': 4.2149877864371296e-05, 'epoch': 0.3189515193690559}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.6648, 'grad_norm': 0.11665374785661697, 'learning_rate': 4.142288007444458e-05, 'epoch': 0.34794711203897005}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.6419, 'grad_norm': 0.12207271158695221, 'learning_rate': 4.0695882284517854e-05, 'epoch': 0.37694270470888425}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.6254, 'grad_norm': 0.1133200079202652, 'learning_rate': 3.9968884494591136e-05, 'epoch': 0.40593829737879844}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.6192, 'grad_norm': 0.10651244968175888, 'learning_rate': 3.924188670466442e-05, 'epoch': 0.4349338900487126}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.6085, 'grad_norm': 0.09888582676649094, 'learning_rate': 3.85148889147377e-05, 'epoch': 0.4639294827186268}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.597, 'grad_norm': 0.11982372403144836, 'learning_rate': 3.778789112481098e-05, 'epoch': 0.49292507538854097}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5852, 'grad_norm': 0.08932626992464066, 'learning_rate': 3.7060893334884264e-05, 'epoch': 0.5219206680584552}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.58, 'grad_norm': 0.11684508621692657, 'learning_rate': 3.6333895544957546e-05, 'epoch': 0.5509162607283693}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5762, 'grad_norm': 0.10181637853384018, 'learning_rate': 3.560689775503083e-05, 'epoch': 0.5799118533982834}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5716, 'grad_norm': 0.11869444698095322, 'learning_rate': 3.4879899965104104e-05, 'epoch': 0.6089074460681977}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5659, 'grad_norm': 0.10058864206075668, 'learning_rate': 3.4152902175177386e-05, 'epoch': 0.6379030387381118}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5601, 'grad_norm': 0.10920839011669159, 'learning_rate': 3.3425904385250675e-05, 'epoch': 0.666898631408026}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5643, 'grad_norm': 0.09661629796028137, 'learning_rate': 3.269890659532395e-05, 'epoch': 0.6958942240779401}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5635, 'grad_norm': 0.13327953219413757, 'learning_rate': 3.197190880539723e-05, 'epoch': 0.7248898167478544}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5551, 'grad_norm': 0.10362333059310913, 'learning_rate': 3.1244911015470514e-05, 'epoch': 0.7538854094177685}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5463, 'grad_norm': 0.10159848630428314, 'learning_rate': 3.0517913225543797e-05, 'epoch': 0.7828810020876826}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5598, 'grad_norm': 0.09804131835699081, 'learning_rate': 2.979091543561708e-05, 'epoch': 0.8118765947575969}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5462, 'grad_norm': 0.11222312599420547, 'learning_rate': 2.9063917645690357e-05, 'epoch': 0.840872187427511}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5467, 'grad_norm': 0.10637957602739334, 'learning_rate': 2.833691985576364e-05, 'epoch': 0.8698677800974252}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.541, 'grad_norm': 0.09008491039276123, 'learning_rate': 2.7609922065836918e-05, 'epoch': 0.8988633727673394}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5442, 'grad_norm': 0.08985596895217896, 'learning_rate': 2.6882924275910204e-05, 'epoch': 0.9278589654372535}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5429, 'grad_norm': 0.09783065319061279, 'learning_rate': 2.6155926485983486e-05, 'epoch': 0.9568545581071677}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5443, 'grad_norm': 0.10353453457355499, 'learning_rate': 2.5428928696056764e-05, 'epoch': 0.9858501507770819}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'eval_loss': 1.644293189048767, 'eval_runtime': 2.3002, 'eval_samples_per_second': 398.221, 'eval_steps_per_second': 12.607, 'epoch': 1.0}
{'loss': 1.5379, 'grad_norm': 0.10625136643648148, 'learning_rate': 2.4701930906130047e-05, 'epoch': 1.014845743446996}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5353, 'grad_norm': 0.09285727143287659, 'learning_rate': 2.397493311620333e-05, 'epoch': 1.0438413361169103}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5317, 'grad_norm': 0.08960399776697159, 'learning_rate': 2.324793532627661e-05, 'epoch': 1.0728369287868245}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5411, 'grad_norm': 0.11073419451713562, 'learning_rate': 2.252093753634989e-05, 'epoch': 1.1018325214567386}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5421, 'grad_norm': 0.10499781370162964, 'learning_rate': 2.179393974642317e-05, 'epoch': 1.1308281141266527}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5303, 'grad_norm': 0.10080563277006149, 'learning_rate': 2.1066941956496454e-05, 'epoch': 1.1598237067965669}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5339, 'grad_norm': 0.10099519044160843, 'learning_rate': 2.0339944166569736e-05, 'epoch': 1.188819299466481}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5283, 'grad_norm': 0.08462749421596527, 'learning_rate': 1.9612946376643014e-05, 'epoch': 1.2178148921363952}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5294, 'grad_norm': 0.08813449740409851, 'learning_rate': 1.88859485867163e-05, 'epoch': 1.2468104848063095}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5265, 'grad_norm': 0.09861095994710922, 'learning_rate': 1.815895079678958e-05, 'epoch': 1.2758060774762237}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5277, 'grad_norm': 0.09582715481519699, 'learning_rate': 1.743195300686286e-05, 'epoch': 1.3048016701461378}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5164, 'grad_norm': 0.09821341931819916, 'learning_rate': 1.6704955216936143e-05, 'epoch': 1.333797262816052}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5267, 'grad_norm': 0.09008031338453293, 'learning_rate': 1.5977957427009425e-05, 'epoch': 1.362792855485966}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5309, 'grad_norm': 0.09864025563001633, 'learning_rate': 1.5250959637082704e-05, 'epoch': 1.3917884481558804}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5257, 'grad_norm': 0.10240889340639114, 'learning_rate': 1.4523961847155984e-05, 'epoch': 1.4207840408257946}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5235, 'grad_norm': 0.0893547311425209, 'learning_rate': 1.3796964057229266e-05, 'epoch': 1.4497796334957087}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5261, 'grad_norm': 0.09472748637199402, 'learning_rate': 1.3069966267302547e-05, 'epoch': 1.4787752261656228}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5281, 'grad_norm': 0.09463916718959808, 'learning_rate': 1.234296847737583e-05, 'epoch': 1.507770818835537}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.53, 'grad_norm': 0.10149039328098297, 'learning_rate': 1.161597068744911e-05, 'epoch': 1.5367664115054511}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.526, 'grad_norm': 0.11554046720266342, 'learning_rate': 1.0888972897522393e-05, 'epoch': 1.5657620041753653}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5269, 'grad_norm': 0.09178656339645386, 'learning_rate': 1.0161975107595673e-05, 'epoch': 1.5947575968452794}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5216, 'grad_norm': 0.08743736892938614, 'learning_rate': 9.434977317668955e-06, 'epoch': 1.6237531895151935}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5204, 'grad_norm': 0.09353647381067276, 'learning_rate': 8.707979527742236e-06, 'epoch': 1.652748782185108}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.521, 'grad_norm': 0.08210296183824539, 'learning_rate': 7.980981737815516e-06, 'epoch': 1.681744374855022}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5237, 'grad_norm': 0.09493698179721832, 'learning_rate': 7.253983947888798e-06, 'epoch': 1.7107399675249362}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5174, 'grad_norm': 0.08647996932268143, 'learning_rate': 6.5269861579620795e-06, 'epoch': 1.7397355601948505}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5169, 'grad_norm': 0.10170598328113556, 'learning_rate': 5.799988368035361e-06, 'epoch': 1.7687311528647647}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5233, 'grad_norm': 0.09631609916687012, 'learning_rate': 5.072990578108643e-06, 'epoch': 1.7977267455346788}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5192, 'grad_norm': 0.10240058600902557, 'learning_rate': 4.345992788181924e-06, 'epoch': 1.826722338204593}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5215, 'grad_norm': 0.10536500811576843, 'learning_rate': 3.6189949982552054e-06, 'epoch': 1.855717930874507}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5195, 'grad_norm': 0.10232336819171906, 'learning_rate': 2.891997208328487e-06, 'epoch': 1.8847135235444212}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5183, 'grad_norm': 0.09199066460132599, 'learning_rate': 2.1649994184017683e-06, 'epoch': 1.9137091162143354}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5202, 'grad_norm': 0.09861557930707932, 'learning_rate': 1.4380016284750495e-06, 'epoch': 1.9427047088842495}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 1.5214, 'grad_norm': 0.09291943162679672, 'learning_rate': 7.110038385483309e-07, 'epoch': 1.9717003015541636}
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'eval_loss': 1.6304030418395996, 'eval_runtime': 2.2973, 'eval_samples_per_second': 398.725, 'eval_steps_per_second': 12.623, 'epoch': 2.0}
{'train_runtime': 6642.2079, 'train_samples_per_second': 166.148, 'train_steps_per_second': 5.192, 'train_loss': 1.6293993998337057, 'epoch': 2.0}
Evaluating model: opt_ia3_1e4 with num_beams=1 and lr=0.0001
Using default tokenizer.
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python experiments_opt.py ...
💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
{'BLEU': 7.0337, 'rogueL': 0.2609, 'COMET': 55.5118, 'chrF': 27.0348, 'gen_len': 1.0, 'model_name': 'opt_ia3_1e4', 'num_beams': 1}
Evaluating model: opt_ia3_1e4 with num_beams=4 and lr=0.0001
Using default tokenizer.
/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python experiments_opt.py ...
💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
{'BLEU': 11.0475, 'rogueL': 0.3053, 'COMET': 60.2169, 'chrF': 30.6911, 'gen_len': 1.0, 'model_name': 'opt_ia3_1e4', 'num_beams': 4}
Results saved to cascade_finetuned.csv
