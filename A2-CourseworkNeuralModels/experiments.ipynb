{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f539d5b",
   "metadata": {},
   "source": [
    "# A2 Coursework Neural Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7539bde",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7567c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d889ff14ce546ed9f299f8fcdb22f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import Trainer, Seq2SeqTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "from transformers import DataCollatorForSeq2Seq, DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments, Seq2SeqTrainingArguments \n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from evaluate import load\n",
    "\n",
    "\n",
    "login(token=\"\")\n",
    "\n",
    "bleu_metric = load(\"sacrebleu\")\n",
    "comet_metric = load(\"comet\")\n",
    "\n",
    "DEBUG_MODE = True\n",
    "DEBUG_FRACTION = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4eb0704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "opus_dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-fr\")\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    opus_dataset = DatasetDict({\n",
    "        split: opus_dataset[split]\n",
    "            .shuffle(seed=42)\n",
    "            .select(range(int(len(opus_dataset[split]) * DEBUG_FRACTION)))\n",
    "        for split in opus_dataset.keys()\n",
    "    })\n",
    "\n",
    "print(opus_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3ca45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tok_length = 16\n",
    "src_code = \"eng_Latn\"\n",
    "tgt_code = \"fra_Latn\"\n",
    "\n",
    "checkpoint_nllb = \"facebook/nllb-200-distilled-600M\"\n",
    "checkpoint_llama = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "tokenizer_nllb = AutoTokenizer.from_pretrained(\n",
    "    checkpoint_nllb, \n",
    "    padding=True, \n",
    "    pad_to_multiple_of=8, \n",
    "    src_lang=src_code, \n",
    "    tgt_lang=tgt_code, \n",
    "    truncation=True, \n",
    "    max_length=max_tok_length,\n",
    ")\n",
    "\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(\n",
    "    checkpoint_llama, \n",
    "    token=True,\n",
    "    padding=True,\n",
    "    pad_to_multiple_of=8,\n",
    "    truncation=True,\n",
    "    max_length=max_tok_length,\n",
    "    padding_side='left',\n",
    "    )\n",
    "tokenizer_llama.pad_token = tokenizer_llama.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5203176",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "\n",
    "def preprocess_function_opus(batch, tokenizer, model_name = \"nllb\"):\n",
    "    source_texts = [t[source_lang] for t in batch[\"translation\"]]\n",
    "    target_texts = [t[target_lang] for t in batch[\"translation\"]]\n",
    "    \n",
    "    args = {\n",
    "        \"truncation\": True,\n",
    "        \"max_length\": max_tok_length,\n",
    "    } if model_name == \"nllb\" else {}\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        source_texts,\n",
    "        text_target=target_texts,\n",
    "        **args\n",
    "    )\n",
    "    \n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bda8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets_nllb = opus_dataset.map(\n",
    "    lambda batch: preprocess_function_opus(batch, tokenizer_nllb, model_name=\"nllb\"),\n",
    "    batched=True, \n",
    "    num_proc=8\n",
    ")\n",
    "\n",
    "tokenized_datasets_nllb = tokenized_datasets_nllb.filter(\n",
    "    lambda x: len(x[\"input_ids\"]) <= max_tok_length and len(x[\"labels\"]) <= max_tok_length,\n",
    "    desc=f\"Discarding source and target sentences with more than {max_tok_length} tokens\", num_proc=8\n",
    ")\n",
    "\n",
    "tokenized_datasets_llama = opus_dataset.map(\n",
    "    lambda batch: preprocess_function_opus(batch, tokenizer_llama, model_name=\"llama\"),\n",
    "    batched=True, \n",
    "    num_proc=8\n",
    ")\n",
    "\n",
    "tokenized_datasets_llama = tokenized_datasets_llama.filter(\n",
    "    lambda x: len(x[\"input_ids\"]) <= max_tok_length and len(x[\"labels\"]) <= max_tok_length,\n",
    "    desc=f\"Discarding source and target sentences with more than {max_tok_length} tokens\", num_proc=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b0234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_length_distribution(tokenized_datasets):\n",
    "    dic = {}\n",
    "    for sample in tokenized_datasets['train']:\n",
    "        sample_length = len(sample['input_ids'])\n",
    "        if sample_length not in dic:\n",
    "            dic[sample_length] = 1\n",
    "        else:\n",
    "            dic[sample_length] += 1 \n",
    "\n",
    "    for i in range(1,max_tok_length+1):\n",
    "        if i in dic:\n",
    "            print(f\"{i:>2} {dic[i]:>3}\")\n",
    "            \n",
    "show_length_distribution(tokenized_datasets_nllb)\n",
    "show_length_distribution(tokenized_datasets_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09910807",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26cf08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nllb_baseline = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    checkpoint_nllb,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "model_nllb_finetuned = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    checkpoint_nllb,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "model_llama_promting = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint_llama,\n",
    "    token=True,\n",
    "    quantization_config=quantization_config,\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model_llama_finetuned = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint_llama,\n",
    "    token=True,\n",
    "    quantization_config=quantization_config,\n",
    "    dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e7b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nllb_finetuned = prepare_model_for_kbit_training(\n",
    "    model_nllb_finetuned,\n",
    "    use_gradient_checkpointing=False,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant':False}\n",
    ")\n",
    "\n",
    "model_llama_finetuned = prepare_model_for_kbit_training(\n",
    "    model_llama_finetuned,\n",
    "    use_gradient_checkpointing=False,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant':False}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f3581",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930ae8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LoraConfig_nllb = LoraConfig(\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c55e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LoraConfig_llama = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    inference_mode=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef97119",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nllb_finetuned = get_peft_model(model_nllb_finetuned, LoraConfig_nllb)\n",
    "print(model_nllb_finetuned.print_trainable_parameters())\n",
    "\n",
    "model_llama_finetuned = get_peft_model(model_llama_finetuned, LoraConfig_llama)\n",
    "print(model_llama_finetuned.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7733769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_nllb = DataCollatorForSeq2Seq(\n",
    "    tokenizer = tokenizer_nllb,\n",
    "    model = model_nllb_finetuned,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "data_collator_llama = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer_llama, \n",
    "    mlm=False, \n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afd5a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_llama = 4\n",
    "gradient_accumulation_steps = 8\n",
    "model_name_llama = checkpoint_llama.split(\"/\")[-1]\n",
    "args_llama = TrainingArguments(\n",
    "    f\"{model_name_llama}-finetuned-en-to-fr\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size_llama,\n",
    "    per_device_eval_batch_size=batch_size_llama,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=100,\n",
    "    optim=\"adamw_bnb_8bit\",\n",
    "    prediction_loss_only=True,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "    bf16=True,\n",
    "    bf16_full_eval=True,\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "batch_size_nllb = 32\n",
    "model_name_nllb = checkpoint_nllb.split(\"/\")[-1]\n",
    "args_nllb = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name_nllb}-finetuned-en-to-fr\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size_nllb,\n",
    "    per_device_eval_batch_size=batch_size_nllb,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    logging_strategy=\"epoch\",\n",
    "    disable_tqdm=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28de699",
   "metadata": {},
   "source": [
    "#### Llama preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651abcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "src = \"en\"\n",
    "tgt = \"fr\"\n",
    "task_prefix = f\"Translate from {src} to {tgt}:\\n\"\n",
    "s = \"\"\n",
    "\n",
    "prefix_tok_len = len(tokenizer_llama.encode(f\"{task_prefix}{src}: {s} = {tgt}: \"))\n",
    "max_tok_len_training = prefix_tok_len\n",
    "# Adding 2 for new line in target sentence and eos_token_id token\n",
    "max_tok_len_training += 2 * max_tok_length + 2\n",
    "\n",
    "def preprocess4training_function(sample, max_tok_len):\n",
    "    \n",
    "    sample_size = len(sample[\"translation\"])\n",
    "\n",
    "    # Creating the prompt with the task description for each source sentence\n",
    "    inputs  = [f\"{task_prefix}{src}: {s[src]} = {tgt}: \" for s in sample[\"translation\"]]\n",
    "\n",
    "    # Appending new line after each sample in the batch\n",
    "    targets = [f\"{s[tgt]}\\n\" for s in sample[\"translation\"]]\n",
    "\n",
    "    # Applying the Llama2 tokenizer to the inputs and targets \n",
    "    # to obtain \"input_ids\" (token_ids) and \"attention mask\" \n",
    "    model_inputs = tokenizer_llama(inputs)\n",
    "    labels = tokenizer_llama(targets)\n",
    "    \n",
    "    # Each input is appended with its target \n",
    "    # Each target is prepended with as many special token id (-100) as the original input length\n",
    "    # Both input and target (label) has the same max_tok_len\n",
    "    # Attention mask is all 1s \n",
    "    for i in range(sample_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer_llama.eos_token_id]\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "\n",
    "    # Each input is applied left padding up to max_tok_len\n",
    "    # Attention mask is 0 for padding\n",
    "    # Each target (label) is left filled with special token id (-100)\n",
    "    # Finally inputs, attention_mask and targets (labels) are truncated to max_tok_len\n",
    "    for i in range(sample_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer_llama.pad_token_id] * (\n",
    "            max_tok_len - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_tok_len - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        labels[\"input_ids\"][i] = [-100] * (max_tok_len - len(sample_input_ids)) + label_input_ids\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_tok_len])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_tok_len])\n",
    "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_tok_len])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c75ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shots = 1\n",
    "shots = \"\"\n",
    "\n",
    "shot_tok_len   = len(tokenizer_llama.encode(f\"{src}: {s} = {tgt}: {s}\\n\"))\n",
    "max_tok_len_test = prefix_tok_len\n",
    "max_tok_len_test += num_shots * (shot_tok_len + 2 * max_tok_length) \n",
    "max_tok_len_test += max_tok_length\n",
    "\n",
    "def preprocess4test_function(sample):\n",
    "    inputs = [f\"{task_prefix}{shots}{src}: {s} = {tgt}: \" for s in sample[\"source_text\"]]\n",
    "    model_inputs = tokenizer_llama(\n",
    "        inputs,\n",
    "        max_length=max_tok_len_test, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82f2181",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train_dataset = tokenized_datasets_llama['train'].map(preprocess4training_function, batched=True)\n",
    "preprocessed_dev_dataset = tokenized_datasets_llama['validation'].map(preprocess4training_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d430c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_test_dataset = tokenized_datasets_llama['test'].map(preprocess4test_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6747c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    \n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer):\n",
    "    training = True if len(eval_preds) == 2 else False\n",
    "    if training:\n",
    "        preds, labels = eval_preds\n",
    "    else:\n",
    "        inputs, preds, labels = eval_preds\n",
    "\n",
    "    if not training and not isinstance(inputs, list):\n",
    "        inputs = list(inputs)\n",
    "    if not isinstance(labels, list):\n",
    "        labels = list(labels)\n",
    "        \n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace negative ids in labels as we can't decode them.\n",
    "    if not training:    \n",
    "        inputs = [\n",
    "            [tokenizer.pad_token_id if j < 0 else j for j in input]\n",
    "            for input in inputs\n",
    "        ]\n",
    "        decoded_inputs = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n",
    "\n",
    "    labels = [\n",
    "        [tokenizer.pad_token_id if j < 0 else j for j in label]\n",
    "        for label in labels\n",
    "    ]\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=[decoded_labels[i] for i in range(len(decoded_labels))])\n",
    "    result = {\"bleu\": bleu_result[\"score\"]}\n",
    "    \n",
    "    if not training:\n",
    "        comet_result = comet_metric.compute(sources=decoded_inputs, predictions=decoded_preds, references=decoded_labels)\n",
    "        result[\"comet\"] = comet_result[\"mean_score\"] * 100\n",
    "\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba5c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_llama = Trainer(\n",
    "    model_llama_finetuned,\n",
    "    args_llama,\n",
    "    train_dataset=preprocessed_train_dataset,\n",
    "    eval_dataset=preprocessed_dev_dataset,\n",
    "    processing_class=tokenizer_llama,\n",
    "    data_collator=data_collator_llama,\n",
    ")\n",
    "\n",
    "trainer_nllb = Seq2SeqTrainer(\n",
    "    model_nllb_finetuned,\n",
    "    args_nllb,\n",
    "    train_dataset=tokenized_datasets_nllb['train'],\n",
    "    eval_dataset=tokenized_datasets_nllb['validation'],\n",
    "    processing_class=tokenizer_nllb,\n",
    "    data_collator=data_collator_nllb,\n",
    "    compute_metrics = lambda eval_preds: compute_metrics(eval_preds, tokenizer_nllb)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486261b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" ---- NLLB Training ----\")\n",
    "trainer_nllb.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6682270",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" ---- LLaMA Training ----\")\n",
    "trainer_llama.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518d3ba",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c4828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config_nllb = GenerationConfig.from_pretrained(\n",
    "    checkpoint_nllb,\n",
    ")\n",
    "    \n",
    "generation_config_llama = GenerationConfig.from_pretrained(\n",
    "    checkpoint_llama,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0eb96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tokenized_test_nllb = tokenized_datasets_nllb['test'].batch(batch_size_nllb)\n",
    "batch_tokenized_test_llama = preprocessed_test_dataset.batch(batch_size_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb74220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, batch_tokenized_test, tokenizer, generation_config):\n",
    "    number_of_batches = len(batch_tokenized_test[\"translation\"])\n",
    "    input_sequences = []\n",
    "    preds_sequences = []\n",
    "    labels_sequences = []\n",
    "    for i in range(number_of_batches):\n",
    "        batch_tokenized_test_src = list(batch_tokenized_test[\"translation\"][i][j][source_lang] for j in range(len(batch_tokenized_test[\"translation\"][i])))\n",
    "        batch_tokenized_test_tgt = list(batch_tokenized_test[\"translation\"][i][j][target_lang] for j in range(len(batch_tokenized_test[\"translation\"][i])))\n",
    "        inputs = tokenizer(\n",
    "            batch_tokenized_test_src, \n",
    "            max_length=max_tok_length, \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True,\n",
    "            )\n",
    "        labels = tokenizer(\n",
    "            batch_tokenized_test_tgt, \n",
    "            max_length=max_tok_length, \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True,\n",
    "        )\n",
    "        with torch.no_grad():    \n",
    "            output_batch = model.generate(\n",
    "                generation_config=generation_config, \n",
    "                input_ids=inputs[\"input_ids\"].cuda(), \n",
    "                attention_mask=inputs[\"attention_mask\"].cuda(), \n",
    "                forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_code), \n",
    "                max_length = max_tok_length, \n",
    "                num_beams=1, \n",
    "                do_sample=False,\n",
    "                )\n",
    "        input_sequences.extend(inputs[\"input_ids\"].cpu())\n",
    "        preds_sequences.extend(output_batch.cpu())\n",
    "        labels_sequences.extend(labels[\"input_ids\"].cpu())\n",
    "    return input_sequences, preds_sequences, labels_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f9637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model,\n",
    "    batch_tokenized_test,\n",
    "    tokenizer,\n",
    "    generation_config,\n",
    "    source_lang,\n",
    "    target_lang,\n",
    "    max_tok_length,\n",
    "    tgt_code=None,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    number_of_batches = len(batch_tokenized_test[\"translation\"])\n",
    "    input_sequences = []\n",
    "    preds_sequences = []\n",
    "    labels_sequences = []\n",
    "\n",
    "    for i in range(number_of_batches):\n",
    "        batch_src = [\n",
    "            batch_tokenized_test[\"translation\"][i][j][source_lang]\n",
    "            for j in range(len(batch_tokenized_test[\"translation\"][i]))\n",
    "        ]\n",
    "        batch_tgt = [\n",
    "            batch_tokenized_test[\"translation\"][i][j][target_lang]\n",
    "            for j in range(len(batch_tokenized_test[\"translation\"][i]))\n",
    "        ]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch_src,\n",
    "            max_length=max_tok_length,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels = tokenizer(\n",
    "            batch_tgt,\n",
    "            max_length=max_tok_length,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        gen_kwargs = dict(\n",
    "            generation_config=generation_config,\n",
    "            input_ids=inputs[\"input_ids\"].to(device),\n",
    "            attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "            max_length=max_tok_length,\n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "        )\n",
    "\n",
    "        if tgt_code is not None:\n",
    "            gen_kwargs[\"forced_bos_token_id\"] = tokenizer.convert_tokens_to_ids(tgt_code)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**gen_kwargs)\n",
    "\n",
    "        input_sequences.extend(inputs[\"input_ids\"].cpu())\n",
    "        preds_sequences.extend(outputs.cpu())\n",
    "        labels_sequences.extend(labels[\"input_ids\"].cpu())\n",
    "\n",
    "    return input_sequences, preds_sequences, labels_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c5311e",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b081da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" ---- NLLB Results ----\")\n",
    "\n",
    "input_sequences, preds_sequences, labels_sequences = evaluate_model(model_nllb_baseline, batch_tokenized_test_nllb, tokenizer_nllb, generation_config_nllb, source_lang, target_lang, max_tok_length, tgt_code)\n",
    "print(\"Baseline:\")\n",
    "nllb_baseline_result = compute_metrics((input_sequences, preds_sequences, labels_sequences), tokenizer_nllb)\n",
    "print(f'BLEU: {nllb_baseline_result[\"bleu\"]}')\n",
    "print(f'COMET: {nllb_baseline_result[\"comet\"]}')\n",
    "\n",
    "input_sequences, preds_sequences, labels_sequences = evaluate_model(model_nllb_finetuned, batch_tokenized_test_nllb, tokenizer_nllb, generation_config_nllb, source_lang, target_lang, max_tok_length, tgt_code)\n",
    "print(\"Finetuned:\")\n",
    "nllb_finetuned_result = compute_metrics((input_sequences, preds_sequences, labels_sequences), tokenizer_nllb)\n",
    "print(f'BLEU: {nllb_finetuned_result[\"bleu\"]}')\n",
    "print(f'COMET: {nllb_finetuned_result[\"comet\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72d0946",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" ---- LLaMA Results ----\")\n",
    "\n",
    "input_sequences, preds_sequences, labels_sequences = evaluate_model(model_llama_promting, batch_tokenized_test_llama, tokenizer_llama, generation_config_llama, source_lang, target_lang, max_tok_len_test)\n",
    "print(\"Prometing:\")\n",
    "llama_promting_result = compute_metrics((input_sequences, preds_sequences, labels_sequences), tokenizer_llama)\n",
    "print(f'BLEU: {llama_promting_result[\"bleu\"]}')\n",
    "print(f'COMET: {llama_promting_result[\"comet\"]}')\n",
    "\n",
    "input_sequences, preds_sequences, labels_sequences = evaluate_model(model_llama_finetuned, batch_tokenized_test_llama, tokenizer_llama, generation_config_llama, source_lang, target_lang, max_tok_len_test)\n",
    "print(\"Finetuned:\")\n",
    "llama_finetuned_result = compute_metrics((input_sequences, preds_sequences, labels_sequences), tokenizer_llama)\n",
    "print(f'BLEU: {llama_finetuned_result[\"bleu\"]}')\n",
    "print(f'COMET: {llama_finetuned_result[\"comet\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
