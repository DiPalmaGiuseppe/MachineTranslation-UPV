{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m model_base \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m model_medium \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedium\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m model_large \u001b[38;5;241m=\u001b[39m \u001b[43mwhisper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlarge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhisper model device:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mnext\u001b[39m(model_base\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     19\u001b[0m bleu_metric \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msacrebleu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/envs/ta-project/lib/python3.10/site-packages/whisper/__init__.py:156\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, device, download_root, in_memory)\u001b[0m\n\u001b[1;32m    154\u001b[0m dims \u001b[38;5;241m=\u001b[39m ModelDimensions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheckpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdims\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    155\u001b[0m model \u001b[38;5;241m=\u001b[39m Whisper(dims)\n\u001b[0;32m--> 156\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m alignment_heads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     model\u001b[38;5;241m.\u001b[39mset_alignment_heads(alignment_heads)\n",
      "File \u001b[0;32m~/envs/ta-project/lib/python3.10/site-packages/torch/nn/modules/module.py:2609\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2602\u001b[0m         out \u001b[38;5;241m=\u001b[39m hook(module, incompatible_keys)\n\u001b[1;32m   2603\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m   2604\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHooks registered with ``register_load_state_dict_post_hook`` are not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2605\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected to return new values, if incompatible_keys need to be modified,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2606\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit should be done inplace.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2607\u001b[0m         )\n\u001b[0;32m-> 2609\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2610\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m load\n\u001b[1;32m   2612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
      "File \u001b[0;32m~/envs/ta-project/lib/python3.10/site-packages/torch/nn/modules/module.py:2597\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2591\u001b[0m         child_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2592\u001b[0m         child_state_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2593\u001b[0m             k: v\n\u001b[1;32m   2594\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   2595\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(child_prefix)\n\u001b[1;32m   2596\u001b[0m         }\n\u001b[0;32m-> 2597\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[1;32m   2599\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[1;32m   2600\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001b[0;32m~/envs/ta-project/lib/python3.10/site-packages/torch/nn/modules/module.py:2597\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2591\u001b[0m         child_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2592\u001b[0m         child_state_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2593\u001b[0m             k: v\n\u001b[1;32m   2594\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   2595\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(child_prefix)\n\u001b[1;32m   2596\u001b[0m         }\n\u001b[0;32m-> 2597\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[1;32m   2599\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[1;32m   2600\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "    \u001b[0;31m[... skipping similar frames: Module.load_state_dict.<locals>.load at line 2597 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/envs/ta-project/lib/python3.10/site-packages/torch/nn/modules/module.py:2597\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2591\u001b[0m         child_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2592\u001b[0m         child_state_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2593\u001b[0m             k: v\n\u001b[1;32m   2594\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   2595\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(child_prefix)\n\u001b[1;32m   2596\u001b[0m         }\n\u001b[0;32m-> 2597\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[1;32m   2599\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[1;32m   2600\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001b[0;32m~/envs/ta-project/lib/python3.10/site-packages/torch/nn/modules/module.py:2580\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m assign:\n\u001b[1;32m   2579\u001b[0m     local_metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massign_to_params_buffers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m assign\n\u001b[0;32m-> 2580\u001b[0m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2584\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2586\u001b[0m \u001b[43m    \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2587\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_msgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2588\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/envs/ta-project/lib/python3.10/site-packages/torch/nn/modules/module.py:2487\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   2485\u001b[0m             \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[1;32m   2486\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2487\u001b[0m             \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2488\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   2489\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswapping\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_swap_tensors \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopying\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import whisper\n",
    "import pandas as pd\n",
    "import time\n",
    "from evaluate import load\n",
    "from opencc import OpenCC\n",
    "from whisper.normalizers.basic import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "cc = OpenCC('t2s')  # traditional -> simplified Chinese\n",
    "\n",
    "model_base = whisper.load_model(\"base\")\n",
    "model_medium = whisper.load_model(\"medium\")\n",
    "model_large = whisper.load_model(\"large\")\n",
    "\n",
    "print(\"Whisper model device:\", next(model_base.parameters()).device)\n",
    "\n",
    "bleu_metric = load(\"sacrebleu\")\n",
    "comet_metric = load(\"comet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['client_id', 'file', 'audio', 'sentence', 'translation', 'id'],\n",
      "        num_rows: 4843\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'file', 'audio', 'sentence', 'translation', 'id'],\n",
      "        num_rows: 4898\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'file', 'audio', 'sentence', 'translation', 'id'],\n",
      "        num_rows: 7085\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"fixie-ai/covost2\", \"zh-CN_en\")\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_to_evaluate = raw_datasets[\"validation\"].select(range(10))\n",
    "data_to_evaluate = raw_datasets[\"validation\"]\n",
    "\n",
    "def evaluate_model(model, model_name, dataset):\n",
    "    \n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "    \n",
    "    sample = {\n",
    "        \"sentence\": [],\n",
    "        \"hypothesis\": [],\n",
    "        \"translation\": [],\n",
    "        \"training_time\": 0\n",
    "    }\n",
    "\n",
    "    t_start = time.time()\n",
    "    for s in dataset:\n",
    "        audio_array = s[\"audio\"][\"array\"].astype(\"float32\")\n",
    "        ref = s[\"sentence\"]\n",
    "        \n",
    "        # Translation\n",
    "        result = model.transcribe(audio_array, language=\"zh\", task=\"translate\")\n",
    "        hyp = result['text']\n",
    "        \n",
    "        sample[\"sentence\"].append(ref.strip())\n",
    "        sample[\"hypothesis\"].append(hyp.strip())\n",
    "        sample[\"translation\"].append(s[\"translation\"].strip())\n",
    "        \n",
    "    sample[\"training_time\"] = time.time() - t_start\n",
    "        \n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_base = evaluate_model(model_base, \"base\", data_to_evaluate)\n",
    "sample_medium = evaluate_model(model_medium, \"medium\", data_to_evaluate)\n",
    "sample_large = evaluate_model(model_large, \"large\", data_to_evaluate)\n",
    "samples = [(\"base\", sample_base), (\"medium\", sample_medium), (\"large\", sample_large)]\n",
    "\n",
    "for model_name, sample in samples:\n",
    "    print(f\"\\n--- Analysis for Whisper {model_name} ---\")\n",
    "    \n",
    "    print(f\"Training time: {sample['training_time']:.2f} seconds\")\n",
    "    \n",
    "    sample[\"clean_sentence\"] = [normalizer(text).strip() for text in sample[\"sentence\"]]\n",
    "    sample[\"clean_hypothesis\"] = [normalizer(text).strip() for text in sample[\"hypothesis\"]]\n",
    "    sample[\"clean_translation\"] = [normalizer(text).strip() for text in sample[\"translation\"]]\n",
    "\n",
    "    # BLEU and COMET\n",
    "    bleu = bleu_metric.compute(\n",
    "        predictions=sample[\"hypothesis\"],\n",
    "        references=sample[\"translation\"]\n",
    "    )\n",
    "    comet = comet_metric.compute(\n",
    "        predictions=sample[\"hypothesis\"],\n",
    "        references=sample[\"translation\"],\n",
    "        sources=sample[\"sentence\"]\n",
    "    )\n",
    "\n",
    "    # BLEU and COMET on cleaned text\n",
    "    bleu_clean = bleu_metric.compute(\n",
    "        predictions=sample[\"clean_hypothesis\"],\n",
    "        references=sample[\"clean_translation\"]\n",
    "    )\n",
    "    comet_clean = comet_metric.compute(\n",
    "        predictions=sample[\"clean_hypothesis\"],\n",
    "        references=sample[\"clean_translation\"],\n",
    "        sources=sample[\"clean_sentence\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"BLEU: {bleu['score']:.2f}\")\n",
    "    print(f\"COMET: {comet['mean_score'] * 100:.2f}\")\n",
    "    print(f\"BLEU (cleaned): {bleu_clean['score']:.2f}\")\n",
    "    print(f\"COMET (cleaned): {comet_clean['mean_score'] * 100:.2f}\")  \n",
    "    \n",
    "    column_order = [\n",
    "        \"sentence\",\n",
    "        \"hypothesis\",\n",
    "        \"translation\",\n",
    "        \"clean_sentence\",\n",
    "        \"clean_hypothesis\",\n",
    "        \"clean_translation\",\n",
    "        \"training_time\"\n",
    "    ]\n",
    "    \n",
    "    dataframe = pd.DataFrame(sample)[column_order]\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    if os.path.exists('ST_csvs') == False:\n",
    "        os.mkdir('ST_csvs')\n",
    "    dataframe.to_csv(f'ST_csvs/ST_{model_name}.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
