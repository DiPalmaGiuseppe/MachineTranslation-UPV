{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778061aff7d24b7da8eab6372a9e054e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/home/alumno.upv.es/gdipal1/envs/ta-project/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "import gc\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from whisper.normalizers.basic import BasicTextNormalizer\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainingArguments \n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import IA3Config\n",
    "from evaluate import load\n",
    "\n",
    "\n",
    "login(token=\"\")\n",
    "\n",
    "bleu_metric = load(\"sacrebleu\")\n",
    "comet_metric = load(\"comet\")\n",
    "\n",
    "DEBUG_MODE = False\n",
    "DEBUG_FRACTION = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'hypothesis_simple', 'hypothesis_traditional', 'translation', 'clean_sentence', 'clean_hypothesis_simple', 'clean_hypothesis_traditional', 'clean_translation', 'training_time'],\n",
      "        num_rows: 33\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'hypothesis_simple', 'hypothesis_traditional', 'translation', 'clean_sentence', 'clean_hypothesis_simple', 'clean_hypothesis_traditional', 'clean_translation', 'training_time'],\n",
      "        num_rows: 7\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'hypothesis_simple', 'hypothesis_traditional', 'translation', 'clean_sentence', 'clean_hypothesis_simple', 'clean_hypothesis_traditional', 'clean_translation', 'training_time'],\n",
      "        num_rows: 8\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"csv\", data_files = \"ASR_csvs/ASR_large.csv\")\n",
    "def is_complete(example):\n",
    "    for v in example.values():\n",
    "\n",
    "        if v is None:\n",
    "            return False\n",
    "\n",
    "        if isinstance(v, float) and math.isnan(v):\n",
    "            return False\n",
    "\n",
    "        if isinstance(v, str) and v.strip() == \"\":\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "raw_datasets = raw_datasets.filter(is_complete)\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    raw_datasets = DatasetDict({\n",
    "        split: raw_datasets[split]\n",
    "        .shuffle(seed=42)\n",
    "        .select(range(int(len(raw_datasets[split]) * DEBUG_FRACTION)))\n",
    "        for split in raw_datasets.keys()\n",
    "    })\n",
    "\n",
    "raw_datasets = raw_datasets[\"train\"].train_test_split(test_size=0.3, shuffle=False)\n",
    "\n",
    "test_valid_split = raw_datasets[\"test\"].train_test_split(test_size=0.5, shuffle=False)\n",
    "\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": raw_datasets[\"train\"],\n",
    "    \"validation\": test_valid_split[\"train\"],  # metà del test diventa validation\n",
    "    \"test\": test_valid_split[\"test\"]          # metà del test rimane test\n",
    "})\n",
    "\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tok_length = 275\n",
    "checkpoint_nllb = \"facebook/nllb-200-distilled-600M\"\n",
    "checkpoint_mbart = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "\n",
    "src_code = \"zho_Hans\"\n",
    "tgt_code = \"eng_Latn\"\n",
    "\n",
    "src_code_mbart_trad = \"zh_CN\"\n",
    "src_code_mbart_simple = \"zh_CN\"\n",
    "tgt_code_mbart = \"en_XX\"\n",
    "\n",
    "tokenizer_nllb = AutoTokenizer.from_pretrained(\n",
    "    checkpoint_nllb, \n",
    "    padding=True, \n",
    "    pad_to_multiple_of=8, \n",
    "    src_lang=src_code, \n",
    "    tgt_lang=tgt_code, \n",
    "    truncation=False, \n",
    "    max_length=max_tok_length,\n",
    ")\n",
    "\n",
    "tokenizer_mbart_simple = AutoTokenizer.from_pretrained(\n",
    "    checkpoint_mbart,\n",
    "    padding=True,\n",
    "    pad_to_multiple_of=8,\n",
    "    src_lang=src_code_mbart_simple,\n",
    "    tgt_lang=tgt_code_mbart,\n",
    "    truncation=True,\n",
    "    max_length=max_tok_length,\n",
    ")\n",
    "\n",
    "tokenizer_mbart_trad = AutoTokenizer.from_pretrained(\n",
    "    checkpoint_mbart,\n",
    "    padding=True,\n",
    "    pad_to_multiple_of=8,\n",
    "    src_lang=src_code_mbart_trad,\n",
    "    tgt_lang=tgt_code_mbart,\n",
    "    truncation=True,\n",
    "    max_length=max_tok_length,\n",
    ")\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(sample, tokenizer, src, tgt):\n",
    "    model_inputs = tokenizer(\n",
    "        sample[src],\n",
    "        text_target=sample[tgt],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_tok_length\n",
    "    )\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5fdb385852461bb86afcbd4bc87cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92b21f51e8f4ef19a21bd37ee36bbeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c532fb212a8b44eba62db3c2b4920ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34761c21f01c4d9c9d2b5553b4fe77d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a934f4b759e0404080b4f30e63dcc7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9787683ed9de435da90a16e105818051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc17cd40dfe4d4c8ecd6c986a629630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2588032c1a8a43a2897d597d52a0a4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab896b2381dc4eca8559354c0326359d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58ad9495ef145048d1e704690e9556c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e290ef07570445c78e0cd8710e0bbaca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a85fb25c5646f690b796a34fde5084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_mbart_simple = raw_datasets.map(\n",
    "    lambda x: preprocess_function(x, tokenizer_mbart_simple, src=\"hypothesis_simple\", tgt=\"translation\"),\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "\n",
    "dataset_mbart_trad = raw_datasets.map(\n",
    "    lambda x: preprocess_function(x, tokenizer_mbart_trad, src=\"hypothesis_traditional\", tgt=\"translation\"),\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83463e36eba8465ba38da13acddfd5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72c50bee7e3442886e07e81c177f0bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9502ee1a01064765b787c7854b708164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf48e8a113a14243aa8c0d92e27327bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d498c9c9f53d492ea951572c6ff903ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223ca688bf4b439091dcd998457d18b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c1ca56b1e94fa2bb1a3027e345f704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daff40d7f0f3432988ee5eb359de5b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99799f37beb64f5ca13e213e7258278c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfc96fe434d4d538b5be560d1f55bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6084a6786e3492d88daf3f3f4a151f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb1c88dad6f46988fa7764d0919cb25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_nllb_simple_clean = raw_datasets.map(\n",
    "    lambda x: preprocess_function(x, tokenizer_nllb, src=\"clean_hypothesis_simple\", tgt=\"clean_translation\"),\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "dataset_nllb_trad_clean = raw_datasets.map(\n",
    "    lambda x: preprocess_function(x, tokenizer_nllb, src=\"clean_hypothesis_traditional\", tgt=\"clean_translation\"),\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    checkpoint,\n",
    "    quantization_config,\n",
    "    config = None,\n",
    "):\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        checkpoint,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    generation_config = GenerationConfig.from_pretrained(\n",
    "        checkpoint,\n",
    "    )\n",
    "    \n",
    "    model = prepare_model_for_kbit_training(\n",
    "        model,\n",
    "        use_gradient_checkpointing=False,\n",
    "        gradient_checkpointing_kwargs={'use_reentrant': False}\n",
    "    )\n",
    "    \n",
    "    if config == \"lora\":\n",
    "        train_config = LoraConfig(\n",
    "            task_type=\"SEQ_2_SEQ_LM\",\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "        )\n",
    "    elif config == \"ia3\":\n",
    "        train_config = IA3Config(\n",
    "            task_type=\"SEQ_2_SEQ_LM\",\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n",
    "            feedforward_modules=[\"fc1\", \"fc2\"]\n",
    "        )\n",
    "    \n",
    "    model = get_peft_model(model, train_config)\n",
    "    model.print_trainable_parameters()        \n",
    "    \n",
    "    return model, generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(sample, training = False):\n",
    "    if training:\n",
    "        inputs, labels = sample\n",
    "    else:\n",
    "        inputs, preds, labels = sample\n",
    "        \n",
    "    inputs = [normalizer(text) for text in inputs]\n",
    "    preds = [normalizer(text) for text in preds]\n",
    "    labels = [normalizer(text) for text in labels]\n",
    "    \n",
    "    bleu_result = bleu_metric.compute(predictions=preds, references=[labels[i] for i in range(len(labels))])[\"score\"]\n",
    "    results = {\n",
    "        \"bleu\": bleu_result,\n",
    "    }\n",
    "    \n",
    "    if not training:\n",
    "        results[\"comet\"] = comet_metric.compute(predictions=preds, references=labels, sources=inputs)[\"mean_score\"] * 100\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 4\n",
    "\n",
    "def get_trainer(\n",
    "    model_name,\n",
    "    model,\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    "    tokenizer,\n",
    "    data_collator,\n",
    "    compute_metrics\n",
    "):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        f\"{model_name}-finetuned-ch-to-en\",\n",
    "        eval_use_gather_object=\"epoch\",\n",
    "        learning_rate=1e-4,\n",
    "        per_device_train_batch_size=test_batch_size,\n",
    "        per_device_eval_batch_size=test_batch_size,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=2,\n",
    "        num_train_epochs=5,\n",
    "        predict_with_generate=True,\n",
    "        disable_tqdm=True\n",
    "    )\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=lambda x: compute_metrics(x, train=True)\n",
    "    )\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(\n",
    "    model_name,\n",
    "    model, \n",
    "    generation_config, \n",
    "    dataset, \n",
    "    tokenizer,\n",
    "    num_beams,\n",
    "):\n",
    "    model.eval()\n",
    "    \n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        padding='longest',\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "        dataset[\"test\"],\n",
    "        batch_size=32,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "    \n",
    "    trainer = get_trainer(\n",
    "        model_name=model_name,\n",
    "        model=model,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=evaluate_metrics\n",
    "    )\n",
    "    \n",
    "    print(\" ---- Training Phase ---- \")\n",
    "    trainer.train()\n",
    "    \n",
    "    \n",
    "    print(\" ---- Evaluation Phase ---- \")\n",
    "    input_sequences = []\n",
    "    pred_sequences = []\n",
    "    label_sequences = []\n",
    "    \n",
    "    for batch in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            output_batch = model.generate(\n",
    "                generation_config=generation_config, \n",
    "                input_ids=batch[\"input_ids\"].cuda(), \n",
    "                attention_mask=batch[\"attention_mask\"].cuda(), \n",
    "                forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_code), \n",
    "                max_length=max_tok_length, \n",
    "                num_beams=num_beams, \n",
    "                do_sample=False,\n",
    "            )\n",
    "            \n",
    "        input_sequences.extend(batch[\"input_ids\"].cpu().numpy())\n",
    "        pred_sequences.extend(output_batch.cpu().numpy())\n",
    "        label_sequences.extend(batch[\"labels\"].cpu().numpy())\n",
    "    \n",
    "    dedoced_inputs = tokenizer.batch_decode(input_sequences, skip_special_tokens=True)\n",
    "    decoded_preds = tokenizer.batch_decode(pred_sequences, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(label_sequences, skip_special_tokens=True)\n",
    "    \n",
    "    del trainer\n",
    "    del data_collator\n",
    "    del test_dataloader\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return dedoced_inputs, decoded_preds, decoded_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: nllb_simple with num_beams=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3832628/1139247043.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You can't train a model that has been loaded in 8-bit or 4-bit precision on a different device than the one you're training on. Make sure you loaded the model on the correct device using for example `device_map={'':torch.cuda.current_device()}` or `device_map={'':torch.xpu.current_device()}`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m num_beams \u001b[38;5;241m=\u001b[39m cfg_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with num_beams=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_beams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m inputs, preds, labels \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_item\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_item\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgeneration_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_item\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_item\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_beams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m metrics \u001b[38;5;241m=\u001b[39m evaluate_metrics((inputs, preds, labels))\n\u001b[1;32m     45\u001b[0m results_list\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: name,\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m: num_beams,\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbleu\u001b[39m\u001b[38;5;124m\"\u001b[39m: metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbleu\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomet\u001b[39m\u001b[38;5;124m\"\u001b[39m: metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomet\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     50\u001b[0m })\n",
      "Cell \u001b[0;32mIn[23], line 34\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model_name, model, generation_config, dataset, tokenizer, num_beams)\u001b[0m\n\u001b[1;32m     18\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     19\u001b[0m     dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     20\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mtest_batch_size,\n\u001b[1;32m     21\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m trainer \u001b[38;5;241m=\u001b[39m get_trainer(\n\u001b[1;32m     25\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[1;32m     26\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mevaluate_metrics\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 34\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m input_sequences \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     37\u001b[0m pred_sequences \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/envs/ta-project/lib/python3.10/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/ta-project/lib/python3.10/site-packages/transformers/trainer.py:2480\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2478\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mprepare(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer)\n\u001b[1;32m   2479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2480\u001b[0m             model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2481\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2482\u001b[0m     \u001b[38;5;66;03m# to handle cases wherein we pass \"DummyScheduler\" such as when it is specified in DeepSpeed config.\u001b[39;00m\n\u001b[1;32m   2483\u001b[0m     model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[1;32m   2484\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\n\u001b[1;32m   2485\u001b[0m     )\n",
      "File \u001b[0;32m~/envs/ta-project/lib/python3.10/site-packages/accelerate/accelerator.py:1559\u001b[0m, in \u001b[0;36mAccelerator.prepare\u001b[0;34m(self, device_placement, *args)\u001b[0m\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp8_backend \u001b[38;5;241m==\u001b[39m FP8BackendType\u001b[38;5;241m.\u001b[39mMSAMP:\n\u001b[1;32m   1558\u001b[0m         args, device_placement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_msamp(\u001b[38;5;241m*\u001b[39margs, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n\u001b[0;32m-> 1559\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1562\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_one(obj, device_placement\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpu_should_fix_optimizer:\n\u001b[1;32m   1564\u001b[0m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/ta-project/lib/python3.10/site-packages/accelerate/accelerator.py:1560\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp8_backend \u001b[38;5;241m==\u001b[39m FP8BackendType\u001b[38;5;241m.\u001b[39mMSAMP:\n\u001b[1;32m   1558\u001b[0m         args, device_placement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_msamp(\u001b[38;5;241m*\u001b[39margs, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n\u001b[1;32m   1559\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m-> 1560\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, device_placement)\n\u001b[1;32m   1561\u001b[0m     )\n\u001b[1;32m   1562\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_one(obj, device_placement\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpu_should_fix_optimizer:\n\u001b[1;32m   1564\u001b[0m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/ta-project/lib/python3.10/site-packages/accelerate/accelerator.py:1402\u001b[0m, in \u001b[0;36mAccelerator._prepare_one\u001b[0;34m(self, obj, first_pass, device_placement)\u001b[0m\n\u001b[1;32m   1400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_data_loader(obj, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m-> 1402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer):\n\u001b[1;32m   1404\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_optimizer(obj, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n",
      "File \u001b[0;32m~/envs/ta-project/lib/python3.10/site-packages/accelerate/accelerator.py:1816\u001b[0m, in \u001b[0;36mAccelerator.prepare_model\u001b[0;34m(self, model, device_placement, evaluation_mode)\u001b[0m\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(current_device_index) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[1;32m   1814\u001b[0m         \u001b[38;5;66;03m# if on the first device (GPU 0) we don't care\u001b[39;00m\n\u001b[1;32m   1815\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (current_device_index \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m-> 1816\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1817\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt train a model that has been loaded in 8-bit or 4-bit precision on a different device than the one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1818\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre training on. Make sure you loaded the model on the correct device using for example `device_map=\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:torch.cuda.current_device()}` or `device_map=\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:torch.xpu.current_device()}`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1819\u001b[0m             )\n\u001b[1;32m   1820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1821\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_devices \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_multi_backend_available())\n\u001b[1;32m   1822\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_devices \u001b[38;5;129;01mand\u001b[39;00m is_xpu_available())\n\u001b[1;32m   1823\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_devices\n\u001b[1;32m   1824\u001b[0m ):\n\u001b[1;32m   1825\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1826\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt train a model that has been loaded in 8-bit or 4-bit precision with CPU or disk offload. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1827\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you want train the 8-bit or 4-bit model in CPU, please install bitsandbytes with multi-backend, see https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1828\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: You can't train a model that has been loaded in 8-bit or 4-bit precision on a different device than the one you're training on. Make sure you loaded the model on the correct device using for example `device_map={'':torch.cuda.current_device()}` or `device_map={'':torch.xpu.current_device()}`"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creiamo lista di configurazioni con più beams\n",
    "cfg_list = []\n",
    "\n",
    "for model_name, base_cfg in {\n",
    "    # \"nllb_simple\": (model_nllb, generation_config_nllb, dataset_nllb_simple, tokenizer_nllb),\n",
    "    # \"nllb_trad\": (model_nllb, generation_config_nllb, dataset_nllb_trad, tokenizer_nllb),\n",
    "    \"mbart_simple_lora\": (checkpoint_mbart, \"lora\", dataset_mbart_simple, tokenizer_mbart_simple,1),\n",
    "    \"mbart_simple_ia3\": (checkpoint_mbart, \"ia3\", dataset_mbart_simple, tokenizer_mbart_simple,1),\n",
    "    \"mbart_trad_lora\": (checkpoint_mbart, \"lora\", dataset_mbart_trad, tokenizer_mbart_trad,1),\n",
    "    \"mbart_trad_ia3\": (checkpoint_mbart, \"ia3\", dataset_mbart_trad, tokenizer_mbart_trad,1),\n",
    "    \"nllb_simple_clean_beam1\": (checkpoint_nllb, \"lora\", dataset_nllb_simple_clean, tokenizer_nllb, 1),\n",
    "    \"nllb_simple_clean_beam4\": (checkpoint_nllb, \"lora\", dataset_nllb_simple_clean, tokenizer_nllb, 4),\n",
    "    \"nllb_trad_clean_beam1\": (checkpoint_nllb, \"lora\", dataset_nllb_trad_clean, tokenizer_nllb, 1),\n",
    "    \"nllb_trad_clean_beam4\": (checkpoint_nllb, \"lora\", dataset_nllb_trad_clean, tokenizer_nllb, 4),\n",
    "    # \"mbart_simple_clean\": (model_mbart, generation_config_mbart, dataset_mbart_simple_clean, tokenizer_mbart_simple),\n",
    "    # \"mbart_trad_clean\": (model_mbart, generation_config_mbart, dataset_mbart_trad_clean, tokenizer_mbart_trad),\n",
    "}.items():\n",
    "    checkpoint, config, dataset, tokenizer, num_beams = base_cfg\n",
    "    cfg_list.append({\n",
    "        \"model_name\": model_name,\n",
    "        \"checkpoint\": checkpoint,\n",
    "        \"config\": config,\n",
    "        \"dataset\": dataset,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"num_beams\": num_beams\n",
    "    })\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for cfg_item in cfg_list:\n",
    "    name = cfg_item[\"model_name\"]\n",
    "    num_beams = cfg_item[\"num_beams\"]\n",
    "    print(f\"Evaluating model: {name} with num_beams={num_beams}\")\n",
    "    \n",
    "    model, generation_config = build_model(\n",
    "        cfg_item[\"checkpoint\"],\n",
    "        quantization_config,\n",
    "        cfg_item[\"config\"],\n",
    "    )\n",
    "\n",
    "    inputs, preds, labels = evaluate_model(\n",
    "        model_name=name,\n",
    "        model=model,\n",
    "        generation_config=generation_config,\n",
    "        dataset=cfg_item[\"dataset\"],\n",
    "        tokenizer=cfg_item[\"tokenizer\"],\n",
    "        num_beams=num_beams,\n",
    "    )\n",
    "    \n",
    "    metrics = evaluate_metrics((inputs, preds, labels))\n",
    "    \n",
    "    results_list.append({\n",
    "        \"model_name\": name,\n",
    "        \"num_beams\": num_beams,\n",
    "        \"bleu\": metrics[\"bleu\"],\n",
    "        \"comet\": metrics[\"comet\"]\n",
    "    })\n",
    "    print(metrics)\n",
    "    \n",
    "    del model\n",
    "    del inputs\n",
    "    del preds\n",
    "    del labels\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "df_results = pd.DataFrame(results_list)\n",
    "df_results.to_csv(\"cascade_finetuned.csv\", index=False)\n",
    "print(\"Results saved to cascade_finetuned.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
