{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import whisper\n",
    "import pandas as pd\n",
    "import jiwer\n",
    "import jieba\n",
    "import re\n",
    "import os\n",
    "from opencc import OpenCC\n",
    "from whisper.normalizers.basic import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "cc = OpenCC('t2s')  # traditional -> simplified Chinese\n",
    "\n",
    "model_base = whisper.load_model(\"base\")\n",
    "model_medium = whisper.load_model(\"medium\")\n",
    "model_large = whisper.load_model(\"large\")\n",
    "\n",
    "print(\"Whisper model device:\", next(model_base.parameters()).device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['client_id', 'file', 'audio', 'sentence', 'translation', 'id'],\n",
      "        num_rows: 4843\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'file', 'audio', 'sentence', 'translation', 'id'],\n",
      "        num_rows: 4898\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'file', 'audio', 'sentence', 'translation', 'id'],\n",
      "        num_rows: 7085\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"fixie-ai/covost2\", \"zh-CN_en\")\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_to_evaluate = raw_datasets[\"validation\"].select(range(10))\n",
    "data_to_evaluate = raw_datasets[\"validation\"]\n",
    "\n",
    "def evaluate_model(model, model_name, dataset):\n",
    "    \n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "    \n",
    "    sample = {\n",
    "        \"sentence\": [],\n",
    "        \"hypothesis_simple\": [],\n",
    "        \"hypothesis_traditional\": [],\n",
    "        \"translation\": [],\n",
    "        \"training_time\": 0\n",
    "    }\n",
    "\n",
    "    t_start = time.time()\n",
    "    for s in dataset:\n",
    "        audio_array = s[\"audio\"][\"array\"].astype(\"float32\")\n",
    "        ref = s[\"sentence\"]\n",
    "        \n",
    "        # ASR\n",
    "        result = model.transcribe(audio_array, language=\"zh\")\n",
    "        hyp_tr = result['text']\n",
    "        \n",
    "        # Conversione e pulizia\n",
    "        hyp_simp = cc.convert(hyp_tr).strip()\n",
    "        ref_clean = ref.strip()\n",
    "\n",
    "        sample[\"sentence\"].append(ref_clean)        \n",
    "        sample[\"hypothesis_simple\"].append(hyp_simp)\n",
    "        sample[\"hypothesis_traditional\"].append(hyp_tr)\n",
    "        sample[\"translation\"].append(s[\"translation\"].strip())\n",
    "    \n",
    "    sample[\"training_time\"] = time.time() - t_start\n",
    "    \n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize_zh(text):\n",
    "    text = normalizer(text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    words = jieba.cut(text)\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_base = evaluate_model(model_base, \"base\", data_to_evaluate)\n",
    "sample_medium = evaluate_model(model_medium, \"medium\", data_to_evaluate)\n",
    "sample_large = evaluate_model(model_large, \"large\", data_to_evaluate)\n",
    "samples = [(\"base\", sample_base), (\"medium\", sample_medium), (\"large\", sample_large)]\n",
    "\n",
    "for model_name, sample in samples:\n",
    "    print(f\"\\n--- Analysis for Whisper {model_name} ---\")\n",
    "    \n",
    "    print(f\"Training time: {sample['training_time']:.2f} seconds\")\n",
    "    \n",
    "    sample[\"clean_sentence\"] = [normalizer(text).strip() for text in sample[\"sentence\"]]\n",
    "    sample[\"clean_hypothesis_traditional\"] = [normalizer(text).strip() for text in sample[\"hypothesis_traditional\"]]\n",
    "    sample[\"clean_hypothesis_simple\"] = [normalizer(text).strip() for text in sample[\"hypothesis_simple\"]]\n",
    "    sample[\"clean_translation\"] = [normalizer(text).strip() for text in sample[\"translation\"]]\n",
    "    \n",
    "    # Calcolo CER (quello che hai gi√† fatto, carattere per carattere)\n",
    "    refs_char = [\" \".join(list(normalizer(text))) for text in sample[\"sentence\"]]\n",
    "    hyps_tr_char = [\" \".join(list(normalizer(text))) for text in sample[\"hypothesis_traditional\"]]\n",
    "    hyps_simp_char = [\" \".join(list(normalizer(text))) for text in sample[\"hypothesis_simple\"]]\n",
    "    cer_tr = jiwer.wer(refs_char, hyps_tr_char)\n",
    "    cer_simp = jiwer.wer(refs_char, hyps_simp_char)\n",
    "    \n",
    "    # Calcolo WER (usando il tokenizzatore jieba)\n",
    "    refs_word = [word_tokenize_zh(text) for text in sample[\"sentence\"]]\n",
    "    hyps_tr_word = [word_tokenize_zh(text) for text in sample[\"hypothesis_traditional\"]]\n",
    "    hyps_simp_word = [word_tokenize_zh(text) for text in sample[\"hypothesis_simple\"]]\n",
    "    wer_tr = jiwer.wer(refs_word, hyps_tr_word)\n",
    "    wer_simp = jiwer.wer(refs_word, hyps_simp_word)\n",
    "\n",
    "    print(f\"CER (Character-level Traditional): {cer_tr*100:.2f} %\")\n",
    "    print(f\"CER (Character-level Simplified): {cer_simp*100:.2f} %\")\n",
    "    print(f\"WER (Word-level Traditional with Jieba): {wer_tr*100:.2f} %\")\n",
    "    print(f\"WER (Word-level Simplified with Jieba): {wer_simp*100:.2f} %\")\n",
    "\n",
    "    column_order = [\n",
    "        \"sentence\",\n",
    "        \"hypothesis_simple\",\n",
    "        \"hypothesis_traditional\",\n",
    "        \"translation\",\n",
    "        \"clean_sentence\",\n",
    "        \"clean_hypothesis_simple\",\n",
    "        \"clean_hypothesis_traditional\",\n",
    "        \"clean_translation\",\n",
    "        \"training_time\"\n",
    "    ]\n",
    "\n",
    "    dataframe = pd.DataFrame(sample)[column_order]\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    if os.path.exists('ASR_csvs') == False:\n",
    "        os.mkdir('ASR_csvs')\n",
    "    dataframe.to_csv(f'ASR_csvs/ASR_{model_name}.csv', encoding='utf-8', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
